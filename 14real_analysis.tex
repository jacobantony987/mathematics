Text Books: \cite{apostol}, \cite{rudin}
%Module 1: Functions of bounded variation and rectifiable curves
%Introduction, properties of monotonic functions, functions of bounded variation, total variation, additive property of total variation, total variation on $(a,x)$ as a functions of $x$, functions of bounded variation expressed as the difference of increasing functions, continuous functions of bounded variation, curves and paths, rectifiable path and arc length, additive and continuity properties of arc length, equivalence of paths, change of parameter.
%(Chapter 6, Section: 6.1 - 6.12. of \cite{apostol}) (20 hours.)
%Module 2: The Riemann-Stieltjes Integral
%Definition and existence of the integral, properties of the integral, integration and differentiation, integration of vector valued functions.
%(Chapter 6 - Section 6.1 to 6.25 of \cite{rudin}) (20 hours.)
%Module 3: Sequence and Series of Functions
%Discussion of main problem, Uniform convergence, Uniform convergence and Continuity, Uniform convergence and Integration, Uniform convergence and Differentiation.
%(Chapter 7 Section. 7.1 to 7.18 of \cite{rudin}) (25 hours.)
%Module 4: Weierstrass Approximation \& Some Special Functions
%Equicontinuous families of functions, the Stone - Weierstrass theorem, Power series, the exponential and logarithmic functions, the trigonometric functions, the algebraic completeness of complex field.
%(Chapter 7 â€“ Sections 7.19 to 7.27, Chapter 8 - Section 8.1 to 8.8 of \cite{rudin}) (25 hours.)

%Module 1 - \cite{apostol} 6
%Module 2 - \cite{rudin} 6
%Module 3 - \cite{rudin} 7a
%Module 4 - \cite{rudin} 7b

%The commands to Riemann upper/lower integrals are defined by Leo Liu in tex-stack-exchange.
%Leo Liu - https://tex.stackexchange.com/a/44245 
\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

%Module 1
%\chapter{Functions of Bounded Variation \& Rectifiable Curves}
{\Large Module 1 : Bounded Variation \& Rectifiable Curves}
\section{Functions of Bounded Variation \& Rectifiable Curves}
%\subsection{Introduction}
\setcounter{subsection}{1}
\subsection{Properties of Monotone Functions}
\begin{theorem}
	Let $f$ be an increasing function defined on closed interval $[a,b]$.
	And let $x_0=a < x_1 < x_2 < \dots <x_{n-1} < x_n = b$.
	Then,
	\[ \sum_{k = 1}^{n-1} [f(x_k+)-f(x_k-)] \le f(b) - f(a) \]
\end{theorem}
\begin{proof}
	Let $f$ be an increasing function on $[a,b]$.
	Let $\{x_0=a,\ x_1, \dots,\ x_n=b\}$ be a partition of $[a,b]$.
	Let $y_k \in (x_{k-1},x_k),\ \forall k$.
	Then, $f(y_k) \le f(x_k+)$ and $f(x_k-) \le f(y_{k-1})$.
	Therefore,
	\[ \sum_{k=1}^{n-1} [f(x_k+) - f(x_k-)] \le \sum_{k=1}^{n-1} [f(y_k) - f(y_{k-1})] \le f(b) - f(a) \]
\end{proof}
\begin{commentary}
	In other words, for monotonic functions the sum of jumps is bounded.
\end{commentary}

\begin{theorem}
	Let $f$ be a monotonic function defined on closed interval $[a,b]$.
	Then the set of discontinuities of $f$ is countable.
\end{theorem}
\begin{proof}
	Without loss of generality, let $f$ be an increasing function on $[a,b]$.
	Let $S_m$ be the set of all points on $[a,b]$ at which the jump exceeds $\frac{1}{m}$.\\

	We know that, the sum of jumps of an increasing function is bounded above by $f(b)-f(a)$.	
	Thus, cardinality of $S_m$ given by,
	\[ |S_m| < m[f(b)-f(a)] \]
	is finite for any positive integer $m$.\\

	If $f$ is discontinuous at a pont $x \in [a,b]$, then there exists some integer $m'$ such that $0 < \frac{1}{m'} < x $ and $x \in S_{m'}$.
	\[ \text{Number of discontinuities } = \left| \bigcup_{m=1}^\infty S_m \right| \le \sum_{m=1}^\infty |S_m| \text{ is countable.}\]
	since countable sum of finite values is countable.
	Therefore, the number of discontinuities of $f$ is countable.
\end{proof}

\subsection{Function of Bounded Variation}
\begin{definition}[partition]
	Let $[a,b]$ be a compact interval.
	Let $x_0 = a$, $x_0 < x_1 < x_2 < \dots < x_n$ and $x_n = b$.
	Then $P = \{ x_0,x_1,\dots,x_n \}$ is a \textbf{partition} of $[a,b]$.
	And $(x_{k-1},x_k)$ is the \textbf{$k$th subinterval} of the partition.
\end{definition}

\begin{definition}[bounded variation]
	Let $f$ be a function defined on closed interval $[a,b]$.
	If there exists a positive real-number $M$ such that
	\[ \sum_{k=1}^n |\Delta f_k| = \sum_{k=1}^n |f(x_k) - f(x_{k-1})| \le M \]
	for any partition $P$ on $[a,b]$.
	Then $f$ is a function of \textbf{bounded variation}, where $(x_{k-1},x_k)$ is the $k$th subinterval of the partition.\\

\begin{commentary}
	In other words, a function is of bounded variation on $[a,b]$ if the sum of variations is bounded for any (finite) partition of $[a,b]$.
\end{commentary}
\end{definition}

\begin{theorem}
	Let $f$ be a monotonic function on $[a,b]$.
	Then $f$ is of bounded variation on $[a,b]$.
\end{theorem}
\begin{proof}
	Without loss of generality, let $f$ be an increasing function.
	Then for any partition $P = \{x_0,x_1,\dots,x_n\}$ of $[a,b]$, we have
	\[ \sum_{k=1}^n \left[ f(x_k)-f(x_{k-1}) \right] \le f(b)-f(a) = M \]
	Therefore, $f$ is of bounded variation on $[a,b]$.
\end{proof}

\begin{theorem}
	Let $f$ be a continuous function on $[a,b]$ and its derivative $f'$ exists and $f'$ is bounded in $(a,b)$.
	Then $f$ is of bounded variation.	
\end{theorem}
\begin{proof}
	Let $f$ be a continuous function with derivative $f'$ on $(a,b)$.
	Since $f$ is continuous and $f'$ exists, from intermediate value theorem we have
	\[ \Delta f_k = f(x_k) - f(x_{k-1}) = f'(t_k) [x_k-x_{k-1}] \text{ where } t_k \in (x_{k-1},x_k) \]
	Since $f'$ is bounded, $f'(t_k) \le M'$ for any $t_k \in (a,b)$.
	Thus,
	\[ \sum_{k=1}^n |\Delta f_k| = \sum_{k=1}^n |f'(t_k)| (x_k-x_{k-1}) \le M'\sum_{k=1}^n x_k-x_{k-1} = M'(b-a) = M \]
	Therefore, $f$ is of bounded variation.
\end{proof}

\begin{theorem}
	Let $f$ be a function on $[a,b]$.
	If $f$ is of bounded variation, then $f$ is bounded.
\end{theorem}
\begin{proof}
	Let $x \in (a,b)$.
	Consider the partition $P = \{ a,x,b \}$.
	Since $f$ is of bounded variation, there exists a positive real-number $M$ such that
	\[ \sum_{k=1}^2 |\Delta f_k| = |f(x)-f(a)| + |f(b)-f(x)| \le M \]
	Clearly, $|f(x)-f(a)| \le M$, since $|f(b)-f(x)| > 0$.
	We have,
	\[ |f(x)| = |f(x)-f(a)+f(a)| \le |f(x)-f(a)|+|f(a)| \le M+|f(a)| \]
	Suppose $x = a$, then $|f(x)| = |f(a)|$.\\
	Suppose $x = b$, then $|f(x)| = |f(b)| = M' + |f(a)|$ where $M' = |f(b)|-|f(a)|$.\\

	Therefore, the function $f$ is bounded on $[a,b]$.
\end{proof}

\subsection{Total Variation}
\begin{definition}[total variation]
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $\Sigma (P)$ be the sum of variations with respect to the partition $P$ of $[a,b]$.
	Then the \textbf{total variation} of the function $f$ on $[a,b]$ is given by,
	\[ V_f(a,b) = V_f = \sup \{ \Sigma (P) : P \in \mathscr{P}[a,b] \} \]
\end{definition}
\textbf{Note 1} : $V_f$ is finite, since $f$ is of boundned variation on $[a,b]$.\\

\textbf{Note 2} : $V_f \ge 0$, since $|\Delta f_k| \ge 0$ for any subinterval of $[a,b]$.\\


\textbf{Note 3} : $V_f = 0$ if only if $f$ is a constant function on $[a,b]$. (Why ?)

\begin{theorem}
	Let $f,g$ be functions of bounded variation on $[a,b]$.
	Then their sum $f+g$, difference $f-g$, and product $fg$ are of bounded variation.
	Also, 
	\[ V_{f \pm g} \le V_f + V_g \quad \text{ and } \quad V_{fg} \le AV_f + BV_g \]
	where $\displaystyle A = \sup \left\{ |g(x)| : x \in [a,b] \right\}$ and $\displaystyle B = \sup \left\{ |f(x)| : x \in [a,b] \right\}$.
\end{theorem}
\begin{proof}
	Let $f,g$ be functions of bounded variation on $[a,b]$.
	Then $f,g$ are bounded and $\sup |f(x)|$ and $\sup |g(x)|$ exists.\\

	\textbf{Step 1 : $V_{f \pm g} \le V_f + V_g$}\\
	We have,
	\[ |(f+g)(x_k) - (f+g)(x_{k-1})| \le | f(x_k) - f(x_{k-1})| + |g(x_k) - g(x_{k-1})| \]
	Then
	\begin{align*}
	V_{f+g}  
		& = \sup_{P \in \mathscr{P}} \sum_{k=1}^n |(f+g)(x_k) - (f+g)(x_{k-1})| \\
		& \le \sup_{P \in \mathscr{P}} \sum_{k=1}^n |f(x_k) - f(x_{k-1})| + \sup_{P \in \mathscr{P}} \sum_{k=1}^n |g(x_k)-g(x_{k-1})| \\
		& = V_f + V_g 
\end{align*}
	Similarly, we have $V_{f-g} \le V_f + V_g$ since
	\[ |(f-g)(x_k) - (f-g)(x_{k-1})| \le |f(x_k)-f(x_{k-1})| + |g(x_k)-g(x_{k-1})| \]

	\textbf{Step 2 : $V_{fg} \le AV_f + BV_g$}\\
	We have,
	\begin{align*}
	|fg(x_k)-fg(x_{k-1})| 
		& = |f(x_k)g(x_k) - f(x_{k-1}g(x_{k-1})| \\
		& = |f(x_k)g(x_k) - f(x_{k-1})g(x_k) + f(x_{k-1}g(x_k) - f(x_{k-1}g(x_{k-1}) | \\
		& \le |g(x_k)|\ |f(x_k)-f(x_{k-1})| + |f(x_{k-1}|\ |g(x_k) - g(x_{k-1})| \\
		& \le A |f(x_k)-f(x_{k-1})| + B |g(x_k)-g(x_{k-1})|
	\end{align*}
	where $A = \sup \{ |g(x)| : x \in [a,b] \}$ and $B = \sup \{ |f(x)| : x \in [a,b] \}$.\\

	Therefore,
	\begin{align*}
	V_{fg} 
		& \le A \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |f(x_k)-f(x_{k-1}| \right\} + B \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |g(x_k)-g(x_{k-1})| \right\} \\
		& = AV_f + BV_g 
	\end{align*}
\end{proof}

\begin{definition}[bounded away from zero]
	A function $f$ is bounded away from zero on $[a,b]$ if there exists a real-number $m$ such that $0 < m \le f(x)$, $\forall x \in [a,b]$.
\end{definition}

\begin{commentary}
	Let $f$ be a function of bounded variation.
	Then $\frac{1}{f}$ is of bounded variation if and only if $f$ is bounded away from zero.(Why ?)
\end{commentary}

\begin{theorem}
	Let $f$ be a function of bounded variation on $[a,b]$ and $f$ is bounded away from zero.
	Then, $g = \frac{1}{f}$ is a function of bounded variation on $[a,b]$ and $V_g \le \frac{V_f}{m^2}$ whenever $0 < m \le |f(x)|$.
\end{theorem}
\begin{proof}
	Suppose $f$ is of bounded variation on $[a,b]$ and $f$ is bounded away from zero.
	That is, there exists positive real-number $m$ such that $0 < m \le |f(x)|$, $\forall x \in [a,b]$.
	Then, $\frac{1}{|f(x)|} \le \frac{1}{m}$, $\forall x \in [a,b]$.\\

	Define $g = \frac{1}{f}$.
	Then, we have
	\[ |\Delta g_k| = \left| \frac{1}{f(x_k)} - \frac{1}{f(x_{k-1})} \right| = \frac{|f(x_k)-f(x_{k-1})|}{|f(x_k)|\ |f(x_{k-1}|} \le \frac{|f(x_k)-f(x_{k-1})|}{m^2} \]
	The total variation of $g$ on $[a,b]$ is given by,
	\begin{align*}
	V_g 
		& = \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |g(x_k)-g(x_{k-1})| \right\} \\
		& \le \frac{1}{m^2} \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |f(x_k)-f(x_{k-1})| \right\} \\
		& \le \frac{V_f}{m^2} \text{ where } 0 < m \le |f(x)|,\ \forall x \in [a,b]
	\end{align*}
\end{proof}
\begin{commentary}
	In other words, if $h,f$ are functions of bounded variation of $[a,b]$.
	And $f$ is bounded away from zero, then $g = \frac{1}{f}$ and $\frac{h}{f} = hg$ is a function of bounded variation and $V_{\frac{h}{f}} = V_{hg} \le AV_h + BV_f$.\\

	Now, we have analyzed sum, difference, product and quotient of functions of bounded variation.
	And we are not surprised about preservation of bounded variation under function composition. (Why ?)
\end{commentary}
\subsection{Additive Property of Total Variation}
\begin{theorem}[additive property]
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $c \in (a,b)$.
	Then $f$ is of bounded variation on both $[a,c]$ and $[c,b]$.
	And, $V_f(a,b) = V_f(a,c)+V_f(c,b)$.
\end{theorem}
\begin{proof}
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $P_1,P_2$ be partitions of $[a,c]$ and $[c,b]$ respectively.
	Then $P_0 = P_1 \cup P_2$ is a partition of $[a,b]$.
	Thus,
	\[ \sum (P_1) + \sum (P_2) = \sum (P_0) \le V_f(a,b) \]
	Taking supremums on the left side, we get
	\[ V_f(a,c) + V_f(c,b) \le V_f(a,b) \]	
	Clearly, $V_f(a,c) \le V_f(a,b)$ and $V_f(c,b) \le V_f(a,b)$.
	Therefore, function $f$ is of bounded variation on both $[a,c]$ and $[c,b]$.\\

	Let $P$ be a paritition of $[a,b]$.
	Then $P_0 = P\cup\{c\}$ is refinement of $P$.
	Now, we have two partitions $P_1, P_2$ of $[a,c]$ and $[c,b]$ such that $P_0 = P_1 \cup P_2$.
	Thus,
	\[ \sum (P) \le \sum (P_0) = \sum (P_1) + \sum (P_2) \le V_f(a,c) + V_f(c,b) \]
	Taking supremum on the left side, we get
	\[ V_f(a,b) \le V_f(a,c) + V_f(c,b) \]
	Therefore,
	\[ V_f(a,b) = V_f(a,c) + V_f(c,b),\quad \forall c \in (a,b) \]
\end{proof}

\subsection{Total Variation on $[a,x]$ as a function of $x$}
\begin{commentary}
	By additive property, we have existence of $V_f(a,x)$ for every $x \in (a,b]$.
	Assigning $V(x) = V_f(a,x)$, we have a well-defined function on $(a,b]$.
\end{commentary}

\begin{theorem}
	Let $f$ be a function of bounded variation on $[a,b]$.
	Define $V : [a,b] \to \mathbb{R}$ given by,
	\[ V(x) = \begin{cases} V_f(a,x) & x \in (a,b] \\ 0 & x = a \end{cases} \]
	Then,
	\begin{enumerate}
		\item $V$ is an increasing function on $[a,b]$.
		\item $V-f$ is an increasing function of $[a,b]$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Suppose $x = a$.
	Then $V(x) = 0$ and $V(y) = V_f(a,y) \ge 0 = V(x)$.\\
	Suppose $x \ne a$.
	Then, $a < x < y \le b$.
	By additive property of total variation, we have $V(y) = V_f(a,y) = V_f(a,x)+V_f(x,y) = V(x) + V_f(x,y)$.
	Since $V_f(x,y) \ge 0$, we have $V(x) \le V(y)$.
	Therefore, $V$ is an increasing function on $[a,b]$.\\

	Define $D : [a,b] \to \mathbb{R}$ given by $D(x)  = (V-f)(x) = V(x) - f(x)$.
	Suppose $a \le x < y \le b$.
	Then, $D(y)-D(x) = V(y)-f(y)-V(x)+f(x) = [V(y)-V(x)] - [f(y)-f(x)]$.
	We have, $V(y) = V_f(a,y) = V_f(a,x)+V_f(x,y)$.
	Thus, $V(y)-V(x) = V_f(x,y)$.
	Also we have, $f$ is of bounded variation on $[x,y]$.
	Consider the trivial partition $P = \{ x,y\}$ of $[x,y]$.
	Then, we have $f(y) - f(x) = \sum (P) \le V_f(x,y)$.
	Thus, $D(y)-D(x) = V_f(x,y) - [f(y)-f(x)] \ge 0$.
	Therefore, $D = V-f$ is an increasing function on $[a,b]$.
\end{proof}

\subsection{Function of bounded variation expressed as the difference of increasing functions}
\begin{theorem}
	Let $f$ be a function on $[a,b]$.
	Function $f$ is of bounded variation on $[a,b]$ if and only if $f$ can be expressed as difference of two increasing functions.
\end{theorem}
\begin{proof}
	Let $f$ be a function of bounded variation, then $f = V-D$ where total variation $V$ and $D = V-f$ are both increasing.\\

	Let $f$ be function on $[a,b]$.
	Let $f = V-D$ where $V,D$ are increasing functions. 
	Then $V,D$ are of bounded variation, since monotonic functions on $[a,b]$ are of bounded variation.
	Also, we have $V-D$ is of bounded variation, since for any two functions of bounded variation their difference is also of bounded variation.
\end{proof}

\subsection{Continuous functions of bounded variation}
\begin{theorem}
	Let $f$ be a function of bounded variation on $[a,b]$. %Why it has to be of bounded variation ? For the existence of $V$ ?
	Let $V$ be the total variation function of $f$ defined on $[a,b]$.
	$f$ is continuous at a point if and only if $V$ is continuous at that point.
\end{theorem}
\begin{proof}
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $V$ be the total variation of $f$.
	Let $x,y \in [a,b]$, such that $x<y$.
	We have $V$ is an increasing function on $[a,b]$.
	And $f$ is difference of two increasing functions on $[a,b]$.
	Thus, $f(x+),\ f(x-),\ V(x+),\ V(x-)$ exists for any $x \in (a,b)$.
	It remains to prove that $V,f$ are continuous at $x \in (a,b)$.\\

	\textbf{Part 1 : $V$ continuous $\implies$ $f$ continuous}\\
	Suppose $x \ne a$, then $a<x<y\le b$.
	Let $P$ be any partition on $[a,x]$.
	Then there exists a partition $P'$ on $[a,y]$ such that $P \subset P'$.
	Consider, $P' = P\cup\{y\}$.
	Then, $V(y) > V(x)$ and $V(y)-V(x) \ge |f(y)-f(x)|$.
	Thus,
	\[ 0 \le |f(y)-f(x)| \le V(y)-V(x) \]
	The inequality is true for any $y>x$.
	Therefore,
	\[ 0 \le |f(x+)-f(x)| \le V(x+)-V(x) \text{ as } y \to x+ \]
	Similarly, let $a<z<x$.
	Then,
	\[ 0 \le |f(x)-f(x-)| \le V(x)-V(x-) \text{ as } z \to x- \]
	Clearly, if $V$ is continuous at $x$ then $f$ is continuous $x$.\\

	\textbf{Part 2 : $f$ continuous $\implies$ $V$ continuous}\\
	Suppose $f$ is continuous at $c \in (a,b)$.
	Let $\varepsilon > 0$.
	We have,
	\[ V_f(c,b) = \sup_{P \in \mathscr{P}} \sum (P) \]
	Thus, there exists a partition $P_1 \in \mathscr{P}[c,b]$ such that $V_f(c,b) - \frac{\varepsilon}{2} < \sum (P_1)$.
	Since $f$ is continuous at $c$, there exists $x_1 \in (c,b)$ such that $|f(x_1)-f(c)| < \frac{\varepsilon}{2}$.
	Then,
	\[ V_f(c,b) - \frac{\varepsilon}{2} < \frac{\varepsilon}{2} + V_f(x_1,b) \]
	We have,
	\begin{align*}
	V(x_1)-V(c) 
		& = V_f(a,x_1) - V_f(a,c) = V_f(c,x_1) \\
		& = V_f(c,b) - V_f(x_1,b) < \varepsilon
	\end{align*}
	Clearly, $V(c+h) \to V(c)$ as $h \to 0$.
	That is, $V(c+) = V(c),\ \forall c \in [a,b)$.
	Similarly we have,
	\[ V_f(a,c) = \sup_{P \in \mathscr{P}} \sum (P) \]
	And there exists $P_2 \in \mathscr{P}[a,c]$ such that
	\[ V_f(a,c) - \frac{\varepsilon}{2} < \sum (P_2) \]
	And there exists $x_2 \in (a,c)$ such that $|f(c)-f(x_2)| < \frac{\varepsilon}{2}$, since $f$ is continuous at $c$.
	Therefore,
	\[ V(c)-V(x_2) = V_f(a,c) - V_f(a,x_2) = V(x_2,c) < \varepsilon \]
	Thus, $V(c-) = V(c),\ \forall c \in (a,b]$.
	Therefore, $V$ is continuous at $c$.
\end{proof}
\begin{theorem}
	Let $f$ be a continuous function on $[a,b]$.
	Function $f$ is of bounded variation on $[a,b]$ if and only if $f$ can be expressed as difference of two increasing continuous functions.
\end{theorem}
\begin{proof}
	Let $f$ be a continuous function on $[a,b]$.
	Then total variation $V$ is a continuous increasing function on $[a,b]$.
	Clearly, $D = V-f$ is also a continuous, increasing function on $[a,b]$.
	Therefore, $f = V - D$ where $V,D$ are continuous, increasing functions.\\

	Let $V,D$ be continuous, increasing functions on $[a,b]$.
	Then $f = V-D$ is also a continuous function on $[a,b]$.
	We have, $V,D$ are increasing functions, therefore both $V,D$ are of bounded variation and their difference $f$ is also of bounded variation.
\end{proof}

\begin{commentary}
	Suppose $f$ is of bounded variation on $[a,b]$.
	Let $id : [a,b] \to [a,b]$ where $id(x) = x$.
	Then $V+id$ is a strictly increasing function and $D = V+id-f$ is also strictly increasing.
	Thus, any function of bounded variation on $[a,b]$ can be characterised as difference of two strictly increasing continuous functions on $[a,b]$.
\end{commentary}
\subsection{Curves and Paths}
\begin{definition}[path]
	Let $f : [a,b] \to \mathbb{R}^n$ be a continuous, vector-valued function.
	Then $f$ is a path in $\mathbb{R}^n$.
	And $f$ is a motion if $[a,b]$ is a time interval.
\end{definition}
\subsection{Rectifiable paths and Arc length}
\begin{definition}[rectifiable path]
	Let $f : [a,b] \to \mathbb{R}^n$ be a path in $\mathbb{R}^n$.
	Let $P = \{ t_0,t_1,\dots,t_m \}$ be a partition of $[a,b]$.
	\[ \Lambda_f (P) = \sum_{k=1}^m \| f(t_k) - f(t_{k-1}) \| = \sum_{k=1}^m \| \Delta f_k \| \]
	If $\Lambda_f (P)$ is bounded for any partition $P \in \mathscr{P}[a,b]$, then path $f$ is \textbf{rectifiable}.
	If $\Lambda_f (P)$ is unbounded, then $f$ is \textbf{nonrectifiable}.\\
\end{definition}
\begin{definition}[arc length]
	Let $f : [a,b] \to \mathbb{R}^n$ be a rectifiable path.
	Then \textbf{arc length} of path $f$ is given by,
	\[ \Lambda_f(a,b) = \sup_{P \in \mathscr{P}} \Lambda_f (P) \]
\end{definition}

\begin{theorem}
	A path $f : [a,b] \to \mathbb{R}^n$ is rectifiable if and only if each component $f_k$ of $f$ is of bounded variation on $[a,b]$.
	Let $V_k(a,b)$ be the total variation of $f_k$ on $[a,b]$.
	Then,
	\[ V_k(a,b) \le \Lambda_f(a,b) \le V_1(a,b) + V_2(a,b) + \dots + V_n(a,b) \]
\end{theorem}
\begin{proof}
	Let $x_j \in \mathbb{R}^n$ for $j = 1,2,\dots,m$.
	Then,
	\[ |x_r| \le \sqrt{\sum_{j=1}^n |x_j|^2} \le \sum_{j=1}^n |x_j| \text{ since } \sum_{j=1}^n x_j^2 \le \left(\sum_{j=1}^n x_j\right)^2 \]
	Let $f : [a,b] \to \mathbb{R}^n$ be a path in $\mathbb{R}^n$.
	Then $f = (f_1,f_2,\dots,f_n)$ where $f_k$'s are components of the path $f$.
	Let $P = \{ t_0,t_1,\dots,t_m \}$ be a partition of $[a,b]$.
	Now $f_r(t_j)-f_r(t_{j-1}) \in \mathbb{R}^n$ for each subinterval of $[a,b]$ and each component of $f$.
	Thus for each subinterval $(t_j,t_{j-1})$ we have,
	\[ |f_r(t_j)-f_r(t_{j-1})| \le \| f(t_j)-f(t_{j-1}) \| \le \sum_{j=1}^n |f_k(t_j)-f_k(t_{j-1})| \] 
	Adding inequalities for every subinterval of the partition, we get
	\[ \sum_{k=1}^m |f_k(t_j)-f_k(t_{j-1})| \le \sum_{k=1}^m \| f(t_j)-f(t_{j-1}) \| \le \sum_{k=1}^m \sum_{j=1}^n |f_k(t_j)-f_k(t_{j-1})| \] 
	Rearranging summation, we get
	\[ \sum (P) \le \Lambda_f (P) \le \sum_{j=1}^n \left( \sum (P)_{f_j} \right) \] 
	Suppose $f$ is a rectifiable path.
	Then $f$ has finite arc length $\Lambda_f (a,b)$.
	Let $f_k$ be a component function of $f$.
	Let $P$ be a partition of $[a,b]$.
	Then,
	\[ \sum (P) \le \Lambda_f(P) \le \Lambda_f (a,b) \]
	Thus, $\sum (P)$ is bounded for any partition $P$.
	Therefore, total variation $V_k(a,b)$ exists for each component function $f_k$.
	And $V_k(a,b) \le \Lambda_f(a,b)$.\\

	Suppose $f_k$'s are of bounded variation.
	Then \[ \Lambda_f(P) \le \sum_{j=1}^n \sum (P) \le \sum_{j=1}^n V_j(a,b) \] is bounded for any partition $P$.
	Thus, $f$ is rectifiable.
	And, arc length $\displaystyle \Lambda_f(a,b) \le \sum_{k=1}^n V_k (a,b)$.
	Combining the inequalities, we get
	\[ V_k(a,b) \le \Lambda_f(a,b) \le \sum_{j=1}^n V_j (a,b) \]
\end{proof}
\subsection{Additive and Continuity Properties of Arc length}
\begin{theorem}[additive]
	Let $f : [a,b] \to \mathbb{R}^n$ be a rectifiable path.	
	Let $c \in (a,b)$.
	Then,
	\[ \Lambda_f(a,b) = \Lambda_f(a,c) + \Lambda_f(c,b) \]
\end{theorem}
\begin{proof}
	Let $P$ be a partition of $[a,b]$.
	Then $P' = P \cup \{c\}$ is a refinement of $P$ such that $P' = P_1 \cup P_2$ where $P_1,P_2$ are partition of $[a,c]$ and $[c,b]$ respectively.
	We have,
	\[ \Lambda_f(P) \le \Lambda_f(P') = \Lambda_f(P_1) + \Lambda_f(P_2) \]
	This inequality if true for any partition of $[a,b]$.
	Thus,
	\[ \Lambda_f(a,b) \le \Lambda_f(a,c) + \Lambda_f(c,b) \]
	Let $P_1,P_2$ be partition of $[a,c]$ and $[c,b]$ respectively.
	Then,
	\[ \Lambda_f(P_1) + \Lambda_f(P_2) \le \Lambda_f(P) \le \Lambda_f(a,b) \]
	This inequality if true for any paritions on $[a,c]$ and $[c,b]$.
	Thus,
	\[ \Lambda_f(a,c) + \Lambda_f(c,b) \le \Lambda_f(a,b) \]
\end{proof}
\begin{theorem}[continuity]
	Let $f : [a,b] \to \mathbb{R}^n$ be  rectifiable path.
	Let function $s : [a,b] \to \mathbb{R}$ defined by
	\[ s(x) = \begin{cases} 0 & x = a \\ \Lambda_f(a,x) & x \in (a,b] \end{cases} \]
	Then,
	\begin{enumerate}
		\item function $s$ is continuous and increasing on $[a,b]$.
		\item if there is no subinterval of $[a,b]$ in which $f$ is constant, then $s$ is strictly increasing.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $a \le x < y \le b$.
	Then, $s(y)-s(x) = \Lambda_f(x,y) = \Lambda(a,y) - \Lambda(a,x) \ge 0$.
	Therefore, $s$ in increasing.\\
	
	Suppose $f$ is not constant in any subinterval $[x,y]$ of $[a,b]$.
	Suppose $s$ is not strictly increasing.
	Then, there exists $x,y \in (a,b)$ such that $x < y$ and $s(y)-s(x) = 0$.
	Thus,
	\[ \Lambda_f(a,y) - \Lambda(a,x) = \Lambda_f(x,y) = 0 \]
	Thus, $V_k(x,y) = 0,\ \forall k$ which is a contradition since $f$ is not constant in $[x,y]$.
	Therefore, $s$ is strictly increasing.
\end{proof}
\subsection{Equivalence of path, Change of parameter}
\begin{definition}[change of parameter]
	Let $f:[a,b] \to \mathbb{R}^n$ be a path.
	Let $g : [c,d] \to \mathbb{R}^n$ be another path.
	Then $f,g$ are \textbf{equivalent} if there exists a continuous, real-valued function, $u : [c,d] \to [a,b]$ such that $g = f \circ u$.
	That is, $g(t) = f(u(t)),\ \forall t \in [c,d]$.
	In other words, $f,g$ are different parametric representations of a common graph.\\

	Function $u$ defines a change of parameter.
	If $u$ is strictly increasing, then $f,g$ are in the same direction.
	And $u$ is an orientation preserving change of parameter.
	If $u$ is strictly decreasing, then $f,g$ are in opposite directions.
	And $u$ is an orientation reversing change of parameter.
\end{definition}

\begin{theorem}[change of parameter]
	Let $f:[a,b] \to \mathbb{R}^n$ and $g : [a,b] \to \mathbb{R}^n$ be two paths.
	Let $f,g$ be both injective functions.
	Then $f$ and $g$ are equivalent if they have the same graph.
\end{theorem}
\begin{proof}
	Let $f : [a,b] \to \mathbb{R}^n$ and $g : [c,d] \to \mathbb{R}^n$ be continuous, injective, vector-valued functions.
	Suppose $f,g$ are equivalent paths, then $f,g$ have the same graph.\\

	Suppose $f,g$ have the same graph.
	Since $f$ is injective and continuous on its domain $[a,b]$, function $f^{-1}$ exists and is continuous on its graph.
	\[ \text{Define } u:[c,d] \to [a,b],\ u(t) = f^{-1}(g(t)) \]
	Then $u$ is continuous and $g(t) = f(u(t))$.
	Suppose $u$ is not a strictly monotonic function. 
	Since $u$ is continuous, there exists $t_1,t_2 \in [c,d]$ such that $u(t_1) = u(t_2)$.
	Then $f(u(t_1)) = f(u(t_2)) \implies g(t_1) = g(t_2)$ which is a contradiction since $g$ is injective on $[c,d]$.
\end{proof}

%Module 2
%\chapter{The Riemann Stieltjes Integral} %Rudin chapter 6$
\pagebreak
{\Large Module 2 : Riemann-Stieltjes Integral}\\
\section{The Riemann-Stieltjes Integral}
\begin{definition}[unti step]
	The unit step function $I : \mathbb{R} \to \mathbb{R}$ is defined by
	\[ I(x) = \begin{cases} 0 & x \le 0 \\ 1 & x > 0 \end{cases} \] 
\end{definition}
\begin{definition}[Riemann Integral]
	Let $f$ be a bounded real function defined on $[a,b]$.
	Let $P = \{x_0,x_1,\dots,x_n\}$ be a partition of $[a,b]$.
	Let $M_k = \sup \{ f(x) : x \in [x_{k-1},x_k] \}$ and $m_k = \inf \{ f(x) : x \in [x_{k-1},x_k]$.\\

	Then Riemann upper sum of function $f$ with respect to parition $P$,
	\[ U(P,f) = \sum_P M_k \Delta x_k = \sum_{k=1}^n M_k (x_k-x_{k-1}) \]
	And Riemann lower sum of $f$ with respect to $P$,
	\[ L(P,f) = \sum_{k=1}^n m_k (x_k-x_{k-1}) \]
	Now, Riemann upper integral of $f$ over $[a,b]$,
	\[ \upint_a^b f\ dx = inf \{ U(P,f) : P \in \mathscr{P}[a,b] \} \]
	And, Riemann lower integral of $f$ over $[a,b]$,
	\[ \lowint_a^b f\ dx = sup \{ L(P,f) : P \in \mathscr{P}[a,b] \} \]
	A function $f$ is Riemann integrable over $[a,b]$ if Riemann lower and upper integrals of $f$ over $[a,b]$ are the same.
	Then Riemann integral of $f$ over $[a,b]$,
	\[ \int_a^b f\ dx = \upint_a^b f\ dx = \lowint_a^b f\ dx \]
\end{definition}
\begin{definition}[Riemann-Stieltjes Integral]
	Let $f$ be a bounded function on $[a,b]$.
	Let $\alpha$ be an increasing function on $[a,b]$.
	Let $P = \{ x_0,x_1,\dots,x_n\}$ be a partition of $[a,b]$.
	Then, the Riemann-Stieltjes upper sum of $f$ with respect to partition $P$ and increasing function $\alpha$,
	\[ U(P,f,\alpha) = \sum_{k=1}^n M_k \Delta \alpha_k \]
	where $M_k = \sup \{ f(x) : x \in [x_{k-1},x_k] \}$ and $\Delta \alpha_k = \alpha(x_k) - \alpha(x_{k-1})$.
	Similarly, Riemann-Stieltjes lower sum,
	\[ L(P,f,\alpha) = \sum_{k=1}^n m_k \Delta \alpha_k \]
	where $m_k = \inf \{ f(x) : x \in [x_{k-1},x_k] \}$.
	And function $f$ is Riemann-Stieltjes integrable if Riemann Stieltjes upper and lower integrals are the same.
	\[ \int_a^b f\ d\alpha = \upint_a^b f\ d\alpha = \lowint_a^b f\ d\alpha \]
	where $\displaystyle \upint_a^b f\ d\alpha = \upint_a^b f\ d\alpha(x) = \inf \left\{ U(P,f,\alpha) : P \in \mathscr{P}[a,b] \right\}$ and\\
	$\displaystyle \lowint_a^b f\ d\alpha = \sup \left\{ L(P,f,\alpha) : P \in \mathscr{P}[a,b] \right\}$.\\

	We write $f \in \mathscr{R}(\alpha)$ on $[a,b]$, which means that a bounded, real function $f$ is Riemann-Stieltjes integrable on $[a,b]$ with respect to the increasing function $\alpha$.
\end{definition}

\textbf{Note : } function $\alpha : [a,b] \to \mathbb{R}$ is monotonic (increasing), but not necessarily continuous.\\

\textbf{Remark : } With $\alpha = id$ identity function, Riemann-Stieltjes integral is Riemann integral itself.
	That is, Riemann integral is a special case of Riemann-Stieltjes integral.

\begin{theorem}
	Let $P^\ast$ be a refinement of a parition $P$ of $[a,b]$.
	Then, $L(P,f,\alpha) \le L(P^\ast,f,\alpha)$ and $U(P^\ast,f,\alpha) \le U(P,f,\alpha)$.
\end{theorem}
\begin{proof}
	Let $P^\ast = P \cup \{ x \}$ where $x$ belongs to the $i$th subinterval $(x_{j-1},x_j)$.
	Define $w_1 = \min \{ f(x) : x \in (x_{j-1},x) \}$ and $w_2 = \min \{ f(x) : x \in (x,x_j) \}$.
	Clearly, $w_1,w_2 \ge \min \{ f(x) : x \in (x_{j-1},x_j) \} = m_i$.
	\begin{align*}
	L(P,f,\alpha) - L(P^\ast,f,\alpha) 
		& = m_i \Delta \alpha_j - w_1 (\alpha(x)-\alpha(x_{j-1})) - w_2 (\alpha(x_j) - \alpha(x)) \\
		& = (m_i - w_1)(\alpha(x)-\alpha(x_{j-1})) + (m_i - w_2)(\alpha(x_j)-\alpha(x))\\
		& \ge 0
	\end{align*}
	By mathematical induction the inequality is true for any refinement $P^\ast$ of $P$.
	Therefore, $L(P,f,\alpha) \ge L(P^\ast,f,\alpha)$.\\

	Similarly, define $W_1 = \max \{f(x) : x \in (x_{j-1},x) \}$ and $W_2 = \max \{ f(x) : x \in (x,x_j) \}$
	where $W_1,W_2 \le \max \{ f(x) : x \in (x_{j-1},x_j) \} = M_i$.
	\begin{align*}
	U(P,f,\alpha) - U(P^\ast,f,\alpha) 
		& = M_i \Delta \alpha_j - W_1 (\alpha(x)-\alpha(x_{j-1})) - W_2 (\alpha(x_j) - \alpha(x)) \\
		& = (M_i - W_1)(\alpha(x)-\alpha(x_{j-1})) + (M_i - W_2)(\alpha(x_j)-\alpha(x))\\
		& \le 0
	\end{align*}
	Again, the result is true for any refinement $P$ and we have
	\[ U(P,f,\alpha) \le U(P^\ast,f,\alpha) \]
\end{proof}

\begin{theorem}
	Let $f$ be a bounded, real function on $[a,b]$ and $\alpha$ increasing function on $[a,b]$.
	Then,
	\[ \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \]
\end{theorem}
\begin{proof}
	Let $P_1,P_2$ be any two partition of $[a,b]$.
	Let $P^\ast = P_1 \cup P_2$ be a refinement of both partitions.
	Then, we have $L(P^\ast,f,\alpha) \le U(P^\ast,f,\alpha)$.
	And,
	\[ \lowint_a^b f\ d\alpha \le L(P_1,f,\alpha) \text{ and } U(P_2,f,\alpha) \le \upint_a^b f\ d\alpha \]
	Therefore, \[ L(P_1,f,\alpha) \le L(P^\ast,f,\alpha) \le U(P^\ast,f,\alpha) \le U(P_2,f,\alpha) \]
	Clearly, the inequality holds independent of the choice of the partition.\\
	Taking supremum on right and infimum on left, we get
	\[ \sup_{P \in \mathscr{P}} L(P,f,\alpha) \le \inf_{ P \in \mathscr{P}} U(P,f,\alpha) \]
	Therefore,
	\[ \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \]
\end{proof}

\begin{theorem}[criterion for integrability]
	Let $f$ be  a bounded, real function on $[a,b]$.
	Let $\alpha$ be an increasing function on $[a,b]$.
	Then, $f$ is Riemann-Stieltjes integrable on $[a,b]$ with repesct to $\alpha$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
\end{theorem}
\begin{commentary}
	In other words, $f \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if 
	\[ \forall \varepsilon > 0, \exists P \in \mathscr{P}[a,b] \text{ such that }U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon \]
\end{commentary}
\begin{proof}
	Let $\varepsilon > 0$.
	Suppose there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	We have,
	\[ L(P,f,\alpha) \le \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \le U(P,f,\alpha) \]
	\[ \upint_a^b f\ d\alpha - \lowint_a^b f\ d\alpha \le U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon \]
	Clearly, $\lowint_a^b f\ d\alpha = \upint_a^b f\ d\alpha$.
	Therefore, $f \in \mathscr{R}(\alpha)$.\\

	Suppose $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then by the definition of infimum there exists a partition $P_1$ of $[a,b]$ such that 
	\[ U(P_1,f,\alpha) - \upint_a^b f\ d\alpha < \frac{\varepsilon}{2} \]
	Similarly, there exists a partition $P_2$ of $[a,b]$ such that
	\[ \lowint_a^b f\ d\alpha - L(P_2,f,\alpha) < \frac{\varepsilon}{2} \]
	Consider $P^\ast = P_1 \cup P_2$.
	Clearly, $P^\ast$ is a refinement for both $P_1$ and $P_2$.
	Thus, \[ U(P^\ast,f,\alpha) - \upint_a^b f\ d\alpha < \frac{\varepsilon}{2} \]
	And,
	\[ \lowint_a^b f\ d\alpha - L(P^\ast,f,\alpha) < \frac{\varepsilon}{2} \]
	Thus, \[ U(P^\ast,f,\alpha) - \upint_a^b f\ d\alpha + \lowint_a^b f\ d\alpha - L(P^\ast,f,\alpha) < \varepsilon \]
	Given, $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Therefore, $U(P^\ast,f,\alpha) - L(P^\ast,f,\alpha) < \varepsilon$.
\end{proof}

\begin{theorem}
	Suppose $\varepsilon > 0$ and $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$ for some partition $P$ of $[a,b]$.
	\begin{enumerate}
		\item The inequaility is true for any refinement of $P$.
		\item Let $s_i,t_i \in [x_{i-1},x_i]$ for each subinterval of the partition $P$.
			\[ \sum_{i=1}^n \left| f_(s_i)-f(t_i) \right|\ \Delta\alpha_i < \varepsilon \]
		\item If $f \in \mathscr{R}(\alpha)$ and $t_i \in [x_{i-1},x_i]$ for each subinterval of the partition $P$, then
			\[ \left| \sum_{i=1}^n f(t_i) \Delta \alpha_i - \int_a^b f \ d\alpha \right| < \varepsilon \]
	\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item Let $\varepsilon > 0$.
		Suppose $U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon$.
		Let $P^\ast$ be a refinement of $P$.
		We have $U(P,f,\alpha) \ge U(P^\ast,f,\alpha)$ and $L(P,f,\alpha) \le L(P^\ast,f,\alpha)$.
		Thus,
		\[ U(P^\ast,f,\alpha)-L(P^\ast,f,\alpha) \le U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon \]
	\item Let $s_i,t_i \in (x_{i-1},x_i)$.
		Clearly, for each subinterval $(x_{i-1},x_i)$, we have $m_i \le f(s_i) \le M_i$ and $m_i \le f(t_i) \le M_i$.
		Thus,
		\[ |f(t_i) - f(s_i)| \le  M_i - m_i \]
		\[ \sum_{i=1}^n |f(t_i) - f(s_i)| \Delta \alpha_i \le \sum_{i=1}^n (M_i - m_i) \Delta \alpha_i \le U(P,f,\alpha)-L(P,f,\alpha) \le \varepsilon \]
	\item Let $\varepsilon > 0$.
		Let $P$ be a partition of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
		We have,
		\[ L(P,f,\alpha) \le \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \le U(P,f,\alpha) \]
		Suppose $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
		Then, 
		\[ L(P,f,\alpha) \le \int_a^b f\ d\alpha \le U(P,f,\alpha) \] 

		Let $t_i \in (x_{i-1},x_i)$ for each subinterval of the partition $P$.
		Clearly, $m_i \le f(t_i) \le M_i$.
		Thus, we also have
		\[ L(P,f,\alpha) \le \sum_{i=1}^n f(t_i) \Delta \alpha_i \le U(P,f,\alpha) \]
		Therefore, 
		\[ \left| \sum_{i=1}^n f(t_i) \ \Delta \alpha_i - \int_a^b f\ d\alpha \right| \le U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon \]
\end{enumerate}
\end{proof}
\begin{theorem}
	If $f$ is continuous on $[a,b]$, then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be a continuous function on $[a,b]$.
	Since continuous functions defined on compact subsets metric spaces are uniformly continuous, we have $f$ is uniformly continuous.\\

	Let $\varepsilon > 0$.
	Choose $\eta > 0$ such that $[\alpha(b)-\alpha(a)]\eta < \varepsilon$.
	Since $f$ is uniformly continuous, there exists $\delta > 0$ such that $|f(x)-f(t)| < \eta$ whenever $|x-t| < \delta$.	
	Consider the partition $P$ such that each subinterval is of length less than $\delta$.
	For any $s_i,t_i \in (x_{i-1},x_i)$, we have
	\begin{align*}
	\sum_{i=1}^n [f(t_i) - f(s_i)] \ [\alpha(x_i)-\alpha(x_{i-1})] 
		& \le \sum_{i=1}^n \eta [\alpha(x_i)-\alpha(x_{i-1})] \\
		& \le \eta \sum_{i=1}^n [\alpha(x_i)-\alpha(x_{i-1})] \\
		&	\le \eta[\alpha(b)-\alpha(a)] \\
		& < \varepsilon 
	\end{align*}
	Clearly, the result is true for minimum and maximum values of $f$ in each subinterval of $P$.
	Therefore, we have
	\[ U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon \]
	Thus, $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\begin{theorem}
	If $f$ is monotonic on $[a,b]$ and $\alpha$ is continuous on $[a,b]$, then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be increasing on $[a,b]$.
	Let $\alpha$ is continuous on $[a,b]$.
	Let $n$ be any integer.
	We can construct a partition $P$ of $[a,b]$ such that each the variation of $\alpha$ in each subinterval is fixed.
	That is, $\Delta \alpha_i = \frac{\alpha(b)-\alpha(a)}{n}$.
	Since $f$ is increasing, in each subinterval $[x_{i-1},x_i]$, we have $M_i = f(x_i)$ and $m_i = f(x_{i-1})$.
	Now we have,
	\begin{align*}
	U(P,f,\alpha) - L(P,f,\alpha) 
		& = \sum_{i=1}^n M_i \Delta \alpha_i - \sum_{i=1}^n m_i \Delta \alpha_i \\
		& = \sum_{i=1}^n (M_i-m_i) \Delta \alpha_i \\
		& = \frac{\alpha(b)-\alpha(a)}{n} \sum_{i=1}^n (f(x_i) - f(x_{i-1})) \\
		& = \frac{\alpha(b)-\alpha(a)}{n} (f(b)-f(a))
	\end{align*}
	Thus, given $\varepsilon > 0$, there exists an integer $n$ such that $ (\alpha(b)-\alpha(a))(f(b)-f(a)) < n\varepsilon $.
	And, we get a partition $P$ by fixing $\Delta \alpha_i = \frac{\alpha(b)-\alpha(a)}{n}$ such that $ U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon $.\\

	In other words, $U(P,f,\alpha)-L(P,f,\alpha)$ depends on $n$ and can be reduced to a value less than $\varepsilon$ by increasing the value of $n$.
	Therefore, $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\begin{theorem}
	If $f$ bounded on $[a,b]$, $f$ has only finitely many points of discontinuities on $[a,b]$ and $\alpha$ is continuous at every point at which $f$ is discontinuous.
	Then $f \in \mathscr{R}(\alpha)$.
\end{theorem}
\begin{proof}
	Suppose $f$ is bounded and has only finitely many discontinuities on $[a,b]$.
	Since $f$ is bounded, we have $-M < f(x) < M$ for some real number $M$.
	Also, suppose that $\alpha$ is continuous at those points where $f$ is discontinuous on $[a,b]$.\\

	Let $E$ be the set of all discontinuitites of $f$ on $[a,b]$.
	Clearly, $|E|$ is finite.\\

	Let $\varepsilon > 0$.
	Since, $\alpha$ is continuous at each point $e_j$ of $E$, \textcolor{blue}{there exists open intervals $(u_j,v_j)$ such that $\alpha$ is continuous on those intervals, $e_j$ belongs to the interior of these intervals and the sum of variation of $\alpha$ in those intervals is less than $\varepsilon$.}\\

	Removing these open intervals from $[a,b]$, we get a compact subset $K$ in which $f$ is continuous.
	Since $K$ is compact and $f$ is continuous on $K$, we have $f$ is uniformly continuous on $K$.\\

	Given $\varepsilon > 0$, there exists $\delta > 0$ such that $|f(x)-f(t)| < \varepsilon$ whenever $|x-t|<\delta$.
	Define a partition $P$ such that $u_j,\ v_j \in P$, $P$ doesn't have any point in the interior of any $(u_j,v_j)$.
	And if $x_{i-1} \ne u_j$, then $x_i$ is so choosen that $x_i-x_{i-1} < \delta$, dividing $K$ into subintervals of length less than $\delta$.
	Now, we have
	\begin{align*}
	U(P,f,\alpha) & - L(P,f,\alpha)
		 = \sum_{i=1}^n (M_i-m_i) \Delta \alpha_i  \\
		& \le \varepsilon \sum_{x_{i-1} \ne u_j} \Delta\alpha_i + 2M\sum_{x_{i-1} = u_j} \Delta\alpha_i \\
		& = \varepsilon \sum_{x_{i-1} \ne u_j} \left(\alpha(x_i)-\alpha(x_{i-1}) \right) + 2M\varepsilon \\
		& \le \varepsilon (\alpha(b)-\alpha(a)) + 2M\varepsilon
	\end{align*}
	Clearly, $U(P,f,\alpha)-L(P,f,\alpha)$ is a function of $\varepsilon$ which can reduced below any real number greater than zero by reducing the value of $\varepsilon$.
	Therefore, $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\begin{theorem}
	Suppose $f \in \mathscr{R}(\alpha)$, $m \le f \le M$ on $[a,b]$, $\phi$ is continuous on $[m,M]$ and $h(x) = \phi(f(x))$.
	Then $h \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be a function on $[a,b]$ such that $m \le f(x) \le M$ and $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
	Let $\phi$ be  continuous function on $[m,M]$.
	Then $\phi$ is uniformly continuous on $[m,M]$, since $[m,M]$ is compact.
	Thus, given $\varepsilon > 0$ there exists $\delta > 0$ such that $|\phi(x)-\phi(t)| < \varepsilon$ whenever $|x-t|<\delta$.
	Without loss of generality, we may assume that $\delta < \varepsilon$.
	Otherwise choose a value less than $\varepsilon$ as $\delta$.\\

	Since $f \in \mathscr{R}(\alpha)$ over $[a,b]$, there exists a partition $P$ of $[a,b]$ such that
	\[ U(P,f,\alpha) - L(P,f,\alpha) < \delta^2 \]
	Consider the $i$th subinterval $[x_{i-1},x_i]$ of the partition $P$.
	Let $M_i$, $m_i$ be the maximum and minimum values of the function $f$ in $i$th subinterval of $P$.
	Now, we divided the collection of subintervals into two sets depending on the value of $M_i-m_i$ in comparison with $\delta$.
	Define $A$ as the set of all subintervals of $P$ such that $M_i-m_i < \delta$.
	And $B$ as the set of all subintervals of $P$ such that $M_i - m_i \ge \delta$.
	We know that,
	\[ \delta \sum_B \Delta \alpha_i \le \sum_B (M_i-m_i) \Delta \alpha_i < U(P,f,\alpha)-L(P,f,\alpha) < \delta^2 \]
	Thus, $\displaystyle \sum_B \Delta \alpha_i < \delta$.\\

	Define $M_i^\ast$, $m_i^\ast$ as the maximum and minimum of the function $\phi \circ f$ each subinterval $[x_{i-1},x_i]$ of the partition $P$.
	Now, we have
	\begin{align*}
	U(P,\phi \circ f,\alpha) - L(P,\phi \circ f, \alpha)
		& = \sum_{i=1}^n (M_i^\ast - m_i^\ast) \Delta \alpha_i \\
		& = \sum_A (M_i^\ast - m_i^\ast) \Delta \alpha_i + \sum_B (M_i^\ast - m_i^\ast) \Delta \alpha_i 
		\intertext{Since $\phi$ is uniformly continuous,}
		& \le \varepsilon \sum_A \Delta \alpha_i + \sum_B (M_i^\ast-m_i^\ast)\Delta \alpha_i 
	\intertext{Since, continuous functions defined on compact subset attains extrema. We have, $K = \sup \{ |\phi(x)| : x \in [m,M] \}$. And $M_i^\ast - m_i^\ast < 2K$.}
		& \le \varepsilon (\alpha(b)-\alpha(a)) + 2K \sum_B \Delta \alpha_i \\
		& \le \varepsilon (\alpha(b)-\alpha(a)) + 2K\delta \\
		& \le \varepsilon (\alpha(b) - \alpha(a)) + 2K\varepsilon
	\end{align*}
	Since $U(P,\phi \circ f,\alpha)-L(P,\phi \circ f, \alpha)$ is function of $\varepsilon$, it can be reduced to any sufficiently small real number of our choice.
	Therefore, $\phi \circ f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\subsection{Properties of the Riemann-Stieltjes Integral}
\begin{theorem}
	Let $f,f_1,f_2$ be bounded real functions on $[a,b]$.
	Let $\alpha,\alpha_1,\alpha_2$ be increasing functions on $[a,b]$.
	\begin{enumerate}
		\item If $f_1,f_2,f \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_1+f_2 \in \mathscr{R}(\alpha)$ on $[a,b]$.
			And,
			\[ \int_a^b \left( f_1 + f_2 \right)\ d\alpha = \int_a^b f_1\ d\alpha + \int_a^b f_2\ d\alpha \]
			If $c \in \mathbb{R}$, then $cf \in \mathscr{R}(\alpha)$ on $[a,b]$.
			And,
			\[ \int_a^b cf\ d\alpha = c\int_a^b f\ d\alpha \]
		\item If $f_1(x) \le f_2(x)$ on $[a,b]$, then
			\[ \int_a^b f_1\ d\alpha \le \int_a^b f_2\ d\alpha \]
		\item If $c \in (a,b)$, then $f \in \mathscr{R}(\alpha)$ on $[a,c]$ and $[c,b]$, then
			\[ \int_a^c f\ d\alpha + \int_c^b f\ d\alpha = \int_a^b f\ d\alpha \]
		\item If $|f(x)| \le M$ on $[a,b]$, then 
			\[ \left| \int_a^b f\ d\alpha \right| \le M[\alpha(b)-\alpha(a)] \]
		\item If $f \in \mathscr{R}(\alpha_1)$ and $f \in \mathscr{R}(\alpha_2)$ on $[a,b]$, then $f \in \mathscr{R}(\alpha_1+\alpha_2)$.
			And,
			\[ \int_a^b f\ d(\alpha_1+\alpha_2) = \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]
			If $f \in \mathscr{R}(\alpha)$ on $[a,b]$, and $c \in \mathbb{R}$, then $f \in \mathscr{R}(c\alpha)$ on $[a,b]$.
			And,
			\[ \int_a^b f\ d(c\alpha) = c\int_a^b f\ d\alpha \]
	\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item Let $\varepsilon > 0$.
	Let $f_1,f_2 \in \mathscr{R}(\alpha)$ over $[a,b]$. \\
	Let $P_1$ be a partition of $[a,b]$ such that $U(P_1,f_1,\alpha) - L(P_1,f_1,\alpha) < \frac{\varepsilon}{2}$.\\
	Let $P_2$ be a partition of $[a,b]$ such that $U(P_2,f_2,\alpha) - L(P_2,f_2,\alpha) < \frac{\varepsilon}{2}$.\\
	Let $P = P_1 \cup P_2$ be the refinedment of both the partitions.
	Then the above inequalities are true of the partition $P$ as well.\\

	We have,
	\begin{align*}
	L(P,f_1,\alpha) + L(P,f_2,\alpha) 
		& \le L(P,f_1+f_2,\alpha) \\
		& \le U(P,f_1+f_2,\alpha) \\
		& \le U(P,f_1,\alpha) + U(P,f_2,\alpha) 
	\end{align*}
	Thus,
	\begin{align*}
	U(P,f_1+f_2,\alpha) & - L(P,f_1+f_2,\alpha)\\
		& \le U(P,f_1,\alpha) - L(P,f_1,\alpha) + U(P,f_2,\alpha) - L(P,f_2,\alpha) \\
		& \le \varepsilon
	\end{align*}
	Therefore, $f_1+f_2 \in \mathscr{R}(\alpha)$ over $[a,b]$.\\

	\hrule \vspace{1em}
	\item
	Let $c$ be any real number greater than zero.
	We have,
	\[ M_i' = \max \{ cf(x) : x \in [x_{i-1},x_i] \} = c \max \{ f(x) : x \in [x_{i-1},x_i] \} = cM_i \]
	Similarly, $m_i' = cm_i$.
	Thus,
	\[ \sum_{i=1}^n M_i'\Delta\alpha_i = c\sum_{i=1}^n M_i\Delta \alpha_i \text{ and } \sum_{i=1}^n m_i' \Delta\alpha_i = c\sum_{i=1}^n m_i \Delta\alpha_i \] 
	Therefore, given $\varepsilon > 0$ we have,
	\begin{align*}
		L(P,cf,\alpha) &= cL(P,f,\alpha) \\
		U(P,cf,\alpha) &= cU(P,f,\alpha)
	\end{align*}
	Since, $f \in \mathscr{R}(\alpha)$, there exists partition $P$ such that $U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon$. Thus,
	\[ U(P,cf,\alpha)-L(P,cf,\alpha) = cU(P,f,\alpha)-cL(P,f,\alpha) < c\varepsilon \]
	Therefore, $cf \in \mathscr{R}(\alpha)$ over $[a,b]$.
	\begin{align*}
	\int_a^b cf\ d\alpha 
		& \le U(P,cf,\alpha) \\
		& \le cU(P,f,\alpha) \\
		& \le c \int_a^b f\ d\alpha + c\varepsilon\\
		& \le c\int_a^b f\ d\alpha
	\intertext{Similarly, considering $-f$ and $-cf$ we get}
	\int_a^b cf\ d\alpha 
		& \ge c\int_a^b f\ d\alpha
	\end{align*}
	Therefore, without loss of generality for any real number $c$, 
		\[ \int_a^b cf\ d\alpha = c\int_a^b f\ d\alpha \]

	\hrule \vspace{1em}
	\item 
	Let $\varepsilon > 0$.
	Suppose $f_1,f_2 \in \mathscr{R}(\alpha)$ and $f_1 \le f_2$.
	Let $P_1,P_2$ be the partitions such that $U(P_1,f_1,\alpha) - L(P_1,f_1,\alpha) < \varepsilon$ and $U(P_2,f_2,\alpha) - L(P_2,f_2,\alpha) < \varepsilon$.
	Consider the common refinement $P = P_1 \cup P_2$.
	Then, for each subinterval of the partition $P$, we have
	\[ \min_{x \in [x_{i-1},x_i]} f_1(x) \le \min_{x \in [x_{i-1},x_i]} f_2(x) \text{ and } \max_{x \in [x_{i-1},x_i]} f_1(x) \le \max_{x \in [x_{i-1},x_i]} f_2(x) \]
	Thus, $L(P,f_1,\alpha) \le L(P,f_2,\alpha)$ and $U(P,f_1,\alpha) \le U(P,f_2,\alpha)$.
	\begin{align*}
	\int_a^b f_1\ d\alpha 
		& \le U(P,f_1,\alpha) \\
		& \le U(P,f_2,\alpha) \\
		& \le \int_a^b f_2\ d\alpha + \varepsilon
	\end{align*}
	Therefore,
	\[ \int_a^b f_1\ d\alpha \le \int_a^b f_2\ d\alpha \]

	\hrule \vspace{1em}
	\item 
	Let $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\varepsilon > 0$.
	Then, there exists a parition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	Let $c \in (a,b)$.
	Then $P^\ast = P \cup \{c\} = P_1 \cup P_2$ is refinement of $P$ such that $P_1,P_2$ are partition of $[a,c]$ and $[c,b]$ respectively.
	Clearly, $U(P_1,f,\alpha) - L(P_1,f,\alpha) < \varepsilon$ and $U(P_2,f,\alpha) - L(P_2,f,\alpha) < \varepsilon$.
	Therefore, $f \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.\\

	For any two partitions $P_1,P_2$ of $[a,c]$ and $[c,b]$, there exists partition $P^\ast = P_1 \cup P_2$ of $[a,b]$.
	Thus,
	\begin{align*}
	\int_a^b f\ d\alpha 
		& \le U(P^\ast,f,\alpha) \\
		& \le U(P_1,f,\alpha) + U(P_2,f,\alpha) \\
		& \le \int_a^c f\ d\alpha + \int_c^b f\ d\alpha + 2\varepsilon
	\end{align*}
	Since $\varepsilon$ is arbitrary, we have
	\[ \int_a^b f\ d\alpha \le \int_a^c f\ d\alpha + \int_c^b f\ d\alpha \]
	And considering $-f$ instead of $f$, we get
	\[ \int_a^b -f\ d\alpha \le \int_a^c -f\ d\alpha + \int_c^b -f\ d\alpha \]
	Thus,
	\[ \int_a^b f\ d\alpha \ge \int_a^c f\ d\alpha + \int_c^b f\ d\alpha \]
	Therefore,
	\[ \int_a^b f\ d\alpha = \int_a^c f\ d\alpha + \int_c^b f\ d\alpha \]

	\hrule \vspace{1em}
	\item
	Let function $f \in \mathscr{R}(\alpha)$ on $[a,b]$ and $|f| \le M$.
	Let $P$ be any partition of $[a,b]$.
	We have, 
	\[ L(P,f,\alpha) \le \int_a^b f\ d\alpha \le U(P,f,\alpha)\]
	And,
	\[ L(P,f,\alpha) = \sum_{i=1}^n m_i \Delta \alpha_i \ge -M\Delta \alpha_i = -M \sum_{i=1}^n \Delta \alpha_i = -M[\alpha(b)-\alpha(a)] \]
	Similarly,
	\[ U(P,f,\alpha) = \sum_{i=1}^n M_i \Delta \alpha_i \le M\Delta \alpha_i = M \sum_{i=1}^n \Delta \alpha_i = M[\alpha(b)-\alpha(a)] \]
	Therefore,
	\[ \left| \int_a^b f\ d\alpha \right| \le M[\alpha(b)-\alpha(a)] \]
		
	\hrule \vspace{1em}
	\item
	Let $\alpha_1,\alpha_2$ be monotonic functions on $[a,b]$.
	Then $|\alpha_1 + \alpha_2 | \le |\alpha_1| + |\alpha_2|$.
	Let $f \in \mathscr{R}(\alpha_1)$ on $[a,b]$ and $f \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists partitions $P_1,P_2$ of $[a,b]$ such that
	\[ U(P_1,f,\alpha_1) - L(P_1,f,\alpha_1) < \varepsilon \]
	\[ U(P_2,f,\alpha_2) - L(P_2,f,\alpha_2) < \varepsilon \]
	Consider the common refinement $P = P_1 \cup P_2$.
	Then, the inequalities are true for $P$ as well.
	And for each subinterval $[x_{i-1},x_i]$ of $P$, we have $\Delta(\alpha_1+\alpha_2)_i \le \Delta (\alpha_1)_i + \Delta (\alpha_2)_i$.
	Thus,
	\begin{align*}
	U(P,f,\alpha_1+\alpha_2) - L(P,f,\alpha_1+\alpha_2)
		& = \sum_{i=1}^n (M_i-m_i) \Delta (\alpha_1+\alpha_2)_i \\
		& \le \sum_{i=1}^n (M_i - m_i) \Delta \alpha_{1,i} + \sum_{i=1}^n (M_i-m_i) \Delta \alpha_{2,i} \\
		& \le 2\varepsilon
	\end{align*}
	Therefore, $f \in \mathscr{R}(\alpha_1+\alpha_2)$ on $[a,b]$.
	\begin{align*}
	\int_a^b f\ d(\alpha_1+\alpha_2) 
		& \le U(P,f,\alpha_1+\alpha_2) \\
		& \le U(P,f,\alpha_1) + U(P,f,\alpha_2) \\
		& \le \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 + 2\varepsilon
	\end{align*}
	Thus,
	\[ \int_a^b f\ d(\alpha_1+\alpha_2) \le \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]
	Considering $-f$ instead of $f$,we get
	\[ \int_a^b f\ d(\alpha_1+\alpha_2) \ge \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]
	Therefore,
	\[ \int_a^b f\ d(\alpha_1+\alpha_2) = \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]

	\hrule \vspace{1em}

	Let $c>0$ and $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	Clearly, $U(P,f,c\alpha) - L(P,f,c\alpha) < c\epsilon$.
	Therefore, $f \in \mathscr{R}(c\alpha)$ on $[a,b]$.
	\begin{align*}
	\int f d(c\alpha) 
		& \le U(P,f,c\alpha) \\
		& \le cU(P,f,\alpha) \\
		& \le c\int f d\alpha + c\varepsilon
	\end{align*}
	Thus,
	\[ \int f d(c\alpha) \le c \int f d\alpha \]
	Taking $-f$ instead of $f$, we get
	\[ \int f d(c\alpha) \ge c \int f d\alpha \]
	Therefore,
	\[ \int f d(c\alpha) = c \int f d\alpha \]
\end{enumerate}
\end{proof}

\begin{theorem}
If $f,g \in \mathscr{R}(\alpha)$ on $[a,b]$, then
\begin{enumerate}
	\item $fg \in \mathscr{R}(\alpha)$ on $[a,b]$
	\item $|f| \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \left| \int_a^b f\ d\alpha \right| \le \int_a^b |f|\ d\alpha \]
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item
	Let $f,g \in \mathscr{R}(\alpha)$ on $[a,b]$.
	By linearity of the integral we have $f+g \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\phi(t) = t^2$.
	Then $\phi$ is continuous.
	Thus, $\phi \circ f$ is continuous on $[a,b]$.
	Therefore, $\phi \circ f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	{\color{blue}Also we have, $\phi \circ (f+g) = (f+g)^2$ and $\phi \circ (f-g) = (f-g)^2$.
	Thus, $(f+g)^2,(f-g)^2 \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Now, $4fg = (f+g)^2 - (f-g)^2$.}
	Therefore, $4fg \in \mathscr{R}(\alpha)$ on $[a,b]$, from linearity of the integral.
	Take $c = \frac{1}{4}$, we get $fg \in \mathscr{R}(\alpha)$ on $[a,b]$ from linearlity of the integral.\\

	\hrule \vspace{1em}
	\item
	Let $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\phi(t) = |t|$.
	Then $\phi$ is continuous.
	Therefore, $\phi \circ f = |f| \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	{\color{blue}Let $c = \pm 1$ such that
	\[ c\int f d\alpha \ge 0 \]
	Then, 
	\[ \left|\int f d\alpha \right| = c\int f d\alpha = \int cf d\alpha \le \int |f| d\alpha \]
	since $cf \le |f|$. }
\end{enumerate}
\end{proof}

\begin{definition}[step]
	The unit step function, $I : \mathbb{R} \to [0,1]$ is defined by
		\[ I(x) = \begin{cases} 0 & x \le 0 \\ 1 & x > 0 \end{cases} \]
\end{definition}

\begin{theorem}
	Let $f$ be bounded on $[a,b]$ and continuous at $s \in (a,b)$.
	Let $\alpha = I(t-s)$.
	Then,
		\[ \int_a^b f d\alpha = f(s) \]
\end{theorem}
\begin{proof}
	Let $P = \{ a,s,x_2,b \}$ be a partition of $[a,b]$.
	Since $f$ is bounded, 
		\[ U(P,f,\alpha) = M_1 \Delta\alpha_1 + M_2 \Delta \alpha_2+ M_3 \Delta\alpha_3 = M_2 \]
	since $\Delta\alpha_1 = 0$, we have $\Delta\alpha_2 = \alpha(x_2) - \alpha(s) = I(x_2-s) - I(0) = 1-0$ and $\Delta \alpha_3 = 0$.
	Similarly, $L(P,f,\alpha) = m_1\Delta\alpha_1 + m_2\Delta\alpha_2 + m_3\Delta\alpha_3 = m_2$.
	We have,
		\[ m_2 = L(P,f,\alpha) \le \lowint_a^b f d\alpha \le \upint_a^b f d\alpha \le U(P,f,\alpha) = M_2 \]
	We know that, $M_2$ is the maximum value of $f$ in the subinteval $[s,x_2]$.
	The lower sum and upper sum remains the same as we reduce the length of the second subinterval.
	We also know that, $f$ is continuous at $s$.
	As $x_2 \to s$, the maximum value of $f$ tends to the value of $f$ at $s$, say $f(s)$.
	Similarly,  $m_2 \to f(s)$ as $x_2 \to s$.
	Thus,
		\[ f(s) \le \lowint_a^b f d\alpha \le \upint_a^b f d\alpha \le f(s) \]
	Therefore, $f \in \mathscr{R}(\alpha)$ on $[a,b]$ and
		\[ \int_a^b f d\alpha = f(s) \]
\end{proof}

\begin{theorem}
	Suppose $\displaystyle \sum_{i=1}^\infty c_n$ converges and $c_n \ge 0$.
	Let $f$ be continuous on $[a,b]$.
	Let sequence $\sequence{s_n}$ be a strictly increasing sequence in $(a,b)$.
	Let $\displaystyle \alpha = \sum_{i=1}^\infty c_n I(t-s_n)$.
	Then,
	\[ \int_a^b f\ d\alpha = \sum_{i=1}^\infty c_n f(s_n) \]
\end{theorem}
\begin{proof}
	Let $\sum_{n=1}^\infty c_n$ be a convergent series.
	Given $\varepsilon > 0$, there exists a natural number $N$ such that 
	\[ \sum_{n=N+1}^\infty c_n < \varepsilon \]
	Let $s_1,s_2,s_3,\dots$ be distinct points in $(a,b)$.
	Without loss of generality, sequence $\sequence{s_n}$ is a striclty increasing sequence in $[a,b]$.
	Let $\alpha = \sum_{n = 1}^\infty c_n I(t-s_n)$.
	Then, $\alpha(a) = 0$ and $\alpha(b) = \sum_{n=1}^\infty c_n$.
	\[ 0 \le \alpha(x) \le \sum_{n=1}^\infty c_n, \quad \forall x \in [a,b] \]
	By comparison test, $\alpha$ is convergent in $[a,b]$.
	We have,
	\[ \int_a^b f(t)\ d(c_1 I(t-s_1)) = c_1 \int_a^b f(t)\ d(I(t-s_1)) =  c_1 f(s_1) \]
	Let $\alpha = \alpha_1 + \alpha_2$ where $\displaystyle \alpha_1 = \sum_{n=1}^N c_n I(t-s_n)$ and $\displaystyle\alpha_2 = \sum_{n=N+1}^\infty c_n I(t-s_n)$.
	And from mathematical induction, we have
	\begin{align*}
	\int_a^b f(t) d(\alpha_1) 
		& = \int_a^b f(t)\ d\left( \sum_{n=1}^N c_n I(t-s_n) \right)\\
		& = \sum_{n=1}^N \int_a^b f(t)\ d(c_nI(t-s_n)) \\
		& = \sum_{n=1}^N c_n \int_a^b f dI(t-s_n) \\
		& = \sum_{n=1}^N c_n f(s_n) 
	\end{align*}
	We have,
	\[ \left| \int_a^b f(t) d\alpha_2 \right| \le M (\alpha_2(b) - \alpha_2(a)) = M \sum_{n=N+1}^\infty < M\varepsilon \]
	where $M = \sup |f(x)|$.
	Thus,
	\begin{align*}
	\left| \int_a^b fd\alpha_2 \right| 
		& = \left| \int_a^b f d(\alpha_1+\alpha_2) - \int_a^b fd\alpha_1 \right| \\
		& = \left| \int_a^b fd\alpha - \sum_{n=1}^N c_n f(s_n) \right| \\
		& < M\varepsilon 
	\end{align*}
	The inequality is true as $N \to \infty$. 
	In other words, the sequence of partial sums converges to the value of integral of $f$.
	Therefore,
	\[ \sum_{n=1}^\infty c_n f(s_n) = \int_a^b f d\alpha \]
\end{proof}

\begin{theorem}
	Let $\alpha$ be increasing function on $[a,b]$ and $\alpha' \in \mathscr{R}$.
	Let $f$ be bounded real-valued function on $[a,b]$.
	Then $f \in \mathscr{R}(\alpha)$ if and only if $f\alpha' \in \mathscr{R}$.
	And
	\[ \int_a^b f d\alpha = \int_a^b f(x)\alpha'(x) dx \]
\end{theorem}
\begin{proof}
	Since $\alpha' \in \mathscr{R}$ on $[a,b]$, for any $\varepsilon > 0$, there exists a partition $P$ of $[a,b]$ such that $U(P,\alpha')-L(P,\alpha') < \varepsilon$.
	We have, $\alpha$ is continuous on $[a,b]$, as it is differentiable on $[a,b]$.
	Then by intermediate value theorem, we have
	\[ \Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}) = \alpha'(t_i) (x_i-x_{i-1}) \]
	where $t_i \in [x_{i-1},x_i]$.
	Let $s_i \in [x_{i-1},x_i]$.
	Then,
	\[ \sum_{i=1}^n |\alpha'(s_i) - \alpha'(t_i)|\Delta x_i \le U(P,\alpha') - L(P,\alpha') < \varepsilon \]
	Let $M = \sup |f|$.
	\textcolor{blue}{Let $u_i \in [x_{i-1},x_i]$.}
	\footnote{In my opinion, using $u_i$ instead of $s_i$ in the first sum makes it simpler.}
	Consider,
	\begin{align*}
	\left| \sum_{i=1}^n f(u_i) \Delta \alpha_i - \sum_{i=1}^n f(s_i) \alpha'(s_i) \Delta x_i \right| 
		& \le M \left| \sum_{i=1}^n \Delta \alpha_i - \alpha'(s_i) \Delta x_i \right| \\
		& \le M \left| \sum_{i=1}^n \alpha'(t_i) \Delta x_i - \alpha'(s_i) \Delta x_i \right| \\
		& \le M  \sum_{i=1}^n |\alpha'(t_i) - \alpha'(s_i)| \Delta x_i \\
		& < M \varepsilon 
	\end{align*}
	Clearly, the inequality is true independent of the choice of $u_i,s_i \in [x_{i-1},x_i]$.
	Thus, selecting $u_i$ such that $f(u_i) = M_i$, we get
	\[ \left| U(P,f,\alpha) - \sum_{i=1}^n \alpha'(s_i)\Delta x_i \right| < M\varepsilon \]
	Therefore,
	\[ \sum_{i=1}^n f(s_i) \alpha'(s_i) \Delta x_i < U(P,f,\alpha) + M\varepsilon \]
	Selecting $s_i$ such that $f\alpha'$ attains maximum at $s_i$, we get
	\[ U(P,f\alpha') < U(P,f,\alpha) + M\varepsilon \]
	Now, first we select $s_i$ such that $f\alpha(s_i)$ is maximum in $i$th subinterval of $P$.
	Then,
	\[ \left| \sum_{i=1}^n f(u_i) \Delta \alpha_i - U(P,f\alpha') \right| < M\varepsilon \]
	Therefore,
	\[ U(P,f\alpha') < \sum_{i=1}^n f(u_i) \Delta \alpha_i + M\varepsilon \]
	And, now selecting $u_i$ such that $f(u_i) = M_i$.
	\[ U(P,f\alpha') < U(P,f,\alpha) + M\varepsilon \]
	Therefore, we have $U(P,f\alpha') = U(P,f,\alpha)$.
	Clearly, it is true for any refinement of $P$.
	Therefore,
	\[ \upint_a^b f\alpha' dx = \upint_a^b f d\alpha \]
	Similary, by selecting $u_i$ and $s_i$ such that $f(u_i)$, $f\alpha'(s_i)$ is minimum in each subinterval of $P$, we get
	\[ \lowint_a^b f\alpha' dx = \lowint_a^b f d\alpha \]
	Cleary, $f \in \mathscr{R}(\alpha) \iff f\alpha' \in \mathscr{R}$. And,
	\[ \int_a^b f\alpha' dx = \int_a^b f d\alpha \]
\end{proof}

\begin{theorem}[change of variable]
	Let $\varphi : [A,B] \to [a,b]$ be a strictly increasing, continuous function onto $[a,b]$.
	Let $\alpha : [a,b] \to \mathbb{R}$ be an increasing function and $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Define $\beta = \alpha \circ \varphi$ and $g = f \circ \varphi$.
	Then $g \in \mathscr{R}(\beta)$ on $[A,B]$ and 
	\[ \int_A^B g d\beta = \int_a^b f d\alpha \]
\end{theorem}
\begin{proof}
	Let $P = \{x_0,x_1,\dots,x_n\}$ be any partition of $[a,b]$.
	Then there exists a partition $Q = \{ y_0,y_1,\dots,y_n\}$ of $[A,B]$ such that $x_j = \varphi(y_j)$ since $\varphi$ is a continuous, bijection.
	\dag\footnote{Strictly increasing functions are always injective.}
	Similarly, for any partition $Q = \{ y_0,y_1,\dots,y_n\}$ of $[A,B]$, there exists a partition $P = \{ x_0,x_1,\dots,x_n\}$ of $[a,b]$ where $x_j= \varphi(y_j)$.
	Clearly, minimum/maximum of $g$ in $i$th subinterval of $Q$ is same as the minimum/maximum of $f$ in $i$th subinterval of $P$.
	And 
	\[ \Delta \beta_i = \beta(y_i) - \beta(y_{i-1}) = \alpha(\varphi(y_i)) - \alpha(\varphi(y_{i-1})) = \alpha(x_i) - \alpha(x_{i-1}) = \Delta \alpha_i \]
	\begin{align*}
	\min \{ g(y) : y \in [y_{i-1},y_i] \}
		& = \min \{ f(\varphi(y)) : y \in [y_{i-1},y_i] \} \\
		& = \min \{ f(x) : x \in [x_{i-1},x_i] \} \\
	\implies L(Q,g,\beta) 
		& = L(P,f,\alpha) \\
	\max \{ g(y) : y \in [y_{i-1},y_i] \}
		& = \max \{ f(\varphi(y)) : y \in [y_{i-1},y_i] \} \\
		& = \max \{ f(x) : x \in [x_{i-1},x_i] \} \\
		\implies U(Q,g,\beta) & = U(P,f,\alpha) \\
	\end{align*}
	Since $f \in \mathscr{R}(\alpha)$ on $[a,b]$, there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	Therefore, there exists a partition $Q$ of $[A,B]$ such that $U(Q,g,\beta) - L(Q,g,\beta) < \varepsilon$.
	Thus, $g \in \mathscr{R}(\beta)$.
	And
	\[ \int_A^B g d\beta = \int_a^b f d\alpha \]
\end{proof}

\subsection{Integration and Differentiation}
\begin{theorem}
	Let $f \in \mathscr{R}$ on $[a,b]$.
	Let $a \le x \le b$.
	Define
	\[ F(x) = \int_a^x f(t)dt \]
	Then $F$ is continuous on $[a,b]$.
	Furthermore, if $f$ is continuous at $x_0$, then $F$ is differentiable at $x_0$.
	And $F'(x_0) = f(x_0)$.
\end{theorem}
\begin{proof}
	Let $a\le x < y \le b$.
	Let $M = \sup |f|$.
	We have,
	\begin{align*}
	|F(y)-F(x)| 
		& = \left| \int_a^y f(t) dt - \int_a^x f(t) dt \right| \\
		& = \left| \int_a^x f(t) dt + \int_x^y f(t) dt - \int_a^x f(t) dt \right| \\
		& = \left| \int_x^y f(t) dt \right| \\
		& \le M(y-x)
	\end{align*}
	Thus, given $\varepsilon > 0$, there exists $\delta > 0$ such that $|F(y)-F(x)| < \varepsilon$ whenever $|y-x| < \delta \le \frac{\varepsilon}{M}$.
	Therefore, $F$ is continuous on $[a,b]$.\\

	Let $f$ be continuous at $x_0 \in [a,b]$.
	Given $\varepsilon > 0$, there exists $\delta > 0$ such that $|f(t)-f(x_0)| < \varepsilon$ whenever $|t-x_0| < \delta$ and $t \in [a,b]$.\\

	Let $x-\delta < s \le x_0 \le t < x+\delta$.
	Clearly, $|f(u)-f(x_0)| < \varepsilon$ whenever $u \in [s,t]$.
	Consider,
	\begin{align*}
	\left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| 
		& = \left| \frac{1}{t-s} \left( \int_a^t f(u) du - \int_a^s f(u) du \right) - \frac{1}{t-s} f(x_0)(t-s) \right| \\
		& = \left| \frac{1}{t-s} \int_s^t f(u) du  - \frac{1}{t-s} \int_s^t f(x_0)du \right| \\
		& = \left| \frac{1}{t-s} \left( \int_s^t (f(u)-f(x_0)) du \right) \right| \\
		& < \frac{\varepsilon}{t-s} \int_s^t du \quad \text{ since } |f(u)-f(x_0)| < \varepsilon  \\
		& < \varepsilon
	\end{align*}
	Clearly, as $\varepsilon \to 0$, $\delta \to 0$.
	Then, $s,t \to x_0$.
	Therefore, $F$ differentiable at $x_0$ and
	\[ F'(x_0) = \lim_{s,t \to x_0} \frac{F(t)-F(s)}{t-s} =  f(x_0) \]
\end{proof}

\begin{theorem}[fundamental theorem of calculus]
	Let $f \in\mathscr{R}$ on $[a,b]$.
	Let $F$ be a differentiable function on $[a,b]$ such that $F'=f$.
	Then,
	\[ \int_a^b f(x)dx = F(b)-F(a) \]
\end{theorem}
\begin{proof}
	Let $\varepsilon > 0$.
	Let $f \in \mathscr{R}$ on $[a,b]$.
	Then, there exists a partition $P = \{ x_0,x_1,\dots,x_n\}$ of $[a,b]$ such that $U(P,f)-L(P,f) < \varepsilon$.\\

	Let $F$ be differentiable function such that $F'=f$.
	Then, $F$ is continuous.
	By intermediate value theorem, there exists $t_i \in [x_{i-1},x_i]$ such that 
	\[ F(x_i) - F(x_{i-1}) = F'(t_i) (x_i-x_{i-1}) = f(t_i)\Delta x_i \]
	Clearly,
	\[ F(b) - F(a) = \sum_{i=1}^n \left( F(x_i) - F(x_{i-1}) \right) = \sum_{i=1}^n f(t_i) \Delta x_i \]
	Since $t_i \in [x_{i-1},x_i]$, we have $m_i \le f(t_i) \le M_i$.
	Thus,
	\[ \left| \int_a^b f(x) dx - \left(F(b)-F(a)\right) \right| = \left| \int_a^b f(x) dx - \sum_{i=1}^n f(t_i) \Delta x_i \right| < \varepsilon \]
	Therefore, 
	\[ \int_a^b f(x) dx = F(b) - F(a) \]
\end{proof}

\begin{theorem}[integration by parts]
	Let $F,G$ be differentiable $[a,b]$.
	Let $F' = f \in \mathscr{R}$ and $G' = g \in \mathscr{R}$.
	Then,
	\[ \int_a^b F(x)g(x) dx = F(b)G(b) - F(a)G(a) - \int_a^b f(x)G(x) dx \]
\end{theorem}
\begin{proof}
	Let $F,G$ be differentiable functions on $[a,b]$ and $F'=f \in \mathscr{R}$ and $G'=g \in \mathscr{R}$.
	Let $H = FG$. 
	Then, $H' = FG' + F'G = Fg + fG$.
	\[ \int_a^b H'(x) dx = \int_a^b F(x)g(x) dx + \int_a^b f(x)G(x) dx \]
	By fundamental theorem of calculus, we also have
	\[ \int_a^b H'(x) = H(b) - H(a) = FG(b) - FG(a) = F(b)G(b) - F(a)G(a) \]
	Rearranging the terms, we get
	\[ \int_a^b F(x)g(x) dx = F(b)G(b) - F(a)G(a) - \int_a^b f(x)G(x) dx \]
\end{proof}

\subsection{Integration of Vector-valued Functions}
\begin{definition}[integrable]
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$ be a vector-valued function.
	Let $\alpha$ be a monotonic function on $[a,b]$.
	Let $f_1,f_2,\dots,f_k$ be the component functions of $\bar{f}$.
	That is, $\bar{f}(x) = \left( f_1(x),f_2(x),\dots,f_k(x) \right)$.
	Then $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if every component function $f_j \in \mathscr{R}(\alpha)$.
\end{definition}
\begin{important}
	In other words, vector-valued function $\bar{f}$ is integrable if and only if every component function of $\bar{f}$ is integrable.
\end{important}

\begin{commentary}
\begin{theorem}[properties]
	Suppose $\bar{f},\bar{g}$ be vector-valued functions from $[a,b]$ into $\mathbb{R}^k$.
\begin{enumerate}
	\item Let $\bar{f},\bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then $\bar{f} + \bar{g} \in \mathscr{R}(\alpha)$.
	And,
	\[ \int_a^b \bar{f} + \bar{g} d\alpha = \int_a^b \bar{f} d\alpha + \int_a^b \bar{g} d \alpha \]
	If $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$ and $c \in \mathbb{R}$, then $c\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \int_a^b c\bar{f} d\alpha = c \int_a^b \bar{f} d\alpha \]
	\item Let $c \in (a,b)$.
	Let $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then $\bar{f} \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.
	And,
	\[ \int_a^b \bar{f} d\alpha = \int_a^c \bar{f} d\alpha + \int_c^b \bar{f} d\alpha \]
	\item Let $\alpha_1,\alpha_2$ be monotonic functions on $[a,b]$.
	Let $\bar{f} \in \mathscr{R}(\alpha_1)$ on $[a,b]$ and $\bar{f} \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	Then, $\bar{f} \in \mathscr{R}(\alpha_1+\alpha_2)$ on $[a,b]$.
	And,
	\[ \int_a^b \bar{f}d(\alpha_1+\alpha_2) = \int_a^b \bar{f} d\alpha_1 + \int_a^b \bar{f} d\alpha_2 \]
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item
	Let $\bar{f},\bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$ where $\bar{f} = (f_1,f_2,\dots, f_k)$ and $\bar{g} = (g_1,g_2,\dots, g_k)$.
	By definition of integrability of vector-valued functions, the component functions $f_j,g_j \in \mathscr{R}(\alpha)$ on $[a,b]$ for $1 \le j \le k$.\\

	We also know that, if $f_j, g_j \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_j + g_j \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Thus, $f_1+g_1, f_2+g_2,\dots, f_k + g_k \in \mathscr{R}(\alpha)$ on $[a,b]$ for $1 \le j \le k$.
	Therefore, $\bar{f}+\bar{g} = (f_1+g_1,f_2+g_2,\dots, f_k+g_k) \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	\hrule \vspace{1em}

	Let $c \in \mathbb{R}$.
	Let $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then, $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha)$ on $[a,b]$.
	We know that, $cf_1 \in \mathscr{R}(\alpha)$ on $[a,b]$, since $f_1 \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Thus, $cf_1,cf_2,\dots,cf_k \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Therefore, $c\bar{f} = (cf_1,cf_2,\dots,cf_k) \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	\hrule \vspace{1em}
	\item
	Let $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then, $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $c \in (a,b)$.
	We know that, if $f_j \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_j \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.
	Thus, $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.
	Therefore, $\bar{f} \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.\\

	\hrule \vspace{1em}
	\item
	Let $\bar{f} \in \mathscr{R}(\alpha_1)$ and $\bar{f} \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	Then $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha_1)$ and $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	We know that, if $f_j \in \mathscr{R}(\alpha_1)$ and $f_j \in \mathscr{R}(\alpha_2)$ on $[a,b]$, then $f_j \in \mathscr{R}(\alpha_1+\alpha_2)$ on $[a,b]$.
\end{enumerate}
\end{proof}
\end{commentary}
\begin{challenge}
	Let $\bar{f},\bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$ where $\bar{f},\bar{g} : [a,b] \to \mathbb{R}^k$.
	Define $\bar{f} \cdot \bar{g} : [a,b] \to \mathbb{R}$ by $(\bar{f} \cdot \bar{g})(x) = f_1(x)g_1(x) + f_2(x)g_2(x) + \dots + f_k(x)g_k(x)$.
	Then, $\bar{f} \cdot \bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \int_a^b \bar{f} \cdot \bar{g} d\alpha = \left( \int_a^b \bar{f} d\alpha \right) \cdot \left( \int_a^b \bar{g} d\alpha \right) \]
\end{challenge}

\begin{commentary}
\begin{theorem}
	Let $\alpha$ be a monotonic function such that $\alpha' \in \mathscr{R}$ on $[a,b]$.
	Let $\bar{f}$ be a bounded function on $[a,b]$.
	Then $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if $f\alpha' \in \mathscr{R}$ on $[a,b]$.
	And,
		\[ \int_a^b \bar{f} d\alpha = \int_a^b \bar{f}\alpha' dx \]
\end{theorem}
\begin{proof}
	Let $\bar{f} = (f_1,f_2,\dots,f_k)$.
	Then, $\bar{f}\alpha' = (f_1\alpha', f_2\alpha',\dots,f_k\alpha')$.\\
	We already know that, $f_j \in \mathscr{R}(\alpha) \iff f_j\alpha' \in \mathscr{R}$.\\
	Therefore,
	\begin{align*}
	\bar{f} \in \mathscr{R}(\alpha) 
		& \iff f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha) \\
		& \iff f_1\alpha', f_2\alpha', \dots, f_k\alpha' \in \mathscr{R} \\
		& \iff \bar{f}\alpha' \in \mathscr{R}
	\end{align*}
	Thus, $\bar{f} \in \mathscr{R}(\alpha) \iff \bar{f}\alpha' \in \mathscr{R}$.\\

	\hrule \vspace{1em}
	We also know that,
		\[ \int_a^b f_j d\alpha = \int_a^b f_j\alpha' dx \]
	Thus,
	\begin{align*}
	\int_a^b \bar{f} d\alpha 
		& = \left( \int_a^b f_1 d\alpha, \int_a^b f_2 d\alpha, \dots,\int_a^b f_k d\alpha \right) \\
		& = \left( \int_a^b f_1\alpha' dx, \int_a^b f_2\alpha' dx,\dots, \int_a^b f_k\alpha' dx \right)\\
		& = \int_a^b \bar{f}\alpha' dx
	\end{align*}
\end{proof}
\end{commentary}

\begin{commentary}
\begin{theorem}
	Let $\bar{f} \in \mathscr{R}$ on $[a,b]$ where $\bar{f} : [a,b] \to \mathbb{R}^k$.
	Define $\bar{F} : [a,b] \to \mathbb{R}^k$ defined by
		\[ \bar{F}(x) = \int_a^x \bar{f}(t) dt \]
	Then $F$ is continuous on $[a,b]$.
	Furthermore, if $\bar{f}$ is continuous at $x_0 \in [a,b]$, then $\bar{F}$ is differentiable at $x_0$ and $\bar{F}'(x_0) = \bar{f}(x_0)$.
\end{theorem}
\begin{proof}
	We know that, $\bar{f} = (f_1,f_2,\dots,f_k)$.
	And from the definition of integral, we have
		\[ \int_a^b \bar{f}(t) dt = \left( \int_a^b f_1(t) dt, \int_a^b f_2(t) dt,\dots, \int_a^b f_k(t) dt \right) \]
	We know that, for $1 \le j \le k$, the function $F_j : [a,b] \to \mathbb{R}$ defined by 
		\[ F_j(x) = \int_a^x f_j(t) dt \]
	is continuous.
	And if $f_j$ is continuous at $x_0$, then $F_j$ is differentiable at $x_0$ and $F_j'(x_0) = f_j(x_0)$.
	Clearly, $\bar{F} = (F_1,F_2,\dots,F_k)$ is continuous, since each component function is continuous.
	And, $\bar{F}$ is differentiable at $x_0$ and
		\[ \bar{F}'(x_0) = \left( F_1'(x_0),F_2'(x_0),\dots,F_k'(x_0) \right) = (f_1(x_0),f_2(x_0),\dots,f_k(x_0)) = \bar{f}(x_0) \]
\end{proof}
\end{commentary}

\begin{theorem}[fundamental theorem of calculus for vector-valued functions]
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$.
	Let $\bar{F} : [a,b] \to \mathbb{R}^k$.
	If $\bar{f} \in \mathscr{R}$ on $[a,b]$ and $\bar{F}' = \bar{f}$, then
		\[ \int_a^b \bar{f}(t) dt = \bar{F}(b) - \bar{F}(a) \]
\end{theorem}
\begin{proof}
By fundamental theorem of calculus, we have
	\[ \int_a^b f_j(t) dt = F_j(b) - F_j(a) \]
 	for $1 \le j \le k$.
Therefore,
\begin{align*}
	\int_a^b \bar{f}(t) dt
		& = \left( \int_a^b f_1(t) dt, \int_a^b f_2(t) dt, \dots, \int_a^b f_k(t) dt \right) \\
		& = \left( F_1(b)-F_1(a), F_2(b) - F_2(a), \dots, F_k(b) - F_k(a) \right) \\
		& = \left( F_1(b),F_2(b),\dots,F_k(b)\right) - \left(F_1(a), F_2(a),\dots,F_k(a)\right) \\
		& = \bar{F}(b) - \bar{F}(a)
\end{align*}
\end{proof}

\begin{theorem}
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$.
	If $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$, then $|\bar{f}| \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And
	\[ \left| \int_a^b \bar{f} d\alpha \right| \le \int_a^b |\bar{f}| d\alpha \]
\end{theorem}
\begin{proof}
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$.
	Then, we have $\bar{f} = (f_1,f_2,\dots,f_k)$.\\
	Suppose $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then, $f_j \in \mathscr{R}(\alpha)$ on $[a,b]$ for $1 \le j \le k$.
	We have, $|\bar{f}| = \left( f_1^2+f_2^2+\dots+f_k^2 \right)^\frac{1}{2}$.
	We know that if $f_j \in \mathscr{R}(\alpha)$, then $f_j^2 \in \mathscr{R}(\alpha)$.
	Again, $\sum f_j^2 \in \mathscr{R}(\alpha)$.\\

	Consider $g : [a,b] \to \mathbb{R}$ given by $g(x) = \sum_{j=1}^k f_j^2(x)$.
	\textcolor{blue}{We have $g \in \mathscr{R}(\alpha)$ on $[a,b]$.}
	Thus $g$ is bounded and there exists $m,M$ such that $m \le g \le M$.
	Clearly, $g \ge 0$.\\

	Consider, the function $\phi : [m,M] \to \mathbb{R}$ given by $\phi(x) = \sqrt{x}$.
	Clearly, $\phi$ is well-defined on $[m,M]$ since $0 \le m$.
	And, $\phi$ is continuous on $[m,M]$.
	Thus, $|\bar{f}| = \phi \circ g = \sqrt{\sum f_j^2} \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	\hrule \vspace{1em}
	
	Let $\displaystyle \bar{y} = \int_a^b \bar{f} d\alpha$.
	Then, $\bar{y} = (y_1,y_2,\dots,y_k)$ and $\displaystyle y_j = \int_a^b f_j d\alpha$.
	\begin{align*}
	|\bar{y}|^2 
		& = y_1^2 + y_2^2 + \dots + y_k^2\\
		& = y_1 \int_a^b f_1 d\alpha + y_2 \int_a^b f_2 d\alpha + \dots + y_k \int_a^b f_k d\alpha \\
		& = \int_a^b (y_1f_1 + y_2f_2 + \dots + y_kf_k) d\alpha
	\end{align*}
	By Schwarz inequality,
		\[ \sum_{j=1}^n y_j f_j \le \left( \sum_{j=1}^n y_j^2 \right)^\frac{1}{2} \left( \sum_{j=1}^n f_j^2 \right)^\frac{1}{2} = |\bar{y}| \ |\bar{f}| \]
	Thus,
		\[ {\color{blue}|\bar{y}|^2} = \int_a^b \left( \sum_{j=1}^k y_jf_j \right) d\alpha \le \int_a^b |\bar{y}|\ |\bar{f}| d\alpha = |\bar{y}| \int_a^b |\bar{f}| d\alpha \]
	Therefore,
		\[ \left| \int_a^b \bar{f} d\alpha \right| = |\bar{y}| \le \int_a^b |\bar{f}| d\alpha \]

\end{proof}

\pagebreak

%\chapter{Sequence \& Series of Functions}
{\Large Module 3}
\section{Sequence and Series of functions}
\begin{definition}
	Let sequence $\sequence{f_n}$ be a sequence of functions defined on $E$.
	Suppose sequence $\sequence{f_n(x)}$ converges forevery $x \in E$.
	Then, sequence $\sequence{f_n}$ converges.
	And \textbf{limit function} $f : E \to \mathbb{R}$ is defined by
	\[ f(x) = \lim_{n \to \infty} f_n(x) \]
\end{definition}
\begin{definition}
	Let $f_n : E \to \mathbb{R},\ \forall n \in \mathbb{N}$.
	Suppose $\sum f_n(x)$ converges for every $x \in E$.
	Then, series $\sum f_n$ converges.
	And \textbf{sum} $f : E \to \mathbb{R}$ is defined by
	\[ f(x) = \sum_{n=1}^\infty f_n(x) \]
\end{definition}

\subsection{Counter Examples - Illustrating Main problem}
\textbf{Main Problem} : Can we obtain the sufficient conditions for preserving desirable properties under convergence ?
\subsubsection*{Interchanging limits}
\[ \text{Generally,}\quad \lim_{n \to \infty} \lim_{m \to \infty} S_{n,m} \ne \lim_{m \to \infty} \lim_{n \to \infty} S_{n,m} \]
\begin{proof}
Consider, $S_{n,m} = \frac{m}{m+n}$.
\begin{align*}
	\lim_{m \to \infty} \lim_{n \to \infty} S_{n,m} & = \lim_{m \to \infty } 0 = 0 \\
	\lim_{n \to \infty} \lim_{m \to \infty} S_{n,m} & = \lim_{n \to \infty } 1 = 1 
\end{align*}
\end{proof}
\subsubsection{Continuity}
\[ \sequence{f_n} \to f,\ f_n \text{ continuous } \not\!\!\!\implies f \text{ continuous} \]
\begin{proof}
Consider $f_n(x) = \frac{x^2}{(1+x^2)^n}$.
	Clearly, $f_n : \mathbb{R} \to \mathbb{R}$ is continuous for every natural number $n$.\\

\textbf{Case 1 : $x = 0$}\\
Suppose $x = 0$.
Then, $f_n(0) = 0$ and therefore $\displaystyle\lim_{n \to \infty} f_n(0) = 0$. \\

\textbf{Case 2 : $x \ne 0$}\\
	Suppose $x \ne 0$.
	Then $\frac{1}{1+x^2} < 1,\ \forall x \in \mathbb{R},\ (x \ne 0)$.\\
Therefore,
\[ f(x) = \lim_{n \to \infty} f_n(x) = x^2 \lim_{n \to \infty} \left(\frac{1}{1+x^2}\right)^n = x^2 \frac{1}{1-\frac{1}{1+x^2}} = x^2 \frac{1+x^2}{x^2} = 1+x^2 \]
	Clearly from cases $1$ and $2$,\\
	\[ f : \mathbb{R} \to \mathbb{R},\ f(x)= \begin{cases} 0 & x = 0 \\ 1+x^2 & x \ne 0 \end{cases} \quad \text{ is not continuous at } 0 \]
\end{proof}
\subsubsection{integrability} 
\[ \sequence{f_n} \to f, f_n \in \mathscr{R} \not\!\!\!\implies f \in \mathscr{R} \]
\begin{proof}
Consider, $\displaystyle f_m(x) = \lim_{n \to \infty} (\cos m!\pi x)^{2n}$.\\

Suppose $m!x$ is not an integer.
Then $\displaystyle f_m(x) = \lim_{n \to \infty} (y^2)^n = 0$ since $-1<y = \cos m! \pi x <1$.
Suppose $m!x$ is an integer.
Then $\cos m! \pi x = \pm 1$ and $\displaystyle f_m(x) = \lim_{n \to \infty} ((\pm 1)^2)^n = 1$.\\

We know that, if $x \in \mathbb{Q}$, then $x = \pm \frac{p}{q}$ where $p,q \in \mathbb{N}$.
Clearly, for any $m > q$, $m! x$ is an integer and $f_m(x) =1 $ for any $m > q$.
Therefore, $\displaystyle \lim_{m \to \infty} f_m(x) = 1$ if $x \in \mathbb{Q}$.
And $\displaystyle \lim_{m \to \infty} f_m(x) = 0$ if $x \notin \mathbb{Q}$ since $m! x$ is not an integer for any $m \in \mathbb{N}$ and $f_m(x) = 0,\ \forall m \in \mathbb{N}$.\\

Now, $f : \mathbb{R} \to \mathbb{R}$ defined by $\displaystyle f(x) = \lim_{m \to \infty} f_m(x)$ is given by,
	\[ f(x) = \begin{cases} 1 & x \in \mathbb{Q} \\ 0 & x \notin \mathbb{Q} \end{cases} \text{ which is \textbf{everywhere discontinuous.}} \]
	Clearly, $f$ is not Riemann integrable since $\mathbb{Q}, \mathbb{R}-\mathbb{Q}$ are dense in any subinterval.
	Thus, $m_i = 0$ and $M_i = 1$ in any subinterval of any partition $P$.
	\[ \lowint_0^1 f(x)\ dx = 0 \ne 1 = \upint_0^1 f(x)\ dx \]

\begin{important}
Let $m \in \mathbb{N}$.
Consider closed interval $E = [-1,1]$.
Then there exists finitely many rational numbers $x$ on $E$ such that $m!x$ is an integer.
Thus, $f_m(x)$ has at most finitely many discontinuities in any bounded interval. 
\end{important}
\begin{commentary}
$f_m(x) = \cos m! \pi x$ is discontinuous at $S = \left\{ \frac{k}{m!} \in E : k \in \mathbb{Z} \right\}$.
For example, suppose $m = 3$.
Then $S = \left\{ 0,\pm\frac{1}{6},\pm\frac{2}{6},\pm\frac{3}{6},\pm\frac{4}{6},\pm\frac{5}{6},\pm 1 \right\}$.
\end{commentary}
Let $\alpha$ be the identity function.
Then $\alpha$ is continuous at those finite points where $f_m$ is discontinuous.
And $f_m$ are bounded functions, since $|f_m| \le 1$.
Therefore, for any bounded interval $E$, $f_m$ is Riemann integrable on $E$ for each $m$.
However, $f$ is not Riemann integrable.
\end{proof}
\subsubsection{Derivative}
\[ \sequence{f_n} \to f \not\!\!\!\implies \sequence{f_n'} \to f' \]
\begin{proof}
	Consider $f_n(x) = \frac{\sin nx}{\sqrt{n}}$.
	\[ f(x) = \lim_{n \to \infty} f_n(x) = 0,\ \forall x \]
	However, $f_n'(x) = \sqrt{n} \cos nx$.
	And $f'(x) = 0$.
	\[ \lim_{n \to \infty} \frac{d}{dx} f_n(x) = \infty \ne f'(x) = \frac{d}{dx} \lim_{n \to \infty} f_n(x),\ \forall x \]
	Clearly, convergence doesn't preserve derivatives.
\end{proof}
\subsubsection{Integral}
\[ \sequence{f_n} \to f \not\!\!\!\implies \sequence{ \int f_n} \to \int f \]
\begin{proof}
	Consider $f_n : [0,1] \to \mathbb{R}$ defined by $f_n(x) = n^2 x (1-x^2)^n$.\\

	We have,
	\[ \lim_{n \to \infty} \frac{n^\alpha}{(1+p)^n} = 0 \]
	Let $\frac{1}{1+p} = 1-x^2$, then $1+p = \frac{1}{1-x^2}$ and $p = \frac{x^2}{1-x^2} > 0$.\\
	Now, we have
	\[ f(x) = \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} n^2 x(1-x^2)^n = x \lim_{n \to \infty} \frac{n^2}{\left(1+\frac{x^2}{1-x^2}\right)^n} = 0 \]
	And, we have
	\[ \int_0^1 f(x)\ dx = 0 \]
	However,
	\[ \int_0^1 f_n(x)\ dx = n^2 \int_0^1 x(1-x^2)^n\ dx = n^2 \int_0^1 \frac{u^n}{2}\ du = n^2 \left[ \frac{u^{n+1}}{2(n+1)} \right]_0^1 = \frac{n^2}{2n+2} \]
	Clearly, 
	\[ \lim_{n \to \infty} \int_0^1 f_n(x)\ dx = \lim_{n \to \infty} \frac{n^2}{2n+2} = \infty \ne 0 = \int_0^1 f(x)\ dx = \int_0^1 \lim_{n \to \infty} f_n(x)\ dx \]
\end{proof}
\subsection{Uniform Convergence}
Uniform convergence is a partial solution to our main problem.
\begin{definition}
	Let sequence $\sequence{f_n}$ be a sequence of functions on $E$.
	Then $\sequence{f_n}$ converges uniformly to limit function $f$ if for any $\varepsilon > 0$, there exists a natural number $N$ such that for any $n > N$ and $x \in E$, $|f_n(x) - f(x)| \le \varepsilon$.
\end{definition}
\begin{important}
	\[ \forall \varepsilon > 0,\ \exists N \in \mathbb{N},\ \forall n > N,\ \forall x \in E,\ |f_n(x) - f(x)| \le \varepsilon \]
\end{important}
\begin{commentary}
	The difference between pointwise convergence and uniform convergence is that the choice of natrual number $N$ is independent of the choice of $x$ in the case of uniform convergence.
\end{commentary}
\subsubsection{Criterion for Uniform Convergence}
\begin{theorem}[Cauchy criterion]
	The sequence of functions $\sequence{f_n}$ defined on $E$ converges uniformly on $E$ if and only if for any $\varepsilon > 0$, there exists a natural number $N$ such that for any $m,n \ge N$ and $x \in E$, $|f_n(x)-f_m(x)| \le \varepsilon$.
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Let $\varepsilon > 0$.
	Then there exists $N \in \mathbb{N}$ such that $\forall n,m > N$ and $\forall x \in E$, $|f_n(x) - f(x)| \le \frac{\varepsilon}{2}$ and $|f_m(x)-f(x)| \le \frac{\varepsilon}{2}$.
	Therefore, $\forall \varepsilon > 0$ we have, $N \in \mathbb{N}$ such that $\forall n,m > N$ and $\forall x \in E$,
	\[ |f_n(x) - f_m(x)| \le |f_n(x)-f(x)| + |f_m(x)-f(x)| \le \varepsilon \]

	Let $\varepsilon > 0$.
	Suppose there exists $N \in \mathbb{N}$ such that $\forall n,m > N$ and $\forall x \in E$,
	\[ |f_n(x) - f_m(x)| \le |f_n(x)-f(x)| + |f_m(x)-f(x)| \le \varepsilon \]
	By Cauchy criterion, $\sequence{f_n} \to f$ pointwise on $E$.
	It remains to prove that this convergence is uniform on $E$.
	Clearly, above inequality is true for any value of $m$ greater than $N$.
	As $m \to \infty$ we have,
	\begin{align*}
		\lim_{m \to \infty} |f_n(x) - f_m(x)|  \le \varepsilon \\
		\implies \left|f_n(x) - \lim_{m \to \infty} f_m(x)\right| = |f_n(x) - f(x) | & \le \varepsilon
	\end{align*}
	Clearly, the convergence is uniform.
	That is, $\sequence{f_n} \to f$ uniformly on $E$.
\end{proof}

\begin{theorem}[Supremum Test]
	Suppose sequence of function $\sequence{f_n} \to f$ pointwise on $E$.
	Suppose $\displaystyle M_n = \sup_{x \in E} \left|f_n(x) - f(x)\right|$.
	Then $\sequence{f_n} \to f$ uniformly on $E$, if and only if sequence $\sequence{M_n} \to 0$.
\end{theorem}
\begin{proof}
	Define $\displaystyle M_n = \sup_{x \in X} |f_n(x) - f(x)|$.
	Clearly, $M_n \ge 0$.
	Suppose $\sequence{M_n} \to 0$ on $E$.
	Let $\varepsilon > 0$.
	Then there exists $N \in \mathbb{N}$ such that for every $n > N$,  we have $|M_n - 0| = M_n \le \varepsilon$.
	Thus for any $n \ge N$ and $x \in E$,
	\[ |f_n(x) - f(x)| \le \sup |f_n(x) - f(x)| = M_n \le \varepsilon \]
	Therefore, $\sequence{f_n} \to f$ uniformly on $E$.\\

	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Let $\varepsilon > 0$.
	Then, there exists $N \in \mathbb{N}$ such that for every $n \ge N$ and $x \in E$, $|f_n(x) -f(x)| \le \varepsilon$.
	This inequality is true for every $x \in E$.
	Thus,
	\[ \sup_{x \in E} \left| f_n(x) - f(x) \right| = M_n \le \varepsilon,\quad \forall n \ge N \]
	Therefore, $\sequence{M_n} \to 0$ on $E$.
\end{proof}

\begin{theorem}[Weierstrass M-test]
	Suppose $\sequence{f_n}$ is a sequence of functions on $E$.
	Suppose $|f_n(x)| \le M_n$.
	Then $\sum f_n$ converges on $E$ if $\sum M_n$ converges.
\end{theorem}
\begin{proof}
	Suppose $|f_n| \le M_n$.
	Then $M_n \ge 0$.
	Suppose $\sum M_n$ converges.
	Let $\sequence{s_n}$ be the sequence of partial sums.
	Let $\varepsilon > 0$.
	By Cauchy criterion, there exists $N \in \mathbb{N}$ such that $\forall n,m \ge N$, $|s_n - s_m| \le \varepsilon$.
	Thus,
	\[ |s_n - s_m| = \left| \sum_{j = 1}^n M_j - \sum_{j = 1}^m M_j \right| =  \sum_{j = n+1}^m M_j \le \varepsilon,\quad ( m > n )\]
	Let $\sequence{S_n}$ be the sequence of partial sums for $\sum f_n$.
	Clearly, $\forall n,m \ge N$ and $\forall x \in E$, we have $|S_n-S_m| \le \varepsilon$ since
	\[ |S_n - S_m| = \left|\sum_{j = 1}^n f_j(x) - \sum_{j = 1}^m f_j(x) \right| = \left| \sum_{j = n+1}^m f_j(x) \right| \le \sum_{j = n+1}^m M_j \le \varepsilon \]
	By Cauchy criterion for uniform convergence, the sequence of partial sums, $\sequence{S_n}$ converges uniformly on $E$.
	Therefore, the series of functions, $\sum f_n$ converges uniformly on $E$.
\end{proof}

\subsection{Uniform Convergence and Continuity}
\begin{theorem} %7.11
	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Let $x$ be a limit point of $E$.
	Suppose $\displaystyle \lim_{t \to x} f_n(t) = A_n$.
	Then $\sequence{A_n}$ converges, and 
	$\displaystyle \lim_{t \to x} f(t) = \lim_{n \to \infty} A_n$.
	In other words,
	\[ \lim_{t \to x} \lim_{n \to \infty} f_n(t) = \lim_{n \to \infty} \lim_{t \to x} f_n(t) \]
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Suppose $\displaystyle \lim_{t \to x} f_n(t)  = A_n$ on $E$.
	Let $\varepsilon > 0$.
	By Cauchy criterion for uniform convergence, there exists $N \in \mathbb{N}$ such that for every $n \ge N$ and every $t \in E$, $|f_n(t)-f_m(t)| \le \varepsilon/3$.
	As $t \to x$, we have $|A_n - A_m| \le \varepsilon/3$.
	Clearly, $\sequence{A_n}$ is Cauchy and $\sequence{A_n} \to A$.\\

	Now fix a natural number $N$ such that for every $n \ge N$ and $t \in E$, $|f_n(t) - f(t) | < \varepsilon /3$ and $|A_n-A| \le \varepsilon/3$.
	Also, choose a neighbourhood $V$ of $x$ such that $|f_n(t) - A_n | \le \varepsilon/3$.
	Then, $\forall x \in V \cap E$ we have
	\[ |f(t) - A| \le |f(t) -f_n(t)| + |f_n(t)-A_n| + |A_n - A| \le \varepsilon \]
	Therefore, $\displaystyle \lim_{t \to x} f(t) = A$.
	\[ \lim_{t \to x} \lim_{n \to \infty} f_n(t) = \lim_{t \to x} f(t)  = \lim_{n \to \infty} A_n = \lim_{n \to \infty} \lim_{t \to x} f_n(t) \]
\end{proof}

\begin{theorem} %7.12
	If $\sequence{f_n}$ is a sequence of continuous functions on $E$, and if $f_n \to f$ uniformly on $E$, then $f$ is continuous on $E$.
\end{theorem}
\begin{important}
	Uniform convergence preserves continuity.
\end{important}
\begin{proof}
	Suppose the sequence of continuous functions, $\sequence{f_n} \to f$ uniformly on $E$.
	Since $f_n$ is continuous $\lim_{t \to x} f_n(t) = f_n(x),\ \forall x \in E$.
	Since the convergence is uniform, the order of limits can be interchanged.
	\[ f(x) = \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \lim_{t \to x} f_n(t) = \lim_{t \to x} \lim_{n \to \infty} f_n(t) = \lim_{t \to x} f(t) \]
	Therefore, the limit function $f$ is continuous.
\end{proof}
\begin{remark}
	Suppose a sequence of continuous functions $\sequence{f_n}$ converges to a function $f$.
	Function $f$ being continuous doesn't imply that the convergence is uniform.
\end{remark}
\begin{proof}
	Consider $f_n : (0,1) \to \mathbb{R}$ defined by $f_n(x) = \frac{1}{nx+1}$.
	Then, $\sequence{f_n} \to 0$.
	Clearly, $f_n,0$ are continuous functions on $(0,1)$.
	However, the convergence is not uniform.
\end{proof}

\begin{theorem} %7.13
	Suppose $K$ is compact, and
	\begin{enumerate}
		\item $\sequence{f_n}$ is sequence of continuous functions on $K$
		\item $\sequence{f_n}$ converges pointwise to a continuous function $f$ on $K$.
		\item $f_n(x) \ge f_{n+1}(x),\ \forall x \in K$
	\end{enumerate}
	Then, $\sequence{f_n} \to f$ uniformly on $K$.
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n} \to f$ pointwise on $K$.
	Let function $g_n = f_n - f$.
	Then, $g_n$ is continuous, $\sequence{g_n} \to 0$ pointwise and $g_n \ge g_{n+1}$.
  	If sequence $\sequence{g_n} \to 0$ uniformly, then sequence $\sequence{f_n} \to f$ uniformly.\\

	Let $\varepsilon > 0$.
	Let $K_n$ be the set of all points $x \in K$ such that $g_n(x) \ge \varepsilon$.
	Then $K_n$ is closed, since $g_n$ is continuous.
	And $K_n$ is compact since $K_n$ is a closed subset of a compact set $K$.\\

	Suppose $x \in K_{n+1}$.
	Then $g_{n+1}(x) \ge \varepsilon$.
	We have, $g(x) \ge g_{n+1}(x)$.
	Thus, $g(x) \ge \varepsilon$.
	Therefore, $K_n \supset K_{n+1} \supset \dots$.\\

	Let $x \in K$.
	We have $\sequence{g_n(x)} \to 0$ pointwise.
	Then there exists $N \in \mathbb{N}$ such that $\forall n > N$, $g_n(x) < \varepsilon$.
	Thus, $x \notin K_n,\ \forall n \ge N$.
	Therefore $\cap K_n$ is empty.\\

	Clearly, $K_N$ is empty for some $N \in \mathbb{N}$.
	Thus, $\forall n \ge N$ and $\forall x \in K$ we have, $0 \le g_n(x) < \varepsilon$.
	In other words, $\sequence{g_n}$ converges to $0$ uniformly on $K$.
	Therefore, $\sequence{f_n}$ converges to $f$ uniformly on $K$.
\end{proof}

\begin{definition} %7.14
	Let $X$ be a metric space.
	Let $\mathscr{C}(X)$ be the set of all complex valued, continuous, bounded functions on $X$.
	Let $f \in \mathscr{C}(X)$.
	Then \textbf{supremum norm} on $\mathscr{C}(X)$ is defined by
	\[ \| f \| = \sup_{x \in X} |f(x)| \]
	And, $\mathscr{C}(X)$ together with \textbf{distance function} $d : \mathscr{C}(X) \times \mathscr{C}(X) \to \mathbb{R}$ defined by $d(f,g) = \| f-g \|$ is a metric space.
\end{definition}

\begin{commentary}
	Let $\mathscr{C}(X)$ be the set of all complex valued, continuous, bounded functions on metric space $X$.
	Let $f \in \mathscr{C}(X)$.
	Then $f$ is bounded, $|f| < \infty$.
	Thus, $\sup |f(x)|$ exists.
	Therefore, $\| f \|$ is well-defined.\\

	And, $\| f \| = \sup |f(x)| \ge 0$ since $|f(x)| \ge 0$.
	Let $f,g \in \mathscr{C}(X)$.
	Then $h = f+g \in \mathscr{C}(X)$ since sum of continuous functions is functions and sum of bounded functions is bounded.
	Also, we have
	\[ \| h \| = \sup_{x \in X} |(f+g)(x)| \le \sup_{x \in X} |f(x)| + \sup_{x \in X} |g(x)| = \| f \| + \| g \| \]
	Therefore, $\| . \| : \mathscr{C}(X) \to \mathbb{R}$ is a norm on $\mathscr{C}(X)$.\\

	Consider the function $d : \mathscr{C}(X) \times \mathscr{C}(X) \to \mathbb{R}$ defined by $d(f,g) = \| f-g \|$.
	Clearly, $d(f,g) \ge 0$ since $|(f-g)(x)| \ge 0$.
	And,
	\[ d(f,g) = 0 \iff |(f-g)(x)| = 0, \forall x \in X \iff f =g  \]
	Also we have,
	\[ d(f,g) \le \| (f-h)+(h-g) \| \le \| f-h \| + \| h-g \| =  d(f,h) + d(h,g) \]
	Therefore, the function $d$ is a distance function/metric on $\mathscr{C}(X)$.
\end{commentary}
\begin{remark}
\begin{important}
	A sequence $\sequence{f_n}$ converges to $f$ in metric space $\mathscr{C}(X)$ if and only if $\sequence{f_n}$ converges to $f$ uniformly on $X$.
\end{important}
\end{remark}
\begin{proof}
	\begin{align*}
		\sequence{f_n} \to f \text{ in } \mathscr{C}(X) & \iff \forall \varepsilon > 0,\ \exists N \in \mathbb{N},\ \forall n > N, d(f_n,f) \le \varepsilon\\
		& \iff \forall n > N, \| f_n - f \| \le \varepsilon \\
		& \iff \forall n > N, \sup |f_n(x)-f(x)| \le \varepsilon \\
		& \iff \forall n > N, \forall x \in X,\ |f_n(x)-f(x)| \le \varepsilon \\
		& \iff \sequence{f_n} \to f \text{ uniformly on } X
	\end{align*}
\end{proof}

\begin{definition}[uniformly closed]
	Closed subset of $\mathscr{C}(X)$ is \textbf{uniformly closed} since every convergent sequence in it corresponds to a uniform convergent sequence in $X$.
\end{definition}

\begin{definition}[uniform closure]
	Let $\mathscr{A} \subset \mathscr{C}(X)$.
	Then its closure, $\bar{\mathscr{A}}$ is the \textbf{uniform closure} of $\mathscr{A}$.
\end{definition}

\begin{definition}[complete metric space]
	A metric space is \textbf{complete} if every Cauchy sequence in it converges.
\end{definition}

For example, $\mathbb{R}$ is complete.
But $\mathbb{Q}$ is not complete since a sequence of rational numbers converging to $\pi$ in $\mathbb{R}$ is Cauchy sequence in $\mathbb{Q}$ which doesn't converge to any point in $\mathbb{Q}$.

\begin{theorem} %7.15
	Let $X$ be a metric space.
	Let $\mathscr{C}(X)$ be the set of all complex valued, continuous, bounded functions on $X$.
	Let $f,g \in \mathscr{C}(X)$.
	Define norm $\| f \| = \sup_{x \in X} |f(x)|$ and metric $d(f,g) = \| f-g \|$.
	Then metric space $\entity{\mathscr{C}(X),d}$ is a complete metric space.
\end{theorem}
\begin{proof}
	Let sequence of functions $\sequence{f_n}$ be a Cauchy sequence in $\mathscr{C}(X)$.
	Let $\varepsilon > 0$.
	Then there exists a natural number $N$ such that for any $n,m > N$, $d(f_n,f_m) < \varepsilon$.
	That is, 
	\[ d(f_n,f_m) = \sup_{x \in X} |f_n(x) - f_m(x)| < \varepsilon \]
	%Thus, the  inequality is true for any $x \in X$.
	%\[ \forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n,m > N, \forall x \in X, |f_n(x)-f_m(x)| <\varepsilon \]
	%Thus, $\sequence{f_n} \to f$ uniformly on $X$.
	Define $M_n = \sup_{x \in X} |f_n(x)|$.
	Then,
	\[ |M_n - M_m| = \left| \sup_{x \in X} |f_n(x)| - \sup_{x \in X}|f_m(x)| \right| \le \sup_{x \in X} |f_n(x) - f_m(x) | < \varepsilon \]
	We have, $\sequence{M_n}$ is a Cauchy sequence in $\mathbb{R}$.
	We know that, every Cauchy sequence in $\mathbb{R}$ is convergent in $\mathbb{R}$ since $\mathbb{R}$ is complete.
	Thus, $\sequence{M_n}$ converges.\\

	We have, corresponding  sequence $\sequence{f_n}$ in $X$ such that sequence $\sequence{M_n}$ defined by $M_n = \sup_{x \in X} |f_n(x)|$ converges.
	Therefore, $\sequence{f_n}$ converges to $f$ uniformly on $X$ by criterion of uniform convergence.\\

	We have, $f_n$ are continuous, bounded functions on $X$.
	And $\sequence{f_n} \to f$ uniformly on $X$.
	\[ \forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n > N, \forall x \in X, |f_n(x) - f_m(x)| < \varepsilon \]
	Since, the convergence is uniform, the limit function $f$ is continuous and \begin{important}bounded\end{important}.
	Therefore, $\sequence{f_n}$ in $\mathscr{C}(X)$ converges to $f \in \mathscr{C}(X)$.\\

	The Cauchy sequence $\sequence{f_n}$ is chosen arbitrarily.
	Thus, every Cauchy sequence in $\mathscr{C}(X)$ is convergent.
	Therefore, $\mathscr{C}(X)$ is complete.
\end{proof}

\hrule
\begin{remark}\cite[Exercise 7.1]{rudin}
	Let $\sequence{f_n}$ be a sequence of bounded functions on a metric space $X$.
	Suppose $\sequence{f_n} \to f$ uniformly on $X$.
	Then $f$ is bounded.
\end{remark}
\begin{proof}
	Suppose the sequence of bounded functions, $\sequence{f_n}$ converges to $f$ uniformly on $X$.
	Consider $\sequence{g_n}$ where $g_n = |f_n - f|$
	Then, $\sequence{g_n}$ converges to $0$ uniformly.
	Suppose $f$ is unbounded.
	Then $g_n$ is an unbounded sequence which doesn't converge.
	This is a contradiction.
	Therefore, $f$ is bounded.
\end{proof}
\hrule

\subsection{Uniform Convergence and Integration}
\begin{theorem} %7.16
	Let $\alpha$ be monotonically increasing on $[a,b]$.
	Suppose $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Suppose $\sequence{f_n} \to f$ uniformly on $[a,b]$.
	Then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \int_a^b f\ d\alpha = \lim_{n \to \infty} \int_a^b f_n \ d\alpha \]
\end{theorem}
\begin{proof}
	Let $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Suppose sequence $\sequence{f_n} \to f$ uniformly on $[a,b]$.
	Define
	\[ \varepsilon_n = \sup_{x \in [a,b]} |f_n(x) - f(x)| \]
	Then, $ -\varepsilon_n \le f - f_n \le \varepsilon_n,\quad \forall x \in [a,b] $. 
	And,
	\[ f_n - \varepsilon_n \le f \le f_n + \varepsilon_n \]
	Since $f_n \in \mathscr{R}(\alpha)$, we have $f_n+\varepsilon, f_n - \varepsilon \in \mathscr{R}(\alpha)$.
	From the definition of lower and upper integrals,
	\[ \int_a^b (f_n - \varepsilon_n)\ d\alpha \le \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \le \int_a^b (f_n + \varepsilon_n)\ d\alpha  \]
	Thus,
	\[ 0 \le \upint_a^b f\ d\alpha - \lowint_a^b f\ d\alpha \le \int_a^b 2\varepsilon_n\ d\alpha = 2\varepsilon_n \left[ \alpha(b)-\alpha(a) \right] \]
	Clearly, $\varepsilon_n \to 0$ as $n \to \infty$.
	Then, $f \in \mathscr{R}(\alpha)$ on $[a,b]$ since the lower and upper integrals of $f$ with respect to $\alpha$ are equal.\\
	
	Also we have, $ 0 \le |f-f_n| \le \varepsilon_n $ for every $x \in [a,b]$.\\
	Therefore,
	\[ 0 \le \left| \int_a^b f\ d\alpha - \int_a^b f_n\ d\alpha \right| \le \int_a^b |f-f_n|\ d\alpha \le \int_a^b \varepsilon_n\ d\alpha = \varepsilon_n \left[ \alpha(b) - \alpha(a) \right] \]
	As $n \to \infty$, we have $\varepsilon_n \to 0$.
	Thus,
	\[ \int_a^b f\ d\alpha = \lim_{n \to \infty} \int_a^b f_n\ d\ \alpha \]
\end{proof} 

\begin{corollary}
	If $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$ and if
	\[ f(x) = \sum_{n = 1}^\infty f_n(x) \]
	the series converging uniformly on $[a,b]$, then
	\[ \int_a^b f\ d\alpha = \sum_{n=1}^\infty \int_a^b f_n \ d\alpha \]
	In other words, the series may be integrated term by term.
\end{corollary}
\begin{proof}
	Suppose $f_n \in \mathscr{R}(\alpha)$ and $\sum f_n$ converges to $f$ uniformly on $[a,b]$.
	Let $\sequence{S_n}$ be the sequence of partial sums converging to $f$ uniformly on $[a,b]$.
	We have, $S_n \in \mathscr{R}(\alpha)$ on $[a,b]$ by linearity of the integral and mathematical induction.
	\[ \sum_{j = 1}^n \int_a^b f_n\ d\alpha = \int_a^b \sum_{j = 1}^n f_j\ d\alpha = \int_a^b S_n\ d\alpha \]
	Since the uniform limit function of integrable functions is integrable,
	\[ \sum_{n = 1}^\infty f_n = \lim_{n \to \infty} S_n  = f \in \mathscr{R}(\alpha) \text{ and } \lim_{n \to \infty} \int_a^b S_n\ d\alpha = \int_a^b f\ d\alpha \]
	By the definition of sum of series,
	\[ \sum_{n = 1}^\infty \int_a^b f_n\ d\alpha = \lim_{n \to \infty} \sum_{j = 1}^n \int_a^b f_j\ d\alpha = \lim_{n \to \infty} \int_a^b S_n\ d\alpha = \int_a^b f\ d\alpha = \int_a^b \sum_{n = 1}^\infty f_n\ d\alpha \]
	Thus, the series may be integrated term by term.
\end{proof}

\subsection{Uniform Convergence and Differentiation}
\begin{theorem} %7.17
	Suppose $\sequence{f_n}$ is a sequence of functions, differentiable on $[a,b]$ and $\sequence{f_n(x_0)}$ converges for some $x_0 \in [a,b]$.
	If $\sequence{f_n'}$ converges uniformly on $[a,b]$, then $\sequence{f_n}$ converges uniformly on $[a,b]$ to a function $f$.
	And,
	\[ f'(x) = \lim_{n \to \infty} f_n'(x) \]
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n(x_0)} \to f(x_0)$.
	Suppose $\sequence{f_n'}$ converges uniformly on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists a natural number $N$ such that
	\[ \forall n,m > N,\ |f_n(x_0) - f_m(x_0)| < \frac{\varepsilon}{2} \text{ and}\] 
	\[ \forall n,m > N,\ |f_n'(t) - f_m'(t)| < \frac{\varepsilon}{2(b-a)} \]
	Let $x,t \in [a,b]$.
	\begin{align*}
		|f_n(x)-f_m(x)-f_n(t)+f_m(t)| & = |(f_n-f_m)(x) - (f_n-f_m)(t)| 
		\intertext{By mean value theorem, there exists $y \in (x,t)$ such that}
		|f_n(x)-f_m(x)-f_n(t)+f_m(t)| & = |(x-t)\ (f_n-f_m)'(y)|\\
		& \le \frac{|x-t|\varepsilon}{2(b-a)} \le \frac{\varepsilon}{2}
	\end{align*}
	since $|(f_n-f_m)'(x)| = |f_n'(x) - f_m'(x)| \le \frac{\varepsilon}{2(b-a)}$. \\

	And the inequality is true of $t = x_0$.
	Thus,
	\begin{align*}
		|f_n(x)-f_m(x)| &
		= |f_n(x)-f_m(x) -f_n(x_0)+f_m(x_0)+f_n(x_0)-f_m(x_0| \\
		& \le |f_n(x)-f_m(x) -f_n(x_0)+f_m(x_0)| + |f_n(x_0)-f_m(x_0| \le \varepsilon
	\end{align*}
	Thus, $\forall \varepsilon > 0$, we have a natural number $N$ such that $\forall n,m > N$, $\forall x \in [a,b]$, $|f_n(x)-f_m(x)| \le \varepsilon$.
	In other words, $\sequence{f_n} \to f$ uniformly on $[a,b]$.\\

	Fix $x \in [a,b]$.
	Define functions $\phi_n,\phi$ on $[a,b]$ except at $t = x$,
	\[ \phi_n(t) = \frac{f_n(t)-f_n(x)}{t-x} \quad \text{ and } \quad \phi(t) = \frac{f(t)-f(x)}{t-x}\]
	Then,
	\[ \lim_{n \to \infty} \phi_n(t) = \lim_{n \to \infty} \frac{f_n(t) - f_n(x)}{t-x} = \frac{\displaystyle \lim_{n \to \infty} f_n(t) - \displaystyle \lim_{n \to \infty} f_n(x)}{t-x} = \frac{f(t) - f(x)}{t-x} = \phi(t)\] 
	since sequence $\sequence{f_n} \to f$ uniformly on $[a,b]$.
	Clearly, $\sequence{\phi_n}$ converges to $\phi$ pointwise on $[a,b] - \{ x \}$.\\

	We have,
	\[ \lim_{t \to x}\phi_n(t) = \lim_{t \to x} \frac{f_n(t) - f_n(x)}{t-x} = f_n'(x) \]
	\[ \lim_{t \to x} \phi(t) = \lim_{t \to x} \frac{f(t)-f(x)}{t-x} = f'(x) \]
	Also we have,
	\[ |\phi_n(t)-\phi_m(t)| = \frac{|f_n(t)-f_n(x)-f_m(t) + f_m(x)|}{t-x} \le \frac{\varepsilon}{2(b-a)} \]
	That is $\forall n,m > N$, $|\phi_n(t)-\phi_m(t)| < \varepsilon$.
	Now, by Cauchy criterion for uniform convergence, sequence $\sequence{\phi_n}$ converges to $\phi$ uniformly on $[a,b]-\{x\}$.\\

	We know that continuity is preserved under uniform convergence.\\
	Therefore,
	\[ f'(t) = \lim_{t \to x} \phi(t) = \lim_{t \to x} \lim_{n \to \infty} \phi_n(t) = \lim_{n \to \infty} \lim_{t \to x} \phi_n(t) = \lim_{n \to \infty} f_n'(x) \]

\end{proof}

\begin{theorem} %7.18
	There exists a real continuous function on the real line which is nowhere differentiable.
\end{theorem}
\begin{proof}
	Define $\phi : [-1,1] \to \mathbb{R}$ by $\phi(x) = |x|$.
	Extend $\phi$ from $[-1,1]$ to $\mathbb{R}$ such that $\phi(x+2) = \phi(x)$.
	Then $|\phi(s)-\phi(t)| \le |s-t|,\ \forall s,t \in \mathbb{R}$.\\

\begin{figure}[h]
	\begin{tikzpicture}
		\draw[-latex] (-7,0) -- (7,0);
		\draw[-latex] (0,0) -- (0,2);
		\draw (-6,0) -- (-5,1) -- (-4,0) -- (-3,1) -- (-2,0) -- (-1,1) -- (0,0) -- (1,1) -- (2,0) -- (3,1) -- (4,0) -- (5,1) -- (6,0);
		\draw (-6,-0.3) node{$-6$};
		\draw (-5,-0.3) node{$-5$};
		\draw (-4,-0.3) node{$-4$};
		\draw (-3,-0.3) node{$-3$};
		\draw (-2,-0.3) node{$-2$};
		\draw (-1,-0.3) node{$-1$};
		\draw (0,-0.3) node{$0$};
		\draw (1,-0.3) node{$1$};
		\draw (2,-0.3) node{$2$};
		\draw (3,-0.3) node{$3$};
		\draw (4,-0.3) node{$4$};
		\draw (5,-0.3) node{$5$};
		\draw (6,-0.3) node{$6$};
	\end{tikzpicture}
	\caption{Graph of $\phi$}
\end{figure}

	Define $f_n(x) = \left(\frac{3}{4}\right)^n \phi(4^n x)$.
	Then $|f_n| \le \left(\frac{3}{4}\right)^n$.
	By Weierstrass M test, $\displaystyle \sum_{n = 0}^\infty f_n$ converges uniformly on $\mathbb{R}$.
	Consider the function $f : \mathbb{R} \to \mathbb{R}$ defined by $\displaystyle f(x) = \sum_{n = 0}^\infty \left(\frac{3}{4}\right)^n \phi(4^n x)$.
	This sum function $f$ is continuous since the convergence is uniform and $f_n$ are all continuous.\\

	Fix a real number $x$ and a positive integer $m$.
	Define $\displaystyle \delta_m = \pm \frac{1}{2}\ 4^{-m}$ such that no integer lies between $4^m x$ and $4^m(x+\delta_m)$.
	This is possible since $|4^m (x+\delta_m) - 4^m x| = |4^m \delta_m| = \frac{1}{2}$.\\

	\begin{commentary}
		The choice of sign of $\delta_m$ is made depending on the value of $4^m x$.
		Suppose $x = 1.1$ and $m = 3$.
		Then $4^m x = 70.4$.
		Now $\delta = \frac{1}{2}\ 4^{-m} = \frac{1}{128}$ so that $4^m(x+\delta_m)= 70.9$.
		Suppose $x = 1.2$ and $m = 3$.
		Then $4^m x = 76.8$.
		Now $\delta = -\frac{1}{2}\ 4^{-m} = -\frac{1}{128}$ so that $4^m(x+\delta) = 76.3$.\\
	\end{commentary}

	Define
	\[\gamma_n = \frac{\phi(4^n(x+\delta_m)) - \phi(4^n x)}{\delta_m} \]

	If $n > m$, then $4^n \delta_m$ is an even integer and $\gamma_n = 0$.\\
	If $n \le m$, then 
	\[ |\gamma_n| = \left|\frac{\phi(4^n(x+\delta_m)) - \phi(4^n x)}{\delta_m} \right| \le \frac{4^n \delta_m }{\delta_m} \le 4^n \]
	since $|\phi(s)-\phi(t)| \le |s-t|$.\\
	In particular, if $n = m$, then
	\[ |\gamma_m| = \left| \frac{\phi(4^m(x+\delta_m))-\phi(4^m x)}{\delta_m} \right| = 4^m \]
	since $|\phi(4^m (x+\delta_m)) - \phi(4^m x)| = |4^m \delta| = \frac{1}{2}$ as there are no integers between $4^m(x+\delta_m)$ and $4^m x$.\\

	From the definition of $\gamma_n$ we have,
	\begin{align*}
	 \left| \frac{f(x+\delta_m) - f(x)}{\delta_m} \right| 
		& = \left| \sum_{n = 0}^m \left( \frac{3}{4} \right)^n \gamma_n \right| \\
		& = \left| 3^m + \sum_{n = 0}^{m-1} \left( \frac{3}{4} \right)^n \gamma_n \right| \\
		& \ge 3^m - \sum_{n = 0}^{m-1} 3^n = \frac{3^m+1}{2}
	\end{align*}
	Therefore, the function $f$ is not differentiable at $x$ since the following limit does not exist as $m \to \infty$.
	\[ \lim_{\delta_m \to 0} \left| \frac{f(x+\delta_m) - f(x)}{\delta_m} \right| \ge \lim_{m \to \infty} \frac{3^m+1}{2} \]
	Since the choice of $x$ is arbitrary, the function $f$ is nowhere differentiable.
\end{proof}

%\chapter{Weierstrass Approximation \& Some Special Functions

