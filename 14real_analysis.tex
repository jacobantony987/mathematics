%Text Books: \cite{apostol}, \cite{rudin}
%Module 1: Functions of bounded variation and rectifiable curves
%Introduction, properties of monotonic functions, functions of bounded variation, total variation, additive property of total variation, total variation on $(a,x)$ as a functions of $x$, functions of bounded variation expressed as the difference of increasing functions, continuous functions of bounded variation, curves and paths, rectifiable path and arc length, additive and continuity properties of arc length, equivalence of paths, change of parameter.
%(Chapter 6, Section: 6.1 - 6.12. of \cite{apostol}) (20 hours.)
%Module 2: The Riemann-Stieltjes Integral
%Definition and existence of the integral, properties of the integral, integration and differentiation, integration of vector valued functions.
%(Chapter 6 - Section 6.1 to 6.25 of \cite{rudin}) (20 hours.)
%Module 3: Sequence and Series of Functions
%Discussion of main problem, Uniform convergence, Uniform convergence and Continuity, Uniform convergence and Integration, Uniform convergence and Differentiation.
%(Chapter 7 Section. 7.1 to 7.18 of \cite{rudin}) (25 hours.)
%Module 4: Weierstrass Approximation \& Some Special Functions
%Equicontinuous families of functions, the Stone - Weierstrass theorem, Power series, the exponential and logarithmic functions, the trigonometric functions, the algebraic completeness of complex field.
%(Chapter 7 â€“ Sections 7.19 to 7.27, Chapter 8 - Section 8.1 to 8.8 of \cite{rudin}) (25 hours.)

%Module 1 - \cite{apostol} 6
%Module 2 - \cite{rudin} 6
%Module 3 - \cite{rudin} 7a
%Module 4 - \cite{rudin} 7b

%The commands to Riemann upper/lower integrals are defined by Leo Liu in tex-stack-exchange.
%Leo Liu - https://tex.stackexchange.com/a/44245 
\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

%Module 1
%\chapter{Functions of Bounded Variation \& Rectifiable Curves}
{\Large Module 1 : Bounded Variation \& Rectifiable Curves}
\section{Functions of Bounded Variation \& Rectifiable Curves}
%\subsection{Introduction}
\setcounter{subsection}{1}
\subsection{Properties of Monotone Functions}
\begin{theorem}
	Let $f$ be an increasing function defined on closed interval $[a,b]$.
	And let $x_0=a < x_1 < x_2 < \dots <x_{n-1} < x_n = b$.
	Then,
	\[ \sum_{k = 1}^{n-1} [f(x_k+)-f(x_k-)] \le f(b) - f(a) \]
\end{theorem}
\begin{proof}
	Let $f$ be an increasing function on $[a,b]$.
	Let $\{x_0=a,\ x_1, \dots,\ x_n=b\}$ be a partition of $[a,b]$.
	Let $y_k \in (x_{k-1},x_k),\ \forall k$.
	Then, $f(y_k) \le f(x_k+)$ and $f(x_k-) \le f(y_{k-1})$.
	Therefore,
	\[ \sum_{k=1}^{n-1} [f(x_k+) - f(x_k-)] \le \sum_{k=1}^{n-1} [f(y_k) - f(y_{k-1})] \le f(b) - f(a) \]
\end{proof}
\begin{commentary}
	In other words, for monotonic functions the sum of jumps is bounded.
\end{commentary}

\begin{theorem}
	Let $f$ be a monotonic function defined on closed interval $[a,b]$.
	Then the set of discontinuities of $f$ is countable.
\end{theorem}
\begin{proof}
	Without loss of generality, let $f$ be an increasing function on $[a,b]$.
	Let $S_m$ be the set of all points on $[a,b]$ at which the jump exceeds $\frac{1}{m}$.\\

	We know that, the sum of jumps of an increasing function is bounded above by $f(b)-f(a)$.	
	Thus, cardinality of $S_m$ given by,
	\[ |S_m| < m[f(b)-f(a)] \]
	is finite for any positive integer $m$.\\

	If $f$ is discontinuous at a pont $x \in [a,b]$, then there exists some integer $m'$ such that $0 < \frac{1}{m'} < x $ and $x \in S_{m'}$.
	\[ \text{Number of discontinuities } = \left| \bigcup_{m=1}^\infty S_m \right| \le \sum_{m=1}^\infty |S_m| \text{ is countable.}\]
	since countable sum of finite values is countable.
	Therefore, the number of discontinuities of $f$ is countable.
\end{proof}

\subsection{Function of Bounded Variation}
\begin{definition}[partition]
	Let $[a,b]$ be a compact interval.
	Let $x_0 = a$, $x_0 < x_1 < x_2 < \dots < x_n$ and $x_n = b$.
	Then $P = \{ x_0,x_1,\dots,x_n \}$ is a \textbf{partition} of $[a,b]$.
	And $(x_{k-1},x_k)$ is the \textbf{$k$th subinterval} of the partition.
\end{definition}

\begin{definition}[bounded variation]
	Let $f$ be a function defined on closed interval $[a,b]$.
	If there exists a positive real-number $M$ such that
	\[ \sum_{k=1}^n |\Delta f_k| = \sum_{k=1}^n |f(x_k) - f(x_{k-1})| \le M \]
	for any partition $P$ on $[a,b]$.
	Then $f$ is a function of \textbf{bounded variation}, where $(x_{k-1},x_k)$ is the $k$th subinterval of the partition.\\

\begin{commentary}
	In other words, a function is of bounded variation on $[a,b]$ if the sum of variations is bounded for any (finite) partition of $[a,b]$.
\end{commentary}
\end{definition}

\begin{theorem}
	Let $f$ be a monotonic function on $[a,b]$.
	Then $f$ is of bounded variation on $[a,b]$.
\end{theorem}
\begin{proof}
	Without loss of generality, let $f$ be an increasing function.
	Then for any partition $P = \{x_0,x_1,\dots,x_n\}$ of $[a,b]$, we have
	\[ \sum_{k=1}^n \left[ f(x_k)-f(x_{k-1}) \right] \le f(b)-f(a) = M \]
	Therefore, $f$ is of bounded variation on $[a,b]$.
\end{proof}

\begin{theorem}
	Let $f$ be a continuous function on $[a,b]$ and its derivative $f'$ exists and $f'$ is bounded in $(a,b)$.
	Then $f$ is of bounded variation.	
\end{theorem}
\begin{proof}
	Let $f$ be a continuous function with derivative $f'$ on $(a,b)$.
	Since $f$ is continuous and $f'$ exists, from intermediate value theorem we have
	\[ \Delta f_k = f(x_k) - f(x_{k-1}) = f'(t_k) [x_k-x_{k-1}] \text{ where } t_k \in (x_{k-1},x_k) \]
	Since $f'$ is bounded, $f'(t_k) \le M'$ for any $t_k \in (a,b)$.
	Thus,
	\[ \sum_{k=1}^n |\Delta f_k| = \sum_{k=1}^n |f'(t_k)| (x_k-x_{k-1}) \le M'\sum_{k=1}^n x_k-x_{k-1} = M'(b-a) = M \]
	Therefore, $f$ is of bounded variation.
\end{proof}

\begin{theorem}
	Let $f$ be a function on $[a,b]$.
	If $f$ is of bounded variation, then $f$ is bounded.
\end{theorem}
\begin{proof}
	Let $x \in (a,b)$.
	Consider the partition $P = \{ a,x,b \}$.
	Since $f$ is of bounded variation, there exists a positive real-number $M$ such that
	\[ \sum_{k=1}^2 |\Delta f_k| = |f(x)-f(a)| + |f(b)-f(x)| \le M \]
	Clearly, $|f(x)-f(a)| \le M$, since $|f(b)-f(x)| > 0$.
	We have,
	\[ |f(x)| = |f(x)-f(a)+f(a)| \le |f(x)-f(a)|+|f(a)| \le M+|f(a)| \]
	Suppose $x = a$, then $|f(x)| = |f(a)|$.\\
	Suppose $x = b$, then $|f(x)| = |f(b)| = M' + |f(a)|$ where $M' = |f(b)|-|f(a)|$.\\

	Therefore, the function $f$ is bounded on $[a,b]$.
\end{proof}

\subsection{Total Variation}
\begin{definition}[total variation]
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $\Sigma (P)$ be the sum of variations with respect to the partition $P$ of $[a,b]$.
	Then the \textbf{total variation} of the function $f$ on $[a,b]$ is given by,
	\[ V_f(a,b) = V_f = \sup \{ \Sigma (P) : P \in \mathscr{P}[a,b] \} \]
\end{definition}
\textbf{Note 1} : $V_f$ is finite, since $f$ is of boundned variation on $[a,b]$.\\

\textbf{Note 2} : $V_f \ge 0$, since $|\Delta f_k| \ge 0$ for any subinterval of $[a,b]$.\\


\textbf{Note 3} : $V_f = 0$ if only if $f$ is a constant function on $[a,b]$. (Why ?)

\begin{theorem}
	Let $f,g$ be functions of bounded variation on $[a,b]$.
	Then their sum $f+g$, difference $f-g$, and product $fg$ are of bounded variation.
	Also, 
	\[ V_{f \pm g} \le V_f + V_g \quad \text{ and } \quad V_{fg} \le AV_f + BV_g \]
	where $\displaystyle A = \sup \left\{ |g(x)| : x \in [a,b] \right\}$ and $\displaystyle B = \sup \left\{ |f(x)| : x \in [a,b] \right\}$.
\end{theorem}
\begin{proof}
	Let $f,g$ be functions of bounded variation on $[a,b]$.
	Then $f,g$ are bounded and $\sup |f(x)|$ and $\sup |g(x)|$ exists.\\

	\textbf{Step 1 : $V_{f \pm g} \le V_f + V_g$}\\
	We have,
	\[ |(f+g)(x_k) - (f+g)(x_{k-1})| \le | f(x_k) - f(x_{k-1})| + |g(x_k) - g(x_{k-1})| \]
	Then
	\begin{align*}
	V_{f+g}  
		& = \sup_{P \in \mathscr{P}} \sum_{k=1}^n |(f+g)(x_k) - (f+g)(x_{k-1})| \\
		& \le \sup_{P \in \mathscr{P}} \sum_{k=1}^n |f(x_k) - f(x_{k-1})| + \sup_{P \in \mathscr{P}} \sum_{k=1}^n |g(x_k)-g(x_{k-1})| \\
		& = V_f + V_g 
\end{align*}
	Similarly, we have $V_{f-g} \le V_f + V_g$ since
	\[ |(f-g)(x_k) - (f-g)(x_{k-1})| \le |f(x_k)-f(x_{k-1})| + |g(x_k)-g(x_{k-1})| \]

	\textbf{Step 2 : $V_{fg} \le AV_f + BV_g$}\\
	We have,
	\begin{align*}
	|fg(x_k)-fg(x_{k-1})| 
		& = |f(x_k)g(x_k) - f(x_{k-1}g(x_{k-1})| \\
		& = |f(x_k)g(x_k) - f(x_{k-1})g(x_k) + f(x_{k-1}g(x_k) - f(x_{k-1}g(x_{k-1}) | \\
		& \le |g(x_k)|\ |f(x_k)-f(x_{k-1})| + |f(x_{k-1}|\ |g(x_k) - g(x_{k-1})| \\
		& \le A |f(x_k)-f(x_{k-1})| + B |g(x_k)-g(x_{k-1})|
	\end{align*}
	where $A = \sup \{ |g(x)| : x \in [a,b] \}$ and $B = \sup \{ |f(x)| : x \in [a,b] \}$.\\

	Therefore,
	\begin{align*}
	V_{fg} 
		& \le A \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |f(x_k)-f(x_{k-1}| \right\} + B \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |g(x_k)-g(x_{k-1})| \right\} \\
		& = AV_f + BV_g 
	\end{align*}
\end{proof}

\begin{definition}[bounded away from zero]
	A function $f$ is bounded away from zero on $[a,b]$ if there exists a real-number $m$ such that $0 < m \le f(x)$, $\forall x \in [a,b]$.
\end{definition}

\begin{commentary}
	Let $f$ be a function of bounded variation.
	Then $\frac{1}{f}$ is of bounded variation if and only if $f$ is bounded away from zero.(Why ?)
\end{commentary}

\begin{theorem}
	Let $f$ be a function of bounded variation on $[a,b]$ and $f$ is bounded away from zero.
	Then, $g = \frac{1}{f}$ is a function of bounded variation on $[a,b]$ and $V_g \le \frac{V_f}{m^2}$ whenever $0 < m \le |f(x)|$.
\end{theorem}
\begin{proof}
	Suppose $f$ is of bounded variation on $[a,b]$ and $f$ is bounded away from zero.
	That is, there exists positive real-number $m$ such that $0 < m \le |f(x)|$, $\forall x \in [a,b]$.
	Then, $\frac{1}{|f(x)|} \le \frac{1}{m}$, $\forall x \in [a,b]$.\\

	Define $g = \frac{1}{f}$.
	Then, we have
	\[ |\Delta g_k| = \left| \frac{1}{f(x_k)} - \frac{1}{f(x_{k-1})} \right| = \frac{|f(x_k)-f(x_{k-1})|}{|f(x_k)|\ |f(x_{k-1}|} \le \frac{|f(x_k)-f(x_{k-1})|}{m^2} \]
	The total variation of $g$ on $[a,b]$ is given by,
	\begin{align*}
	V_g 
		& = \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |g(x_k)-g(x_{k-1})| \right\} \\
		& \le \frac{1}{m^2} \sup_{P \in \mathscr{P}} \left\{ \sum_{k=1}^n |f(x_k)-f(x_{k-1})| \right\} \\
		& \le \frac{V_f}{m^2} \text{ where } 0 < m \le |f(x)|,\ \forall x \in [a,b]
	\end{align*}
\end{proof}
\begin{commentary}
	In other words, if $h,f$ are functions of bounded variation of $[a,b]$.
	And $f$ is bounded away from zero, then $g = \frac{1}{f}$ and $\frac{h}{f} = hg$ is a function of bounded variation and $V_{\frac{h}{f}} = V_{hg} \le AV_h + BV_f$.\\

	Now, we have analyzed sum, difference, product and quotient of functions of bounded variation.
	And we are not surprised about preservation of bounded variation under function composition. (Why ?)
\end{commentary}
\subsection{Additive Property of Total Variation}
\begin{theorem}[additive property]
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $c \in (a,b)$.
	Then $f$ is of bounded variation on both $[a,c]$ and $[c,b]$.
	And, $V_f(a,b) = V_f(a,c)+V_f(c,b)$.
\end{theorem}
\begin{proof}
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $P_1,P_2$ be partitions of $[a,c]$ and $[c,b]$ respectively.
	Then $P_0 = P_1 \cup P_2$ is a partition of $[a,b]$.
	Thus,
	\[ \sum (P_1) + \sum (P_2) = \sum (P_0) \le V_f(a,b) \]
	Taking supremums on the left side, we get
	\[ V_f(a,c) + V_f(c,b) \le V_f(a,b) \]	
	Clearly, $V_f(a,c) \le V_f(a,b)$ and $V_f(c,b) \le V_f(a,b)$.
	Therefore, function $f$ is of bounded variation on both $[a,c]$ and $[c,b]$.\\

	Let $P$ be a paritition of $[a,b]$.
	Then $P_0 = P\cup\{c\}$ is refinement of $P$.
	Now, we have two partitions $P_1, P_2$ of $[a,c]$ and $[c,b]$ such that $P_0 = P_1 \cup P_2$.
	Thus,
	\[ \sum (P) \le \sum (P_0) = \sum (P_1) + \sum (P_2) \le V_f(a,c) + V_f(c,b) \]
	Taking supremum on the left side, we get
	\[ V_f(a,b) \le V_f(a,c) + V_f(c,b) \]
	Therefore,
	\[ V_f(a,b) = V_f(a,c) + V_f(c,b),\quad \forall c \in (a,b) \]
\end{proof}

\subsection{Total Variation on $[a,x]$ as a function of $x$}
\begin{commentary}
	By additive property, we have existence of $V_f(a,x)$ for every $x \in (a,b]$.
	Assigning $V(x) = V_f(a,x)$, we have a well-defined function on $(a,b]$.
\end{commentary}

\begin{theorem}
	Let $f$ be a function of bounded variation on $[a,b]$.
	Define $V : [a,b] \to \mathbb{R}$ given by,
	\[ V(x) = \begin{cases} V_f(a,x) & x \in (a,b] \\ 0 & x = a \end{cases} \]
	Then,
	\begin{enumerate}
		\item $V$ is an increasing function on $[a,b]$.
		\item $V-f$ is an increasing function of $[a,b]$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Suppose $x = a$.
	Then $V(x) = 0$ and $V(y) = V_f(a,y) \ge 0 = V(x)$.\\
	Suppose $x \ne a$.
	Then, $a < x < y \le b$.
	By additive property of total variation, we have $V(y) = V_f(a,y) = V_f(a,x)+V_f(x,y) = V(x) + V_f(x,y)$.
	Since $V_f(x,y) \ge 0$, we have $V(x) \le V(y)$.
	Therefore, $V$ is an increasing function on $[a,b]$.\\

	Define $D : [a,b] \to \mathbb{R}$ given by $D(x)  = (V-f)(x) = V(x) - f(x)$.
	Suppose $a \le x < y \le b$.
	Then, $D(y)-D(x) = V(y)-f(y)-V(x)+f(x) = [V(y)-V(x)] - [f(y)-f(x)]$.
	We have, $V(y) = V_f(a,y) = V_f(a,x)+V_f(x,y)$.
	Thus, $V(y)-V(x) = V_f(x,y)$.
	Also we have, $f$ is of bounded variation on $[x,y]$.
	Consider the trivial partition $P = \{ x,y\}$ of $[x,y]$.
	Then, we have $f(y) - f(x) = \sum (P) \le V_f(x,y)$.
	Thus, $D(y)-D(x) = V_f(x,y) - [f(y)-f(x)] \ge 0$.
	Therefore, $D = V-f$ is an increasing function on $[a,b]$.
\end{proof}

\subsection{Function of bounded variation expressed as the difference of increasing functions}
\begin{theorem}
	Let $f$ be a function on $[a,b]$.
	Function $f$ is of bounded variation on $[a,b]$ if and only if $f$ can be expressed as difference of two increasing functions.
\end{theorem}
\begin{proof}
	Let $f$ be a function of bounded variation, then $f = V-D$ where total variation $V$ and $D = V-f$ are both increasing.\\

	Let $f$ be function on $[a,b]$.
	Let $f = V-D$ where $V,D$ are increasing functions. 
	Then $V,D$ are of bounded variation, since monotonic functions on $[a,b]$ are of bounded variation.
	Also, we have $V-D$ is of bounded variation, since for any two functions of bounded variation their difference is also of bounded variation.
\end{proof}

\subsection{Continuous functions of bounded variation}
\begin{theorem}
	Let $f$ be a function of bounded variation on $[a,b]$. %Why it has to be of bounded variation ? For the existence of $V$ ?
	Let $V$ be the total variation function of $f$ defined on $[a,b]$.
	$f$ is continuous at a point if and only if $V$ is continuous at that point.
\end{theorem}
\begin{proof}
	Let $f$ be a function of bounded variation on $[a,b]$.
	Let $V$ be the total variation of $f$.
	Let $x,y \in [a,b]$, such that $x<y$.
	We have $V$ is an increasing function on $[a,b]$.
	And $f$ is difference of two increasing functions on $[a,b]$.
	Thus, $f(x+),\ f(x-),\ V(x+),\ V(x-)$ exists for any $x \in (a,b)$.
	It remains to prove that $V,f$ are continuous at $x \in (a,b)$.\\

	\textbf{Part 1 : $V$ continuous $\implies$ $f$ continuous}\\
	Suppose $x \ne a$, then $a<x<y\le b$.
	Let $P$ be any partition on $[a,x]$.
	Then there exists a partition $P'$ on $[a,y]$ such that $P \subset P'$.
	Consider, $P' = P\cup\{y\}$.
	Then, $V(y) > V(x)$ and $V(y)-V(x) \ge |f(y)-f(x)|$.
	Thus,
	\[ 0 \le |f(y)-f(x)| \le V(y)-V(x) \]
	The inequality is true for any $y>x$.
	Therefore,
	\[ 0 \le |f(x+)-f(x)| \le V(x+)-V(x) \text{ as } y \to x+ \]
	Similarly, let $a<z<x$.
	Then,
	\[ 0 \le |f(x)-f(x-)| \le V(x)-V(x-) \text{ as } z \to x- \]
	Clearly, if $V$ is continuous at $x$ then $f$ is continuous $x$.\\

	\textbf{Part 2 : $f$ continuous $\implies$ $V$ continuous}\\
	Suppose $f$ is continuous at $c \in (a,b)$.
	Let $\varepsilon > 0$.
	We have,
	\[ V_f(c,b) = \sup_{P \in \mathscr{P}} \sum (P) \]
	Thus, there exists a partition $P_1 \in \mathscr{P}[c,b]$ such that $V_f(c,b) - \frac{\varepsilon}{2} < \sum (P_1)$.
	Since $f$ is continuous at $c$, there exists $x_1 \in (c,b)$ such that $|f(x_1)-f(c)| < \frac{\varepsilon}{2}$.
	Then,
	\[ V_f(c,b) - \frac{\varepsilon}{2} < \frac{\varepsilon}{2} + V_f(x_1,b) \]
	We have,
	\begin{align*}
	V(x_1)-V(c) 
		& = V_f(a,x_1) - V_f(a,c) = V_f(c,x_1) \\
		& = V_f(c,b) - V_f(x_1,b) < \varepsilon
	\end{align*}
	Clearly, $V(c+h) \to V(c)$ as $h \to 0$.
	That is, $V(c+) = V(c),\ \forall c \in [a,b)$.
	Similarly we have,
	\[ V_f(a,c) = \sup_{P \in \mathscr{P}} \sum (P) \]
	And there exists $P_2 \in \mathscr{P}[a,c]$ such that
	\[ V_f(a,c) - \frac{\varepsilon}{2} < \sum (P_2) \]
	And there exists $x_2 \in (a,c)$ such that $|f(c)-f(x_2)| < \frac{\varepsilon}{2}$, since $f$ is continuous at $c$.
	Therefore,
	\[ V(c)-V(x_2) = V_f(a,c) - V_f(a,x_2) = V(x_2,c) < \varepsilon \]
	Thus, $V(c-) = V(c),\ \forall c \in (a,b]$.
	Therefore, $V$ is continuous at $c$.
\end{proof}
\begin{theorem}
	Let $f$ be a continuous function on $[a,b]$.
	Function $f$ is of bounded variation on $[a,b]$ if and only if $f$ can be expressed as difference of two increasing continuous functions.
\end{theorem}
\begin{proof}
	Let $f$ be a continuous function on $[a,b]$.
	Then total variation $V$ is a continuous increasing function on $[a,b]$.
	Clearly, $D = V-f$ is also a continuous, increasing function on $[a,b]$.
	Therefore, $f = V - D$ where $V,D$ are continuous, increasing functions.\\

	Let $V,D$ be continuous, increasing functions on $[a,b]$.
	Then $f = V-D$ is also a continuous function on $[a,b]$.
	We have, $V,D$ are increasing functions, therefore both $V,D$ are of bounded variation and their difference $f$ is also of bounded variation.
\end{proof}

\begin{commentary}
	Suppose $f$ is of bounded variation on $[a,b]$.
	Let $id : [a,b] \to [a,b]$ where $id(x) = x$.
	Then $V+id$ is a strictly increasing function and $D = V+id-f$ is also strictly increasing.
	Thus, any function of bounded variation on $[a,b]$ can be characterised as difference of two strictly increasing continuous functions on $[a,b]$.
\end{commentary}
\subsection{Curves and Paths}
\begin{definition}[path]
	Let $f : [a,b] \to \mathbb{R}^n$ be a continuous, vector-valued function.
	Then $f$ is a path in $\mathbb{R}^n$.
	And $f$ is a motion if $[a,b]$ is a time interval.
\end{definition}
\subsection{Rectifiable paths and Arc length}
\begin{definition}[rectifiable path]
	Let $f : [a,b] \to \mathbb{R}^n$ be a path in $\mathbb{R}^n$.
	Let $P = \{ t_0,t_1,\dots,t_m \}$ be a partition of $[a,b]$.
	\[ \Lambda_f (P) = \sum_{k=1}^m \| f(t_k) - f(t_{k-1}) \| = \sum_{k=1}^m \| \Delta f_k \| \]
	If $\Lambda_f (P)$ is bounded for any partition $P \in \mathscr{P}[a,b]$, then path $f$ is \textbf{rectifiable}.
	If $\Lambda_f (P)$ is unbounded, then $f$ is \textbf{nonrectifiable}.\\
\end{definition}
\begin{definition}[arc length]
	Let $f : [a,b] \to \mathbb{R}^n$ be a rectifiable path.
	Then \textbf{arc length} of path $f$ is given by,
	\[ \Lambda_f(a,b) = \sup_{P \in \mathscr{P}} \Lambda_f (P) \]
\end{definition}

\begin{theorem}
	A path $f : [a,b] \to \mathbb{R}^n$ is rectifiable if and only if each component $f_k$ of $f$ is of bounded variation on $[a,b]$.
	Let $V_k(a,b)$ be the total variation of $f_k$ on $[a,b]$.
	Then,
	\[ V_k(a,b) \le \Lambda_f(a,b) \le V_1(a,b) + V_2(a,b) + \dots + V_n(a,b) \]
\end{theorem}
\begin{proof}
	Let $x_j \in \mathbb{R}^n$ for $j = 1,2,\dots,m$.
	Then,
	\[ |x_r| \le \sqrt{\sum_{j=1}^n |x_j|^2} \le \sum_{j=1}^n |x_j| \text{ since } \sum_{j=1}^n x_j^2 \le \left(\sum_{j=1}^n x_j\right)^2 \]
	Let $f : [a,b] \to \mathbb{R}^n$ be a path in $\mathbb{R}^n$.
	Then $f = (f_1,f_2,\dots,f_n)$ where $f_k$'s are components of the path $f$.
	Let $P = \{ t_0,t_1,\dots,t_m \}$ be a partition of $[a,b]$.
	Now $f_r(t_j)-f_r(t_{j-1}) \in \mathbb{R}^n$ for each subinterval of $[a,b]$ and each component of $f$.
	Thus for each subinterval $(t_j,t_{j-1})$ we have,
	\[ |f_r(t_j)-f_r(t_{j-1})| \le \| f(t_j)-f(t_{j-1}) \| \le \sum_{j=1}^n |f_k(t_j)-f_k(t_{j-1})| \] 
	Adding inequalities for every subinterval of the partition, we get
	\[ \sum_{k=1}^m |f_k(t_j)-f_k(t_{j-1})| \le \sum_{k=1}^m \| f(t_j)-f(t_{j-1}) \| \le \sum_{k=1}^m \sum_{j=1}^n |f_k(t_j)-f_k(t_{j-1})| \] 
	Rearranging summation, we get
	\[ \sum (P) \le \Lambda_f (P) \le \sum_{j=1}^n \left( \sum (P)_{f_j} \right) \] 
	Suppose $f$ is a rectifiable path.
	Then $f$ has finite arc length $\Lambda_f (a,b)$.
	Let $f_k$ be a component function of $f$.
	Let $P$ be a partition of $[a,b]$.
	Then,
	\[ \sum (P) \le \Lambda_f(P) \le \Lambda_f (a,b) \]
	Thus, $\sum (P)$ is bounded for any partition $P$.
	Therefore, total variation $V_k(a,b)$ exists for each component function $f_k$.
	And $V_k(a,b) \le \Lambda_f(a,b)$.\\

	Suppose $f_k$'s are of bounded variation.
	Then \[ \Lambda_f(P) \le \sum_{j=1}^n \sum (P) \le \sum_{j=1}^n V_j(a,b) \] is bounded for any partition $P$.
	Thus, $f$ is rectifiable.
	And, arc length $\displaystyle \Lambda_f(a,b) \le \sum_{k=1}^n V_k (a,b)$.
	Combining the inequalities, we get
	\[ V_k(a,b) \le \Lambda_f(a,b) \le \sum_{j=1}^n V_j (a,b) \]
\end{proof}
\subsection{Additive and Continuity Properties of Arc length}
\begin{theorem}[additive]
	Let $f : [a,b] \to \mathbb{R}^n$ be a rectifiable path.	
	Let $c \in (a,b)$.
	Then,
	\[ \Lambda_f(a,b) = \Lambda_f(a,c) + \Lambda_f(c,b) \]
\end{theorem}
\begin{proof}
	Let $P$ be a partition of $[a,b]$.
	Then $P' = P \cup \{c\}$ is a refinement of $P$ such that $P' = P_1 \cup P_2$ where $P_1,P_2$ are partition of $[a,c]$ and $[c,b]$ respectively.
	We have,
	\[ \Lambda_f(P) \le \Lambda_f(P') = \Lambda_f(P_1) + \Lambda_f(P_2) \]
	This inequality if true for any partition of $[a,b]$.
	Thus,
	\[ \Lambda_f(a,b) \le \Lambda_f(a,c) + \Lambda_f(c,b) \]
	Let $P_1,P_2$ be partition of $[a,c]$ and $[c,b]$ respectively.
	Then,
	\[ \Lambda_f(P_1) + \Lambda_f(P_2) \le \Lambda_f(P) \le \Lambda_f(a,b) \]
	This inequality if true for any paritions on $[a,c]$ and $[c,b]$.
	Thus,
	\[ \Lambda_f(a,c) + \Lambda_f(c,b) \le \Lambda_f(a,b) \]
\end{proof}
\begin{theorem}[continuity]
	Let $f : [a,b] \to \mathbb{R}^n$ be  rectifiable path.
	Let function $s : [a,b] \to \mathbb{R}$ defined by
	\[ s(x) = \begin{cases} 0 & x = a \\ \Lambda_f(a,x) & x \in (a,b] \end{cases} \]
	Then,
	\begin{enumerate}
		\item function $s$ is continuous and increasing on $[a,b]$.
		\item if there is no subinterval of $[a,b]$ in which $f$ is constant, then $s$ is strictly increasing.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $a \le x < y \le b$.
	Then, $s(y)-s(x) = \Lambda_f(x,y) = \Lambda(a,y) - \Lambda(a,x) \ge 0$.
	Therefore, $s$ in increasing.\\
	
	Suppose $f$ is not constant in any subinterval $[x,y]$ of $[a,b]$.
	Suppose $s$ is not strictly increasing.
	Then, there exists $x,y \in (a,b)$ such that $x < y$ and $s(y)-s(x) = 0$.
	Thus,
	\[ \Lambda_f(a,y) - \Lambda(a,x) = \Lambda_f(x,y) = 0 \]
	Thus, $V_k(x,y) = 0,\ \forall k$ which is a contradition since $f$ is not constant in $[x,y]$.
	Therefore, $s$ is strictly increasing.
\end{proof}
\subsection{Equivalence of path, Change of parameter}
\begin{definition}[change of parameter]
	Let $f:[a,b] \to \mathbb{R}^n$ be a path.
	Let $g : [c,d] \to \mathbb{R}^n$ be another path.
	Then $f,g$ are \textbf{equivalent} if there exists a continuous, real-valued function, $u : [c,d] \to [a,b]$ such that $g = f \circ u$.
	That is, $g(t) = f(u(t)),\ \forall t \in [c,d]$.
	In other words, $f,g$ are different parametric representations of a common graph.\\

	Function $u$ defines a change of parameter.
	If $u$ is strictly increasing, then $f,g$ are in the same direction.
	And $u$ is an orientation preserving change of parameter.
	If $u$ is strictly decreasing, then $f,g$ are in opposite directions.
	And $u$ is an orientation reversing change of parameter.
\end{definition}

\begin{theorem}[change of parameter]
	Let $f:[a,b] \to \mathbb{R}^n$ and $g : [a,b] \to \mathbb{R}^n$ be two paths.
	Let $f,g$ be both injective functions.
	Then $f$ and $g$ are equivalent if they have the same graph.
\end{theorem}
\begin{proof}
	Let $f : [a,b] \to \mathbb{R}^n$ and $g : [c,d] \to \mathbb{R}^n$ be continuous, injective, vector-valued functions.
	Suppose $f,g$ are equivalent paths, then $f,g$ have the same graph.\\

	Suppose $f,g$ have the same graph.
	Since $f$ is injective and continuous on its domain $[a,b]$, function $f^{-1}$ exists and is continuous on its graph.
	\[ \text{Define } u:[c,d] \to [a,b],\ u(t) = f^{-1}(g(t)) \]
	Then $u$ is continuous and $g(t) = f(u(t))$.
	Suppose $u$ is not a strictly monotonic function. 
	Since $u$ is continuous, there exists $t_1,t_2 \in [c,d]$ such that $u(t_1) = u(t_2)$.
	Then $f(u(t_1)) = f(u(t_2)) \implies g(t_1) = g(t_2)$ which is a contradiction since $g$ is injective on $[c,d]$.
\end{proof}

%Module 2
%\chapter{The Riemann Stieltjes Integral} %Rudin chapter 6$
\pagebreak
{\Large Module 2 : Riemann-Stieltjes Integral}\\
\section{The Riemann-Stieltjes Integral}
\begin{definition}[unti step]
	The unit step function $I : \mathbb{R} \to \mathbb{R}$ is defined by
	\[ I(x) = \begin{cases} 0 & x \le 0 \\ 1 & x > 0 \end{cases} \] 
\end{definition}
\begin{definition}[Riemann Integral]
	Let $f$ be a bounded real function defined on $[a,b]$.
	Let $P = \{x_0,x_1,\dots,x_n\}$ be a partition of $[a,b]$.
	Let $M_k = \sup \{ f(x) : x \in [x_{k-1},x_k] \}$ and $m_k = \inf \{ f(x) : x \in [x_{k-1},x_k]$.\\

	Then Riemann upper sum of function $f$ with respect to parition $P$,
	\[ U(P,f) = \sum_P M_k \Delta x_k = \sum_{k=1}^n M_k (x_k-x_{k-1}) \]
	And Riemann lower sum of $f$ with respect to $P$,
	\[ L(P,f) = \sum_{k=1}^n m_k (x_k-x_{k-1}) \]
	Now, Riemann upper integral of $f$ over $[a,b]$,
	\[ \upint_a^b f\ dx = inf \{ U(P,f) : P \in \mathscr{P}[a,b] \} \]
	And, Riemann lower integral of $f$ over $[a,b]$,
	\[ \lowint_a^b f\ dx = sup \{ L(P,f) : P \in \mathscr{P}[a,b] \} \]
	A function $f$ is Riemann integrable over $[a,b]$ if Riemann lower and upper integrals of $f$ over $[a,b]$ are the same.
	Then Riemann integral of $f$ over $[a,b]$,
	\[ \int_a^b f\ dx = \upint_a^b f\ dx = \lowint_a^b f\ dx \]
\end{definition}
\begin{definition}[Riemann-Stieltjes Integral]
	Let $f$ be a bounded function on $[a,b]$.
	Let $\alpha$ be an increasing function on $[a,b]$.
	Let $P = \{ x_0,x_1,\dots,x_n\}$ be a partition of $[a,b]$.
	Then, the Riemann-Stieltjes upper sum of $f$ with respect to partition $P$ and increasing function $\alpha$,
	\[ U(P,f,\alpha) = \sum_{k=1}^n M_k \Delta \alpha_k \]
	where $M_k = \sup \{ f(x) : x \in [x_{k-1},x_k] \}$ and $\Delta \alpha_k = \alpha(x_k) - \alpha(x_{k-1})$.
	Similarly, Riemann-Stieltjes lower sum,
	\[ L(P,f,\alpha) = \sum_{k=1}^n m_k \Delta \alpha_k \]
	where $m_k = \inf \{ f(x) : x \in [x_{k-1},x_k] \}$.
	And function $f$ is Riemann-Stieltjes integrable if Riemann Stieltjes upper and lower integrals are the same.
	\[ \int_a^b f\ d\alpha = \upint_a^b f\ d\alpha = \lowint_a^b f\ d\alpha \]
	where $\displaystyle \upint_a^b f\ d\alpha = \upint_a^b f\ d\alpha(x) = \inf \left\{ U(P,f,\alpha) : P \in \mathscr{P}[a,b] \right\}$ and\\
	$\displaystyle \lowint_a^b f\ d\alpha = \sup \left\{ L(P,f,\alpha) : P \in \mathscr{P}[a,b] \right\}$.\\

	We write $f \in \mathscr{R}(\alpha)$ on $[a,b]$, which means that a bounded, real function $f$ is Riemann-Stieltjes integrable on $[a,b]$ with respect to the increasing function $\alpha$.
\end{definition}

\textbf{Note : } function $\alpha : [a,b] \to \mathbb{R}$ is monotonic (increasing), but not necessarily continuous.\\

\textbf{Remark : } With $\alpha = id$ identity function, Riemann-Stieltjes integral is Riemann integral itself.
	That is, Riemann integral is a special case of Riemann-Stieltjes integral.

\begin{theorem}
	Let $P^\ast$ be a refinement of a parition $P$ of $[a,b]$.
	Then, $L(P,f,\alpha) \le L(P^\ast,f,\alpha)$ and $U(P^\ast,f,\alpha) \le U(P,f,\alpha)$.
\end{theorem}
\begin{proof}
	Let $P^\ast = P \cup \{ x \}$ where $x$ belongs to the $i$th subinterval $(x_{j-1},x_j)$.
	Define $w_1 = \min \{ f(x) : x \in (x_{j-1},x) \}$ and $w_2 = \min \{ f(x) : x \in (x,x_j) \}$.
	Clearly, $w_1,w_2 \ge \min \{ f(x) : x \in (x_{j-1},x_j) \} = m_i$.
	\begin{align*}
	L(P,f,\alpha) - L(P^\ast,f,\alpha) 
		& = m_i \Delta \alpha_j - w_1 (\alpha(x)-\alpha(x_{j-1})) - w_2 (\alpha(x_j) - \alpha(x)) \\
		& = (m_i - w_1)(\alpha(x)-\alpha(x_{j-1})) + (m_i - w_2)(\alpha(x_j)-\alpha(x))\\
		& \ge 0
	\end{align*}
	By mathematical induction the inequality is true for any refinement $P^\ast$ of $P$.
	Therefore, $L(P,f,\alpha) \ge L(P^\ast,f,\alpha)$.\\

	Similarly, define $W_1 = \max \{f(x) : x \in (x_{j-1},x) \}$ and $W_2 = \max \{ f(x) : x \in (x,x_j) \}$
	where $W_1,W_2 \le \max \{ f(x) : x \in (x_{j-1},x_j) \} = M_i$.
	\begin{align*}
	U(P,f,\alpha) - U(P^\ast,f,\alpha) 
		& = M_i \Delta \alpha_j - W_1 (\alpha(x)-\alpha(x_{j-1})) - W_2 (\alpha(x_j) - \alpha(x)) \\
		& = (M_i - W_1)(\alpha(x)-\alpha(x_{j-1})) + (M_i - W_2)(\alpha(x_j)-\alpha(x))\\
		& \le 0
	\end{align*}
	Again, the result is true for any refinement $P$ and we have
	\[ U(P,f,\alpha) \le U(P^\ast,f,\alpha) \]
\end{proof}

\begin{theorem}
	Let $f$ be a bounded, real function on $[a,b]$ and $\alpha$ increasing function on $[a,b]$.
	Then,
	\[ \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \]
\end{theorem}
\begin{proof}
	Let $P_1,P_2$ be any two partition of $[a,b]$.
	Let $P^\ast = P_1 \cup P_2$ be a refinement of both partitions.
	Then, we have $L(P^\ast,f,\alpha) \le U(P^\ast,f,\alpha)$.
	And,
	\[ \lowint_a^b f\ d\alpha \le L(P_1,f,\alpha) \text{ and } U(P_2,f,\alpha) \le \upint_a^b f\ d\alpha \]
	Therefore, \[ L(P_1,f,\alpha) \le L(P^\ast,f,\alpha) \le U(P^\ast,f,\alpha) \le U(P_2,f,\alpha) \]
	Clearly, the inequality holds independent of the choice of the partition.\\
	Taking supremum on right and infimum on left, we get
	\[ \sup_{P \in \mathscr{P}} L(P,f,\alpha) \le \inf_{ P \in \mathscr{P}} U(P,f,\alpha) \]
	Therefore,
	\[ \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \]
\end{proof}

\begin{theorem}[criterion for integrability]
	Let $f$ be  a bounded, real function on $[a,b]$.
	Let $\alpha$ be an increasing function on $[a,b]$.
	Then, $f$ is Riemann-Stieltjes integrable on $[a,b]$ with repesct to $\alpha$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
\end{theorem}
\begin{commentary}
	In other words, $f \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if 
	\[ \forall \varepsilon > 0, \exists P \in \mathscr{P}[a,b] \text{ such that }U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon \]
\end{commentary}
\begin{proof}
	Let $\varepsilon > 0$.
	Suppose there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	We have,
	\[ L(P,f,\alpha) \le \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \le U(P,f,\alpha) \]
	\[ \upint_a^b f\ d\alpha - \lowint_a^b f\ d\alpha \le U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon \]
	Clearly, $\lowint_a^b f\ d\alpha = \upint_a^b f\ d\alpha$.
	Therefore, $f \in \mathscr{R}(\alpha)$.\\

	Suppose $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then by the definition of infimum there exists a partition $P_1$ of $[a,b]$ such that 
	\[ U(P_1,f,\alpha) - \upint_a^b f\ d\alpha < \frac{\varepsilon}{2} \]
	Similarly, there exists a partition $P_2$ of $[a,b]$ such that
	\[ \lowint_a^b f\ d\alpha - L(P_2,f,\alpha) < \frac{\varepsilon}{2} \]
	Consider $P^\ast = P_1 \cup P_2$.
	Clearly, $P^\ast$ is a refinement for both $P_1$ and $P_2$.
	Thus, \[ U(P^\ast,f,\alpha) - \upint_a^b f\ d\alpha < \frac{\varepsilon}{2} \]
	And,
	\[ \lowint_a^b f\ d\alpha - L(P^\ast,f,\alpha) < \frac{\varepsilon}{2} \]
	Thus, \[ U(P^\ast,f,\alpha) - \upint_a^b f\ d\alpha + \lowint_a^b f\ d\alpha - L(P^\ast,f,\alpha) < \varepsilon \]
	Given, $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Therefore, $U(P^\ast,f,\alpha) - L(P^\ast,f,\alpha) < \varepsilon$.
\end{proof}

\begin{theorem}
	Suppose $\varepsilon > 0$ and $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$ for some partition $P$ of $[a,b]$.
	\begin{enumerate}
		\item The inequaility is true for any refinement of $P$.
		\item Let $s_i,t_i \in [x_{i-1},x_i]$ for each subinterval of the partition $P$.
			\[ \sum_{i=1}^n \left| f_(s_i)-f(t_i) \right|\ \Delta\alpha_i < \varepsilon \]
		\item If $f \in \mathscr{R}(\alpha)$ and $t_i \in [x_{i-1},x_i]$ for each subinterval of the partition $P$, then
			\[ \left| \sum_{i=1}^n f(t_i) \Delta \alpha_i - \int_a^b f \ d\alpha \right| < \varepsilon \]
	\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item Let $\varepsilon > 0$.
		Suppose $U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon$.
		Let $P^\ast$ be a refinement of $P$.
		We have $U(P,f,\alpha) \ge U(P^\ast,f,\alpha)$ and $L(P,f,\alpha) \le L(P^\ast,f,\alpha)$.
		Thus,
		\[ U(P^\ast,f,\alpha)-L(P^\ast,f,\alpha) \le U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon \]
	\item Let $s_i,t_i \in (x_{i-1},x_i)$.
		Clearly, for each subinterval $(x_{i-1},x_i)$, we have $m_i \le f(s_i) \le M_i$ and $m_i \le f(t_i) \le M_i$.
		Thus,
		\[ |f(t_i) - f(s_i)| \le  M_i - m_i \]
		\[ \sum_{i=1}^n |f(t_i) - f(s_i)| \Delta \alpha_i \le \sum_{i=1}^n (M_i - m_i) \Delta \alpha_i \le U(P,f,\alpha)-L(P,f,\alpha) \le \varepsilon \]
	\item Let $\varepsilon > 0$.
		Let $P$ be a partition of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
		We have,
		\[ L(P,f,\alpha) \le \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \le U(P,f,\alpha) \]
		Suppose $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
		Then, 
		\[ L(P,f,\alpha) \le \int_a^b f\ d\alpha \le U(P,f,\alpha) \] 

		Let $t_i \in (x_{i-1},x_i)$ for each subinterval of the partition $P$.
		Clearly, $m_i \le f(t_i) \le M_i$.
		Thus, we also have
		\[ L(P,f,\alpha) \le \sum_{i=1}^n f(t_i) \Delta \alpha_i \le U(P,f,\alpha) \]
		Therefore, 
		\[ \left| \sum_{i=1}^n f(t_i) \ \Delta \alpha_i - \int_a^b f\ d\alpha \right| \le U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon \]
\end{enumerate}
\end{proof}
\begin{theorem}
	If $f$ is continuous on $[a,b]$, then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be a continuous function on $[a,b]$.
	Since continuous functions defined on compact subsets metric spaces are uniformly continuous, we have $f$ is uniformly continuous.\\

	Let $\varepsilon > 0$.
	Choose $\eta > 0$ such that $[\alpha(b)-\alpha(a)]\eta < \varepsilon$.
	Since $f$ is uniformly continuous, there exists $\delta > 0$ such that $|f(x)-f(t)| < \eta$ whenever $|x-t| < \delta$.	
	Consider the partition $P$ such that each subinterval is of length less than $\delta$.
	For any $s_i,t_i \in (x_{i-1},x_i)$, we have
	\begin{align*}
	\sum_{i=1}^n [f(t_i) - f(s_i)] \ [\alpha(x_i)-\alpha(x_{i-1})] 
		& \le \sum_{i=1}^n \eta [\alpha(x_i)-\alpha(x_{i-1})] \\
		& \le \eta \sum_{i=1}^n [\alpha(x_i)-\alpha(x_{i-1})] \\
		&	\le \eta[\alpha(b)-\alpha(a)] \\
		& < \varepsilon 
	\end{align*}
	Clearly, the result is true for minimum and maximum values of $f$ in each subinterval of $P$.
	Therefore, we have
	\[ U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon \]
	Thus, $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\begin{theorem}
	If $f$ is monotonic on $[a,b]$ and $\alpha$ is continuous on $[a,b]$, then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be increasing on $[a,b]$.
	Let $\alpha$ is continuous on $[a,b]$.
	Let $n$ be any integer.
	We can construct a partition $P$ of $[a,b]$ such that each the variation of $\alpha$ in each subinterval is fixed.
	That is, $\Delta \alpha_i = \frac{\alpha(b)-\alpha(a)}{n}$.
	Since $f$ is increasing, in each subinterval $[x_{i-1},x_i]$, we have $M_i = f(x_i)$ and $m_i = f(x_{i-1})$.
	Now we have,
	\begin{align*}
	U(P,f,\alpha) - L(P,f,\alpha) 
		& = \sum_{i=1}^n M_i \Delta \alpha_i - \sum_{i=1}^n m_i \Delta \alpha_i \\
		& = \sum_{i=1}^n (M_i-m_i) \Delta \alpha_i \\
		& = \frac{\alpha(b)-\alpha(a)}{n} \sum_{i=1}^n (f(x_i) - f(x_{i-1})) \\
		& = \frac{\alpha(b)-\alpha(a)}{n} (f(b)-f(a))
	\end{align*}
	Thus, given $\varepsilon > 0$, there exists an integer $n$ such that $ (\alpha(b)-\alpha(a))(f(b)-f(a)) < n\varepsilon $.
	And, we get a partition $P$ by fixing $\Delta \alpha_i = \frac{\alpha(b)-\alpha(a)}{n}$ such that $ U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon $.\\

	In other words, $U(P,f,\alpha)-L(P,f,\alpha)$ depends on $n$ and can be reduced to a value less than $\varepsilon$ by increasing the value of $n$.
	Therefore, $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\begin{theorem}
	If $f$ bounded on $[a,b]$, $f$ has only finitely many points of discontinuities on $[a,b]$ and $\alpha$ is continuous at every point at which $f$ is discontinuous.
	Then $f \in \mathscr{R}(\alpha)$.
\end{theorem}
\begin{proof}
	Suppose $f$ is bounded and has only finitely many discontinuities on $[a,b]$.
	Since $f$ is bounded, we have $-M < f(x) < M$ for some real number $M$.
	Also, suppose that $\alpha$ is continuous at those points where $f$ is discontinuous on $[a,b]$.\\

	Let $E$ be the set of all discontinuitites of $f$ on $[a,b]$.
	Clearly, $|E|$ is finite.\\

	Let $\varepsilon > 0$.
	Since, $\alpha$ is continuous at each point $e_j$ of $E$, \textcolor{blue}{there exists open intervals $(u_j,v_j)$ such that $\alpha$ is continuous on those intervals, $e_j$ belongs to the interior of these intervals and the sum of variation of $\alpha$ in those intervals is less than $\varepsilon$.}\\

	Removing these open intervals from $[a,b]$, we get a compact subset $K$ in which $f$ is continuous.
	Since $K$ is compact and $f$ is continuous on $K$, we have $f$ is uniformly continuous on $K$.\\

	Given $\varepsilon > 0$, there exists $\delta > 0$ such that $|f(x)-f(t)| < \varepsilon$ whenever $|x-t|<\delta$.
	Define a partition $P$ such that $u_j,\ v_j \in P$, $P$ doesn't have any point in the interior of any $(u_j,v_j)$.
	And if $x_{i-1} \ne u_j$, then $x_i$ is so choosen that $x_i-x_{i-1} < \delta$, dividing $K$ into subintervals of length less than $\delta$.
	Now, we have
	\begin{align*}
	U(P,f,\alpha) & - L(P,f,\alpha)
		 = \sum_{i=1}^n (M_i-m_i) \Delta \alpha_i  \\
		& \le \varepsilon \sum_{x_{i-1} \ne u_j} \Delta\alpha_i + 2M\sum_{x_{i-1} = u_j} \Delta\alpha_i \\
		& = \varepsilon \sum_{x_{i-1} \ne u_j} \left(\alpha(x_i)-\alpha(x_{i-1}) \right) + 2M\varepsilon \\
		& \le \varepsilon (\alpha(b)-\alpha(a)) + 2M\varepsilon
	\end{align*}
	Clearly, $U(P,f,\alpha)-L(P,f,\alpha)$ is a function of $\varepsilon$ which can reduced below any real number greater than zero by reducing the value of $\varepsilon$.
	Therefore, $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\begin{theorem}
	Suppose $f \in \mathscr{R}(\alpha)$, $m \le f \le M$ on $[a,b]$, $\phi$ is continuous on $[m,M]$ and $h(x) = \phi(f(x))$.
	Then $h \in \mathscr{R}(\alpha)$ on $[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be a function on $[a,b]$ such that $m \le f(x) \le M$ and $f \in \mathscr{R}(\alpha)$ over $[a,b]$.
	Let $\phi$ be  continuous function on $[m,M]$.
	Then $\phi$ is uniformly continuous on $[m,M]$, since $[m,M]$ is compact.
	Thus, given $\varepsilon > 0$ there exists $\delta > 0$ such that $|\phi(x)-\phi(t)| < \varepsilon$ whenever $|x-t|<\delta$.
	Without loss of generality, we may assume that $\delta < \varepsilon$.
	Otherwise choose a value less than $\varepsilon$ as $\delta$.\\

	Since $f \in \mathscr{R}(\alpha)$ over $[a,b]$, there exists a partition $P$ of $[a,b]$ such that
	\[ U(P,f,\alpha) - L(P,f,\alpha) < \delta^2 \]
	Consider the $i$th subinterval $[x_{i-1},x_i]$ of the partition $P$.
	Let $M_i$, $m_i$ be the maximum and minimum values of the function $f$ in $i$th subinterval of $P$.
	Now, we divided the collection of subintervals into two sets depending on the value of $M_i-m_i$ in comparison with $\delta$.
	Define $A$ as the set of all subintervals of $P$ such that $M_i-m_i < \delta$.
	And $B$ as the set of all subintervals of $P$ such that $M_i - m_i \ge \delta$.
	We know that,
	\[ \delta \sum_B \Delta \alpha_i \le \sum_B (M_i-m_i) \Delta \alpha_i < U(P,f,\alpha)-L(P,f,\alpha) < \delta^2 \]
	Thus, $\displaystyle \sum_B \Delta \alpha_i < \delta$.\\

	Define $M_i^\ast$, $m_i^\ast$ as the maximum and minimum of the function $\phi \circ f$ each subinterval $[x_{i-1},x_i]$ of the partition $P$.
	Now, we have
	\begin{align*}
	U(P,\phi \circ f,\alpha) - L(P,\phi \circ f, \alpha)
		& = \sum_{i=1}^n (M_i^\ast - m_i^\ast) \Delta \alpha_i \\
		& = \sum_A (M_i^\ast - m_i^\ast) \Delta \alpha_i + \sum_B (M_i^\ast - m_i^\ast) \Delta \alpha_i 
		\intertext{Since $\phi$ is uniformly continuous,}
		& \le \varepsilon \sum_A \Delta \alpha_i + \sum_B (M_i^\ast-m_i^\ast)\Delta \alpha_i 
	\intertext{Since, continuous functions defined on compact subset attains extrema. We have, $K = \sup \{ |\phi(x)| : x \in [m,M] \}$. And $M_i^\ast - m_i^\ast < 2K$.}
		& \le \varepsilon (\alpha(b)-\alpha(a)) + 2K \sum_B \Delta \alpha_i \\
		& \le \varepsilon (\alpha(b)-\alpha(a)) + 2K\delta \\
		& \le \varepsilon (\alpha(b) - \alpha(a)) + 2K\varepsilon
	\end{align*}
	Since $U(P,\phi \circ f,\alpha)-L(P,\phi \circ f, \alpha)$ is function of $\varepsilon$, it can be reduced to any sufficiently small real number of our choice.
	Therefore, $\phi \circ f \in \mathscr{R}(\alpha)$ over $[a,b]$.
\end{proof}

\subsection{Properties of the Riemann-Stieltjes Integral}
\begin{theorem}
	Let $f,f_1,f_2$ be bounded real functions on $[a,b]$.
	Let $\alpha,\alpha_1,\alpha_2$ be increasing functions on $[a,b]$.
	\begin{enumerate}
		\item If $f_1,f_2,f \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_1+f_2 \in \mathscr{R}(\alpha)$ on $[a,b]$.
			And,
			\[ \int_a^b \left( f_1 + f_2 \right)\ d\alpha = \int_a^b f_1\ d\alpha + \int_a^b f_2\ d\alpha \]
			If $c \in \mathbb{R}$, then $cf \in \mathscr{R}(\alpha)$ on $[a,b]$.
			And,
			\[ \int_a^b cf\ d\alpha = c\int_a^b f\ d\alpha \]
		\item If $f_1(x) \le f_2(x)$ on $[a,b]$, then
			\[ \int_a^b f_1\ d\alpha \le \int_a^b f_2\ d\alpha \]
		\item If $c \in (a,b)$, then $f \in \mathscr{R}(\alpha)$ on $[a,c]$ and $[c,b]$, then
			\[ \int_a^c f\ d\alpha + \int_c^b f\ d\alpha = \int_a^b f\ d\alpha \]
		\item If $|f(x)| \le M$ on $[a,b]$, then 
			\[ \left| \int_a^b f\ d\alpha \right| \le M[\alpha(b)-\alpha(a)] \]
		\item If $f \in \mathscr{R}(\alpha_1)$ and $f \in \mathscr{R}(\alpha_2)$ on $[a,b]$, then $f \in \mathscr{R}(\alpha_1+\alpha_2)$.
			And,
			\[ \int_a^b f\ d(\alpha_1+\alpha_2) = \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]
			If $f \in \mathscr{R}(\alpha)$ on $[a,b]$, and $c \in \mathbb{R}$, then $f \in \mathscr{R}(c\alpha)$ on $[a,b]$.
			And,
			\[ \int_a^b f\ d(c\alpha) = c\int_a^b f\ d\alpha \]
	\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item Let $\varepsilon > 0$.
	Let $f_1,f_2 \in \mathscr{R}(\alpha)$ over $[a,b]$. \\
	Let $P_1$ be a partition of $[a,b]$ such that $U(P_1,f_1,\alpha) - L(P_1,f_1,\alpha) < \frac{\varepsilon}{2}$.\\
	Let $P_2$ be a partition of $[a,b]$ such that $U(P_2,f_2,\alpha) - L(P_2,f_2,\alpha) < \frac{\varepsilon}{2}$.\\
	Let $P = P_1 \cup P_2$ be the refinedment of both the partitions.
	Then the above inequalities are true of the partition $P$ as well.\\

	We have,
	\begin{align*}
	L(P,f_1,\alpha) + L(P,f_2,\alpha) 
		& \le L(P,f_1+f_2,\alpha) \\
		& \le U(P,f_1+f_2,\alpha) \\
		& \le U(P,f_1,\alpha) + U(P,f_2,\alpha) 
	\end{align*}
	Thus,
	\begin{align*}
	U(P,f_1+f_2,\alpha) & - L(P,f_1+f_2,\alpha)\\
		& \le U(P,f_1,\alpha) - L(P,f_1,\alpha) + U(P,f_2,\alpha) - L(P,f_2,\alpha) \\
		& \le \varepsilon
	\end{align*}
	Therefore, $f_1+f_2 \in \mathscr{R}(\alpha)$ over $[a,b]$.\\

	\hrule \vspace{1em}
	\item
	Let $c$ be any real number greater than zero.
	We have,
	\[ M_i' = \max \{ cf(x) : x \in [x_{i-1},x_i] \} = c \max \{ f(x) : x \in [x_{i-1},x_i] \} = cM_i \]
	Similarly, $m_i' = cm_i$.
	Thus,
	\[ \sum_{i=1}^n M_i'\Delta\alpha_i = c\sum_{i=1}^n M_i\Delta \alpha_i \text{ and } \sum_{i=1}^n m_i' \Delta\alpha_i = c\sum_{i=1}^n m_i \Delta\alpha_i \] 
	Therefore, given $\varepsilon > 0$ we have,
	\begin{align*}
		L(P,cf,\alpha) &= cL(P,f,\alpha) \\
		U(P,cf,\alpha) &= cU(P,f,\alpha)
	\end{align*}
	Since, $f \in \mathscr{R}(\alpha)$, there exists partition $P$ such that $U(P,f,\alpha)-L(P,f,\alpha) < \varepsilon$. Thus,
	\[ U(P,cf,\alpha)-L(P,cf,\alpha) = cU(P,f,\alpha)-cL(P,f,\alpha) < c\varepsilon \]
	Therefore, $cf \in \mathscr{R}(\alpha)$ over $[a,b]$.
	\begin{align*}
	\int_a^b cf\ d\alpha 
		& \le U(P,cf,\alpha) \\
		& \le cU(P,f,\alpha) \\
		& \le c \int_a^b f\ d\alpha + c\varepsilon\\
		& \le c\int_a^b f\ d\alpha
	\intertext{Similarly, considering $-f$ and $-cf$ we get}
	\int_a^b cf\ d\alpha 
		& \ge c\int_a^b f\ d\alpha
	\end{align*}
	Therefore, without loss of generality for any real number $c$, 
		\[ \int_a^b cf\ d\alpha = c\int_a^b f\ d\alpha \]

	\hrule \vspace{1em}
	\item 
	Let $\varepsilon > 0$.
	Suppose $f_1,f_2 \in \mathscr{R}(\alpha)$ and $f_1 \le f_2$.
	Let $P_1,P_2$ be the partitions such that $U(P_1,f_1,\alpha) - L(P_1,f_1,\alpha) < \varepsilon$ and $U(P_2,f_2,\alpha) - L(P_2,f_2,\alpha) < \varepsilon$.
	Consider the common refinement $P = P_1 \cup P_2$.
	Then, for each subinterval of the partition $P$, we have
	\[ \min_{x \in [x_{i-1},x_i]} f_1(x) \le \min_{x \in [x_{i-1},x_i]} f_2(x) \text{ and } \max_{x \in [x_{i-1},x_i]} f_1(x) \le \max_{x \in [x_{i-1},x_i]} f_2(x) \]
	Thus, $L(P,f_1,\alpha) \le L(P,f_2,\alpha)$ and $U(P,f_1,\alpha) \le U(P,f_2,\alpha)$.
	\begin{align*}
	\int_a^b f_1\ d\alpha 
		& \le U(P,f_1,\alpha) \\
		& \le U(P,f_2,\alpha) \\
		& \le \int_a^b f_2\ d\alpha + \varepsilon
	\end{align*}
	Therefore,
	\[ \int_a^b f_1\ d\alpha \le \int_a^b f_2\ d\alpha \]

	\hrule \vspace{1em}
	\item 
	Let $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\varepsilon > 0$.
	Then, there exists a parition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	Let $c \in (a,b)$.
	Then $P^\ast = P \cup \{c\} = P_1 \cup P_2$ is refinement of $P$ such that $P_1,P_2$ are partition of $[a,c]$ and $[c,b]$ respectively.
	Clearly, $U(P_1,f,\alpha) - L(P_1,f,\alpha) < \varepsilon$ and $U(P_2,f,\alpha) - L(P_2,f,\alpha) < \varepsilon$.
	Therefore, $f \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.\\

	For any two partitions $P_1,P_2$ of $[a,c]$ and $[c,b]$, there exists partition $P^\ast = P_1 \cup P_2$ of $[a,b]$.
	Thus,
	\begin{align*}
	\int_a^b f\ d\alpha 
		& \le U(P^\ast,f,\alpha) \\
		& \le U(P_1,f,\alpha) + U(P_2,f,\alpha) \\
		& \le \int_a^c f\ d\alpha + \int_c^b f\ d\alpha + 2\varepsilon
	\end{align*}
	Since $\varepsilon$ is arbitrary, we have
	\[ \int_a^b f\ d\alpha \le \int_a^c f\ d\alpha + \int_c^b f\ d\alpha \]
	And considering $-f$ instead of $f$, we get
	\[ \int_a^b -f\ d\alpha \le \int_a^c -f\ d\alpha + \int_c^b -f\ d\alpha \]
	Thus,
	\[ \int_a^b f\ d\alpha \ge \int_a^c f\ d\alpha + \int_c^b f\ d\alpha \]
	Therefore,
	\[ \int_a^b f\ d\alpha = \int_a^c f\ d\alpha + \int_c^b f\ d\alpha \]

	\hrule \vspace{1em}
	\item
	Let function $f \in \mathscr{R}(\alpha)$ on $[a,b]$ and $|f| \le M$.
	Let $P$ be any partition of $[a,b]$.
	We have, 
	\[ L(P,f,\alpha) \le \int_a^b f\ d\alpha \le U(P,f,\alpha)\]
	And,
	\[ L(P,f,\alpha) = \sum_{i=1}^n m_i \Delta \alpha_i \ge -M\Delta \alpha_i = -M \sum_{i=1}^n \Delta \alpha_i = -M[\alpha(b)-\alpha(a)] \]
	Similarly,
	\[ U(P,f,\alpha) = \sum_{i=1}^n M_i \Delta \alpha_i \le M\Delta \alpha_i = M \sum_{i=1}^n \Delta \alpha_i = M[\alpha(b)-\alpha(a)] \]
	Therefore,
	\[ \left| \int_a^b f\ d\alpha \right| \le M[\alpha(b)-\alpha(a)] \]
		
	\hrule \vspace{1em}
	\item
	Let $\alpha_1,\alpha_2$ be monotonic functions on $[a,b]$.
	Then $|\alpha_1 + \alpha_2 | \le |\alpha_1| + |\alpha_2|$.
	Let $f \in \mathscr{R}(\alpha_1)$ on $[a,b]$ and $f \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists partitions $P_1,P_2$ of $[a,b]$ such that
	\[ U(P_1,f,\alpha_1) - L(P_1,f,\alpha_1) < \varepsilon \]
	\[ U(P_2,f,\alpha_2) - L(P_2,f,\alpha_2) < \varepsilon \]
	Consider the common refinement $P = P_1 \cup P_2$.
	Then, the inequalities are true for $P$ as well.
	And for each subinterval $[x_{i-1},x_i]$ of $P$, we have $\Delta(\alpha_1+\alpha_2)_i \le \Delta (\alpha_1)_i + \Delta (\alpha_2)_i$.
	Thus,
	\begin{align*}
	U(P,f,\alpha_1+\alpha_2) - L(P,f,\alpha_1+\alpha_2)
		& = \sum_{i=1}^n (M_i-m_i) \Delta (\alpha_1+\alpha_2)_i \\
		& \le \sum_{i=1}^n (M_i - m_i) \Delta \alpha_{1,i} + \sum_{i=1}^n (M_i-m_i) \Delta \alpha_{2,i} \\
		& \le 2\varepsilon
	\end{align*}
	Therefore, $f \in \mathscr{R}(\alpha_1+\alpha_2)$ on $[a,b]$.
	\begin{align*}
	\int_a^b f\ d(\alpha_1+\alpha_2) 
		& \le U(P,f,\alpha_1+\alpha_2) \\
		& \le U(P,f,\alpha_1) + U(P,f,\alpha_2) \\
		& \le \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 + 2\varepsilon
	\end{align*}
	Thus,
	\[ \int_a^b f\ d(\alpha_1+\alpha_2) \le \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]
	Considering $-f$ instead of $f$,we get
	\[ \int_a^b f\ d(\alpha_1+\alpha_2) \ge \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]
	Therefore,
	\[ \int_a^b f\ d(\alpha_1+\alpha_2) = \int_a^b f\ d\alpha_1 + \int_a^b f\ d\alpha_2 \]

	\hrule \vspace{1em}

	Let $c>0$ and $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	Clearly, $U(P,f,c\alpha) - L(P,f,c\alpha) < c\epsilon$.
	Therefore, $f \in \mathscr{R}(c\alpha)$ on $[a,b]$.
	\begin{align*}
	\int f d(c\alpha) 
		& \le U(P,f,c\alpha) \\
		& \le cU(P,f,\alpha) \\
		& \le c\int f d\alpha + c\varepsilon
	\end{align*}
	Thus,
	\[ \int f d(c\alpha) \le c \int f d\alpha \]
	Taking $-f$ instead of $f$, we get
	\[ \int f d(c\alpha) \ge c \int f d\alpha \]
	Therefore,
	\[ \int f d(c\alpha) = c \int f d\alpha \]
\end{enumerate}
\end{proof}

\begin{theorem}
If $f,g \in \mathscr{R}(\alpha)$ on $[a,b]$, then
\begin{enumerate}
	\item $fg \in \mathscr{R}(\alpha)$ on $[a,b]$
	\item $|f| \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \left| \int_a^b f\ d\alpha \right| \le \int_a^b |f|\ d\alpha \]
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item
	Let $f,g \in \mathscr{R}(\alpha)$ on $[a,b]$.
	By linearity of the integral we have $f+g \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\phi(t) = t^2$.
	Then $\phi$ is continuous.
	Thus, $\phi \circ f$ is continuous on $[a,b]$.
	Therefore, $\phi \circ f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	{\color{blue}Also we have, $\phi \circ (f+g) = (f+g)^2$ and $\phi \circ (f-g) = (f-g)^2$.
	Thus, $(f+g)^2,(f-g)^2 \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Now, $4fg = (f+g)^2 - (f-g)^2$.}
	Therefore, $4fg \in \mathscr{R}(\alpha)$ on $[a,b]$, from linearity of the integral.
	Take $c = \frac{1}{4}$, we get $fg \in \mathscr{R}(\alpha)$ on $[a,b]$ from linearlity of the integral.\\

	\hrule \vspace{1em}
	\item
	Let $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $\phi(t) = |t|$.
	Then $\phi$ is continuous.
	Therefore, $\phi \circ f = |f| \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	{\color{blue}Let $c = \pm 1$ such that
	\[ c\int f d\alpha \ge 0 \]
	Then, 
	\[ \left|\int f d\alpha \right| = c\int f d\alpha = \int cf d\alpha \le \int |f| d\alpha \]
	since $cf \le |f|$. }
\end{enumerate}
\end{proof}

\begin{definition}[step]
	The unit step function, $I : \mathbb{R} \to [0,1]$ is defined by
		\[ I(x) = \begin{cases} 0 & x \le 0 \\ 1 & x > 0 \end{cases} \]
\end{definition}

\begin{theorem}
	Let $f$ be bounded on $[a,b]$ and continuous at $s \in (a,b)$.
	Let $\alpha = I(t-s)$.
	Then,
		\[ \int_a^b f d\alpha = f(s) \]
\end{theorem}
\begin{proof}
	Let $P = \{ a,s,x_2,b \}$ be a partition of $[a,b]$.
	Since $f$ is bounded, 
		\[ U(P,f,\alpha) = M_1 \Delta\alpha_1 + M_2 \Delta \alpha_2+ M_3 \Delta\alpha_3 = M_2 \]
	since $\Delta\alpha_1 = 0$, we have $\Delta\alpha_2 = \alpha(x_2) - \alpha(s) = I(x_2-s) - I(0) = 1-0$ and $\Delta \alpha_3 = 0$.
	Similarly, $L(P,f,\alpha) = m_1\Delta\alpha_1 + m_2\Delta\alpha_2 + m_3\Delta\alpha_3 = m_2$.
	We have,
		\[ m_2 = L(P,f,\alpha) \le \lowint_a^b f d\alpha \le \upint_a^b f d\alpha \le U(P,f,\alpha) = M_2 \]
	We know that, $M_2$ is the maximum value of $f$ in the subinteval $[s,x_2]$.
	The lower sum and upper sum remains the same as we reduce the length of the second subinterval.
	We also know that, $f$ is continuous at $s$.
	As $x_2 \to s$, the maximum value of $f$ tends to the value of $f$ at $s$, say $f(s)$.
	Similarly,  $m_2 \to f(s)$ as $x_2 \to s$.
	Thus,
		\[ f(s) \le \lowint_a^b f d\alpha \le \upint_a^b f d\alpha \le f(s) \]
	Therefore, $f \in \mathscr{R}(\alpha)$ on $[a,b]$ and
		\[ \int_a^b f d\alpha = f(s) \]
\end{proof}

\begin{theorem}
	Suppose $\displaystyle \sum_{i=1}^\infty c_n$ converges and $c_n \ge 0$.
	Let $f$ be continuous on $[a,b]$.
	Let sequence $\sequence{s_n}$ be a strictly increasing sequence in $(a,b)$.
	Let $\displaystyle \alpha = \sum_{i=1}^\infty c_n I(t-s_n)$.
	Then,
	\[ \int_a^b f\ d\alpha = \sum_{i=1}^\infty c_n f(s_n) \]
\end{theorem}
\begin{proof}
	Let $\sum_{n=1}^\infty c_n$ be a convergent series.
	Given $\varepsilon > 0$, there exists a natural number $N$ such that 
	\[ \sum_{n=N+1}^\infty c_n < \varepsilon \]
	Let $s_1,s_2,s_3,\dots$ be distinct points in $(a,b)$.
	Without loss of generality, sequence $\sequence{s_n}$ is a striclty increasing sequence in $[a,b]$.
	Let $\alpha = \sum_{n = 1}^\infty c_n I(t-s_n)$.
	Then, $\alpha(a) = 0$ and $\alpha(b) = \sum_{n=1}^\infty c_n$.
	\[ 0 \le \alpha(x) \le \sum_{n=1}^\infty c_n, \quad \forall x \in [a,b] \]
	By comparison test, $\alpha$ is convergent in $[a,b]$.
	We have,
	\[ \int_a^b f(t)\ d(c_1 I(t-s_1)) = c_1 \int_a^b f(t)\ d(I(t-s_1)) =  c_1 f(s_1) \]
	Let $\alpha = \alpha_1 + \alpha_2$ where $\displaystyle \alpha_1 = \sum_{n=1}^N c_n I(t-s_n)$ and $\displaystyle\alpha_2 = \sum_{n=N+1}^\infty c_n I(t-s_n)$.
	And from mathematical induction, we have
	\begin{align*}
	\int_a^b f(t) d(\alpha_1) 
		& = \int_a^b f(t)\ d\left( \sum_{n=1}^N c_n I(t-s_n) \right)\\
		& = \sum_{n=1}^N \int_a^b f(t)\ d(c_nI(t-s_n)) \\
		& = \sum_{n=1}^N c_n \int_a^b f dI(t-s_n) \\
		& = \sum_{n=1}^N c_n f(s_n) 
	\end{align*}
	We have,
	\[ \left| \int_a^b f(t) d\alpha_2 \right| \le M (\alpha_2(b) - \alpha_2(a)) = M \sum_{n=N+1}^\infty < M\varepsilon \]
	where $M = \sup |f(x)|$.
	Thus,
	\begin{align*}
	\left| \int_a^b fd\alpha_2 \right| 
		& = \left| \int_a^b f d(\alpha_1+\alpha_2) - \int_a^b fd\alpha_1 \right| \\
		& = \left| \int_a^b fd\alpha - \sum_{n=1}^N c_n f(s_n) \right| \\
		& < M\varepsilon 
	\end{align*}
	The inequality is true as $N \to \infty$. 
	In other words, the sequence of partial sums converges to the value of integral of $f$.
	Therefore,
	\[ \sum_{n=1}^\infty c_n f(s_n) = \int_a^b f d\alpha \]
\end{proof}

\begin{theorem}
	Let $\alpha$ be increasing function on $[a,b]$ and $\alpha' \in \mathscr{R}$.
	Let $f$ be bounded real-valued function on $[a,b]$.
	Then $f \in \mathscr{R}(\alpha)$ if and only if $f\alpha' \in \mathscr{R}$.
	And
	\[ \int_a^b f d\alpha = \int_a^b f(x)\alpha'(x) dx \]
\end{theorem}
\begin{proof}
	Since $\alpha' \in \mathscr{R}$ on $[a,b]$, for any $\varepsilon > 0$, there exists a partition $P$ of $[a,b]$ such that $U(P,\alpha')-L(P,\alpha') < \varepsilon$.
	We have, $\alpha$ is continuous on $[a,b]$, as it is differentiable on $[a,b]$.
	Then by intermediate value theorem, we have
	\[ \Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1}) = \alpha'(t_i) (x_i-x_{i-1}) \]
	where $t_i \in [x_{i-1},x_i]$.
	Let $s_i \in [x_{i-1},x_i]$.
	Then,
	\[ \sum_{i=1}^n |\alpha'(s_i) - \alpha'(t_i)|\Delta x_i \le U(P,\alpha') - L(P,\alpha') < \varepsilon \]
	Let $M = \sup |f|$.
	\textcolor{blue}{Let $u_i \in [x_{i-1},x_i]$.}
	\footnote{In my opinion, using $u_i$ instead of $s_i$ in the first sum makes it simpler.}
	Consider,
	\begin{align*}
	\left| \sum_{i=1}^n f(u_i) \Delta \alpha_i - \sum_{i=1}^n f(s_i) \alpha'(s_i) \Delta x_i \right| 
		& \le M \left| \sum_{i=1}^n \Delta \alpha_i - \alpha'(s_i) \Delta x_i \right| \\
		& \le M \left| \sum_{i=1}^n \alpha'(t_i) \Delta x_i - \alpha'(s_i) \Delta x_i \right| \\
		& \le M  \sum_{i=1}^n |\alpha'(t_i) - \alpha'(s_i)| \Delta x_i \\
		& < M \varepsilon 
	\end{align*}
	Clearly, the inequality is true independent of the choice of $u_i,s_i \in [x_{i-1},x_i]$.
	Thus, selecting $u_i$ such that $f(u_i) = M_i$, we get
	\[ \left| U(P,f,\alpha) - \sum_{i=1}^n \alpha'(s_i)\Delta x_i \right| < M\varepsilon \]
	Therefore,
	\[ \sum_{i=1}^n f(s_i) \alpha'(s_i) \Delta x_i < U(P,f,\alpha) + M\varepsilon \]
	Selecting $s_i$ such that $f\alpha'$ attains maximum at $s_i$, we get
	\[ U(P,f\alpha') < U(P,f,\alpha) + M\varepsilon \]
	Now, first we select $s_i$ such that $f\alpha(s_i)$ is maximum in $i$th subinterval of $P$.
	Then,
	\[ \left| \sum_{i=1}^n f(u_i) \Delta \alpha_i - U(P,f\alpha') \right| < M\varepsilon \]
	Therefore,
	\[ U(P,f\alpha') < \sum_{i=1}^n f(u_i) \Delta \alpha_i + M\varepsilon \]
	And, now selecting $u_i$ such that $f(u_i) = M_i$.
	\[ U(P,f\alpha') < U(P,f,\alpha) + M\varepsilon \]
	Therefore, we have $U(P,f\alpha') = U(P,f,\alpha)$.
	Clearly, it is true for any refinement of $P$.
	Therefore,
	\[ \upint_a^b f\alpha' dx = \upint_a^b f d\alpha \]
	Similary, by selecting $u_i$ and $s_i$ such that $f(u_i)$, $f\alpha'(s_i)$ is minimum in each subinterval of $P$, we get
	\[ \lowint_a^b f\alpha' dx = \lowint_a^b f d\alpha \]
	Cleary, $f \in \mathscr{R}(\alpha) \iff f\alpha' \in \mathscr{R}$. And,
	\[ \int_a^b f\alpha' dx = \int_a^b f d\alpha \]
\end{proof}

\begin{theorem}[change of variable]
	Let $\varphi : [A,B] \to [a,b]$ be a strictly increasing, continuous function onto $[a,b]$.
	Let $\alpha : [a,b] \to \mathbb{R}$ be an increasing function and $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Define $\beta = \alpha \circ \varphi$ and $g = f \circ \varphi$.
	Then $g \in \mathscr{R}(\beta)$ on $[A,B]$ and 
	\[ \int_A^B g d\beta = \int_a^b f d\alpha \]
\end{theorem}
\begin{proof}
	Let $P = \{x_0,x_1,\dots,x_n\}$ be any partition of $[a,b]$.
	Then there exists a partition $Q = \{ y_0,y_1,\dots,y_n\}$ of $[A,B]$ such that $x_j = \varphi(y_j)$ since $\varphi$ is a continuous, bijection.
	\dag\footnote{Strictly increasing functions are always injective.}
	Similarly, for any partition $Q = \{ y_0,y_1,\dots,y_n\}$ of $[A,B]$, there exists a partition $P = \{ x_0,x_1,\dots,x_n\}$ of $[a,b]$ where $x_j= \varphi(y_j)$.
	Clearly, minimum/maximum of $g$ in $i$th subinterval of $Q$ is same as the minimum/maximum of $f$ in $i$th subinterval of $P$.
	And 
	\[ \Delta \beta_i = \beta(y_i) - \beta(y_{i-1}) = \alpha(\varphi(y_i)) - \alpha(\varphi(y_{i-1})) = \alpha(x_i) - \alpha(x_{i-1}) = \Delta \alpha_i \]
	\begin{align*}
	\min \{ g(y) : y \in [y_{i-1},y_i] \}
		& = \min \{ f(\varphi(y)) : y \in [y_{i-1},y_i] \} \\
		& = \min \{ f(x) : x \in [x_{i-1},x_i] \} \\
	\implies L(Q,g,\beta) 
		& = L(P,f,\alpha) \\
	\max \{ g(y) : y \in [y_{i-1},y_i] \}
		& = \max \{ f(\varphi(y)) : y \in [y_{i-1},y_i] \} \\
		& = \max \{ f(x) : x \in [x_{i-1},x_i] \} \\
		\implies U(Q,g,\beta) & = U(P,f,\alpha) \\
	\end{align*}
	Since $f \in \mathscr{R}(\alpha)$ on $[a,b]$, there exists a partition $P$ of $[a,b]$ such that $U(P,f,\alpha) - L(P,f,\alpha) < \varepsilon$.
	Therefore, there exists a partition $Q$ of $[A,B]$ such that $U(Q,g,\beta) - L(Q,g,\beta) < \varepsilon$.
	Thus, $g \in \mathscr{R}(\beta)$.
	And
	\[ \int_A^B g d\beta = \int_a^b f d\alpha \]
\end{proof}

\subsection{Integration and Differentiation}
\begin{theorem}
	Let $f \in \mathscr{R}$ on $[a,b]$.
	Let $a \le x \le b$.
	Define
	\[ F(x) = \int_a^x f(t)dt \]
	Then $F$ is continuous on $[a,b]$.
	Furthermore, if $f$ is continuous at $x_0$, then $F$ is differentiable at $x_0$.
	And $F'(x_0) = f(x_0)$.
\end{theorem}
\begin{proof}
	Let $a\le x < y \le b$.
	Let $M = \sup |f|$.
	We have,
	\begin{align*}
	|F(y)-F(x)| 
		& = \left| \int_a^y f(t) dt - \int_a^x f(t) dt \right| \\
		& = \left| \int_a^x f(t) dt + \int_x^y f(t) dt - \int_a^x f(t) dt \right| \\
		& = \left| \int_x^y f(t) dt \right| \\
		& \le M(y-x)
	\end{align*}
	Thus, given $\varepsilon > 0$, there exists $\delta > 0$ such that $|F(y)-F(x)| < \varepsilon$ whenever $|y-x| < \delta \le \frac{\varepsilon}{M}$.
	Therefore, $F$ is continuous on $[a,b]$.\\

	Let $f$ be continuous at $x_0 \in [a,b]$.
	Given $\varepsilon > 0$, there exists $\delta > 0$ such that $|f(t)-f(x_0)| < \varepsilon$ whenever $|t-x_0| < \delta$ and $t \in [a,b]$.\\

	Let $x-\delta < s \le x_0 \le t < x+\delta$.
	Clearly, $|f(u)-f(x_0)| < \varepsilon$ whenever $u \in [s,t]$.
	Consider,
	\begin{align*}
	\left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| 
		& = \left| \frac{1}{t-s} \left( \int_a^t f(u) du - \int_a^s f(u) du \right) - \frac{1}{t-s} f(x_0)(t-s) \right| \\
		& = \left| \frac{1}{t-s} \int_s^t f(u) du  - \frac{1}{t-s} \int_s^t f(x_0)du \right| \\
		& = \left| \frac{1}{t-s} \left( \int_s^t (f(u)-f(x_0)) du \right) \right| \\
		& < \frac{\varepsilon}{t-s} \int_s^t du \quad \text{ since } |f(u)-f(x_0)| < \varepsilon  \\
		& < \varepsilon
	\end{align*}
	Clearly, as $\varepsilon \to 0$, $\delta \to 0$.
	Then, $s,t \to x_0$.
	Therefore, $F$ differentiable at $x_0$ and
	\[ F'(x_0) = \lim_{s,t \to x_0} \frac{F(t)-F(s)}{t-s} =  f(x_0) \]
\end{proof}

\begin{theorem}[fundamental theorem of calculus]
	Let $f \in\mathscr{R}$ on $[a,b]$.
	Let $F$ be a differentiable function on $[a,b]$ such that $F'=f$.
	Then,
	\[ \int_a^b f(x)dx = F(b)-F(a) \]
\end{theorem}
\begin{proof}
	Let $\varepsilon > 0$.
	Let $f \in \mathscr{R}$ on $[a,b]$.
	Then, there exists a partition $P = \{ x_0,x_1,\dots,x_n\}$ of $[a,b]$ such that $U(P,f)-L(P,f) < \varepsilon$.\\

	Let $F$ be differentiable function such that $F'=f$.
	Then, $F$ is continuous.
	By intermediate value theorem, there exists $t_i \in [x_{i-1},x_i]$ such that 
	\[ F(x_i) - F(x_{i-1}) = F'(t_i) (x_i-x_{i-1}) = f(t_i)\Delta x_i \]
	Clearly,
	\[ F(b) - F(a) = \sum_{i=1}^n \left( F(x_i) - F(x_{i-1}) \right) = \sum_{i=1}^n f(t_i) \Delta x_i \]
	Since $t_i \in [x_{i-1},x_i]$, we have $m_i \le f(t_i) \le M_i$.
	Thus,
	\[ \left| \int_a^b f(x) dx - \left(F(b)-F(a)\right) \right| = \left| \int_a^b f(x) dx - \sum_{i=1}^n f(t_i) \Delta x_i \right| < \varepsilon \]
	Therefore, 
	\[ \int_a^b f(x) dx = F(b) - F(a) \]
\end{proof}

\begin{theorem}[integration by parts]
	Let $F,G$ be differentiable $[a,b]$.
	Let $F' = f \in \mathscr{R}$ and $G' = g \in \mathscr{R}$.
	Then,
	\[ \int_a^b F(x)g(x) dx = F(b)G(b) - F(a)G(a) - \int_a^b f(x)G(x) dx \]
\end{theorem}
\begin{proof}
	Let $F,G$ be differentiable functions on $[a,b]$ and $F'=f \in \mathscr{R}$ and $G'=g \in \mathscr{R}$.
	Let $H = FG$. 
	Then, $H' = FG' + F'G = Fg + fG$.
	\[ \int_a^b H'(x) dx = \int_a^b F(x)g(x) dx + \int_a^b f(x)G(x) dx \]
	By fundamental theorem of calculus, we also have
	\[ \int_a^b H'(x) = H(b) - H(a) = FG(b) - FG(a) = F(b)G(b) - F(a)G(a) \]
	Rearranging the terms, we get
	\[ \int_a^b F(x)g(x) dx = F(b)G(b) - F(a)G(a) - \int_a^b f(x)G(x) dx \]
\end{proof}

\subsection{Integration of Vector-valued Functions}
\begin{definition}[integrable]
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$ be a vector-valued function.
	Let $\alpha$ be a monotonic function on $[a,b]$.
	Let $f_1,f_2,\dots,f_k$ be the component functions of $\bar{f}$.
	That is, $\bar{f}(x) = \left( f_1(x),f_2(x),\dots,f_k(x) \right)$.
	Then $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if every component function $f_j \in \mathscr{R}(\alpha)$.
\end{definition}
\begin{important}
	In other words, vector-valued function $\bar{f}$ is integrable if and only if every component function of $\bar{f}$ is integrable.
\end{important}

\begin{commentary}
\begin{theorem}[properties]
	Suppose $\bar{f},\bar{g}$ be vector-valued functions from $[a,b]$ into $\mathbb{R}^k$.
\begin{enumerate}
	\item Let $\bar{f},\bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then $\bar{f} + \bar{g} \in \mathscr{R}(\alpha)$.
	And,
	\[ \int_a^b \bar{f} + \bar{g} d\alpha = \int_a^b \bar{f} d\alpha + \int_a^b \bar{g} d \alpha \]
	If $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$ and $c \in \mathbb{R}$, then $c\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \int_a^b c\bar{f} d\alpha = c \int_a^b \bar{f} d\alpha \]
	\item Let $c \in (a,b)$.
	Let $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then $\bar{f} \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.
	And,
	\[ \int_a^b \bar{f} d\alpha = \int_a^c \bar{f} d\alpha + \int_c^b \bar{f} d\alpha \]
	\item Let $\alpha_1,\alpha_2$ be monotonic functions on $[a,b]$.
	Let $\bar{f} \in \mathscr{R}(\alpha_1)$ on $[a,b]$ and $\bar{f} \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	Then, $\bar{f} \in \mathscr{R}(\alpha_1+\alpha_2)$ on $[a,b]$.
	And,
	\[ \int_a^b \bar{f}d(\alpha_1+\alpha_2) = \int_a^b \bar{f} d\alpha_1 + \int_a^b \bar{f} d\alpha_2 \]
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item
	Let $\bar{f},\bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$ where $\bar{f} = (f_1,f_2,\dots, f_k)$ and $\bar{g} = (g_1,g_2,\dots, g_k)$.
	By definition of integrability of vector-valued functions, the component functions $f_j,g_j \in \mathscr{R}(\alpha)$ on $[a,b]$ for $1 \le j \le k$.\\

	We also know that, if $f_j, g_j \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_j + g_j \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Thus, $f_1+g_1, f_2+g_2,\dots, f_k + g_k \in \mathscr{R}(\alpha)$ on $[a,b]$ for $1 \le j \le k$.
	Therefore, $\bar{f}+\bar{g} = (f_1+g_1,f_2+g_2,\dots, f_k+g_k) \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	\hrule \vspace{1em}

	Let $c \in \mathbb{R}$.
	Let $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then, $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha)$ on $[a,b]$.
	We know that, $cf_1 \in \mathscr{R}(\alpha)$ on $[a,b]$, since $f_1 \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Thus, $cf_1,cf_2,\dots,cf_k \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Therefore, $c\bar{f} = (cf_1,cf_2,\dots,cf_k) \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	\hrule \vspace{1em}
	\item
	Let $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then, $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Let $c \in (a,b)$.
	We know that, if $f_j \in \mathscr{R}(\alpha)$ on $[a,b]$, then $f_j \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.
	Thus, $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.
	Therefore, $\bar{f} \in \mathscr{R}(\alpha)$ on both $[a,c]$ and $[c,b]$.\\

	\hrule \vspace{1em}
	\item
	Let $\bar{f} \in \mathscr{R}(\alpha_1)$ and $\bar{f} \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	Then $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha_1)$ and $f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha_2)$ on $[a,b]$.
	We know that, if $f_j \in \mathscr{R}(\alpha_1)$ and $f_j \in \mathscr{R}(\alpha_2)$ on $[a,b]$, then $f_j \in \mathscr{R}(\alpha_1+\alpha_2)$ on $[a,b]$.
\end{enumerate}
\end{proof}
\end{commentary}
\begin{challenge}
	Let $\bar{f},\bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$ where $\bar{f},\bar{g} : [a,b] \to \mathbb{R}^k$.
	Define $\bar{f} \cdot \bar{g} : [a,b] \to \mathbb{R}$ by $(\bar{f} \cdot \bar{g})(x) = f_1(x)g_1(x) + f_2(x)g_2(x) + \dots + f_k(x)g_k(x)$.
	Then, $\bar{f} \cdot \bar{g} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \int_a^b \bar{f} \cdot \bar{g} d\alpha = \left( \int_a^b \bar{f} d\alpha \right) \cdot \left( \int_a^b \bar{g} d\alpha \right) \]
\end{challenge}

\begin{commentary}
\begin{theorem}
	Let $\alpha$ be a monotonic function such that $\alpha' \in \mathscr{R}$ on $[a,b]$.
	Let $\bar{f}$ be a bounded function on $[a,b]$.
	Then $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$ if and only if $f\alpha' \in \mathscr{R}$ on $[a,b]$.
	And,
		\[ \int_a^b \bar{f} d\alpha = \int_a^b \bar{f}\alpha' dx \]
\end{theorem}
\begin{proof}
	Let $\bar{f} = (f_1,f_2,\dots,f_k)$.
	Then, $\bar{f}\alpha' = (f_1\alpha', f_2\alpha',\dots,f_k\alpha')$.\\
	We already know that, $f_j \in \mathscr{R}(\alpha) \iff f_j\alpha' \in \mathscr{R}$.\\
	Therefore,
	\begin{align*}
	\bar{f} \in \mathscr{R}(\alpha) 
		& \iff f_1,f_2,\dots,f_k \in \mathscr{R}(\alpha) \\
		& \iff f_1\alpha', f_2\alpha', \dots, f_k\alpha' \in \mathscr{R} \\
		& \iff \bar{f}\alpha' \in \mathscr{R}
	\end{align*}
	Thus, $\bar{f} \in \mathscr{R}(\alpha) \iff \bar{f}\alpha' \in \mathscr{R}$.\\

	\hrule \vspace{1em}
	We also know that,
		\[ \int_a^b f_j d\alpha = \int_a^b f_j\alpha' dx \]
	Thus,
	\begin{align*}
	\int_a^b \bar{f} d\alpha 
		& = \left( \int_a^b f_1 d\alpha, \int_a^b f_2 d\alpha, \dots,\int_a^b f_k d\alpha \right) \\
		& = \left( \int_a^b f_1\alpha' dx, \int_a^b f_2\alpha' dx,\dots, \int_a^b f_k\alpha' dx \right)\\
		& = \int_a^b \bar{f}\alpha' dx
	\end{align*}
\end{proof}
\end{commentary}

\begin{commentary}
\begin{theorem}
	Let $\bar{f} \in \mathscr{R}$ on $[a,b]$ where $\bar{f} : [a,b] \to \mathbb{R}^k$.
	Define $\bar{F} : [a,b] \to \mathbb{R}^k$ defined by
		\[ \bar{F}(x) = \int_a^x \bar{f}(t) dt \]
	Then $F$ is continuous on $[a,b]$.
	Furthermore, if $\bar{f}$ is continuous at $x_0 \in [a,b]$, then $\bar{F}$ is differentiable at $x_0$ and $\bar{F}'(x_0) = \bar{f}(x_0)$.
\end{theorem}
\begin{proof}
	We know that, $\bar{f} = (f_1,f_2,\dots,f_k)$.
	And from the definition of integral, we have
		\[ \int_a^b \bar{f}(t) dt = \left( \int_a^b f_1(t) dt, \int_a^b f_2(t) dt,\dots, \int_a^b f_k(t) dt \right) \]
	We know that, for $1 \le j \le k$, the function $F_j : [a,b] \to \mathbb{R}$ defined by 
		\[ F_j(x) = \int_a^x f_j(t) dt \]
	is continuous.
	And if $f_j$ is continuous at $x_0$, then $F_j$ is differentiable at $x_0$ and $F_j'(x_0) = f_j(x_0)$.
	Clearly, $\bar{F} = (F_1,F_2,\dots,F_k)$ is continuous, since each component function is continuous.
	And, $\bar{F}$ is differentiable at $x_0$ and
		\[ \bar{F}'(x_0) = \left( F_1'(x_0),F_2'(x_0),\dots,F_k'(x_0) \right) = (f_1(x_0),f_2(x_0),\dots,f_k(x_0)) = \bar{f}(x_0) \]
\end{proof}
\end{commentary}

\begin{theorem}[fundamental theorem of calculus for vector-valued functions]
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$.
	Let $\bar{F} : [a,b] \to \mathbb{R}^k$.
	If $\bar{f} \in \mathscr{R}$ on $[a,b]$ and $\bar{F}' = \bar{f}$, then
		\[ \int_a^b \bar{f}(t) dt = \bar{F}(b) - \bar{F}(a) \]
\end{theorem}
\begin{proof}
By fundamental theorem of calculus, we have
	\[ \int_a^b f_j(t) dt = F_j(b) - F_j(a) \]
 	for $1 \le j \le k$.
Therefore,
\begin{align*}
	\int_a^b \bar{f}(t) dt
		& = \left( \int_a^b f_1(t) dt, \int_a^b f_2(t) dt, \dots, \int_a^b f_k(t) dt \right) \\
		& = \left( F_1(b)-F_1(a), F_2(b) - F_2(a), \dots, F_k(b) - F_k(a) \right) \\
		& = \left( F_1(b),F_2(b),\dots,F_k(b)\right) - \left(F_1(a), F_2(a),\dots,F_k(a)\right) \\
		& = \bar{F}(b) - \bar{F}(a)
\end{align*}
\end{proof}

\begin{theorem}
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$.
	If $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$, then $|\bar{f}| \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And
	\[ \left| \int_a^b \bar{f} d\alpha \right| \le \int_a^b |\bar{f}| d\alpha \]
\end{theorem}
\begin{proof}
	Let $\bar{f} : [a,b] \to \mathbb{R}^k$.
	Then, we have $\bar{f} = (f_1,f_2,\dots,f_k)$.\\
	Suppose $\bar{f} \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Then, $f_j \in \mathscr{R}(\alpha)$ on $[a,b]$ for $1 \le j \le k$.
	We have, $|\bar{f}| = \left( f_1^2+f_2^2+\dots+f_k^2 \right)^\frac{1}{2}$.
	We know that if $f_j \in \mathscr{R}(\alpha)$, then $f_j^2 \in \mathscr{R}(\alpha)$.
	Again, $\sum f_j^2 \in \mathscr{R}(\alpha)$.\\

	Consider $g : [a,b] \to \mathbb{R}$ given by $g(x) = \sum_{j=1}^k f_j^2(x)$.
	\textcolor{blue}{We have $g \in \mathscr{R}(\alpha)$ on $[a,b]$.}
	Thus $g$ is bounded and there exists $m,M$ such that $m \le g \le M$.
	Clearly, $g \ge 0$.\\

	Consider, the function $\phi : [m,M] \to \mathbb{R}$ given by $\phi(x) = \sqrt{x}$.
	Clearly, $\phi$ is well-defined on $[m,M]$ since $0 \le m$.
	And, $\phi$ is continuous on $[m,M]$.
	Thus, $|\bar{f}| = \phi \circ g = \sqrt{\sum f_j^2} \in \mathscr{R}(\alpha)$ on $[a,b]$.\\

	\hrule \vspace{1em}
	
	Let $\displaystyle \bar{y} = \int_a^b \bar{f} d\alpha$.
	Then, $\bar{y} = (y_1,y_2,\dots,y_k)$ and $\displaystyle y_j = \int_a^b f_j d\alpha$.
	\begin{align*}
	|\bar{y}|^2 
		& = y_1^2 + y_2^2 + \dots + y_k^2\\
		& = y_1 \int_a^b f_1 d\alpha + y_2 \int_a^b f_2 d\alpha + \dots + y_k \int_a^b f_k d\alpha \\
		& = \int_a^b (y_1f_1 + y_2f_2 + \dots + y_kf_k) d\alpha
	\end{align*}
	By Schwarz inequality,
		\[ \sum_{j=1}^n y_j f_j \le \left( \sum_{j=1}^n y_j^2 \right)^\frac{1}{2} \left( \sum_{j=1}^n f_j^2 \right)^\frac{1}{2} = |\bar{y}| \ |\bar{f}| \]
	Thus,
		\[ {\color{blue}|\bar{y}|^2} = \int_a^b \left( \sum_{j=1}^k y_jf_j \right) d\alpha \le \int_a^b |\bar{y}|\ |\bar{f}| d\alpha = |\bar{y}| \int_a^b |\bar{f}| d\alpha \]
	Therefore,
		\[ \left| \int_a^b \bar{f} d\alpha \right| = |\bar{y}| \le \int_a^b |\bar{f}| d\alpha \]

\end{proof}

\pagebreak

%\chapter{Sequence \& Series of Functions}
{\Large Module 3}
\section{Sequence and Series of functions}
\begin{definition}
	Let sequence $\sequence{f_n}$ be a sequence of functions defined on $E$.
	Suppose sequence $\sequence{f_n(x)}$ converges forevery $x \in E$.
	Then, sequence $\sequence{f_n}$ converges.
	And \textbf{limit function} $f : E \to \mathbb{R}$ is defined by
	\[ f(x) = \lim_{n \to \infty} f_n(x) \]
\end{definition}
\begin{definition}
	Let $f_n : E \to \mathbb{R},\ \forall n \in \mathbb{N}$.
	Suppose $\sum f_n(x)$ converges for every $x \in E$.
	Then, series $\sum f_n$ converges.
	And \textbf{sum} $f : E \to \mathbb{R}$ is defined by
	\[ f(x) = \sum_{n=1}^\infty f_n(x) \]
\end{definition}

\subsection{Counter Examples - Illustrating Main problem}
\textbf{Main Problem} : Can we obtain the sufficient conditions for preserving desirable properties under convergence ?
\subsubsection*{Interchanging limits}
\[ \text{Generally,}\quad \lim_{n \to \infty} \lim_{m \to \infty} S_{n,m} \ne \lim_{m \to \infty} \lim_{n \to \infty} S_{n,m} \]
\begin{proof}
Consider, $S_{n,m} = \frac{m}{m+n}$.
\begin{align*}
	\lim_{m \to \infty} \lim_{n \to \infty} S_{n,m} & = \lim_{m \to \infty } 0 = 0 \\
	\lim_{n \to \infty} \lim_{m \to \infty} S_{n,m} & = \lim_{n \to \infty } 1 = 1 
\end{align*}
\end{proof}
\subsubsection{Continuity}
\[ \sequence{f_n} \to f,\ f_n \text{ continuous } \nimplies f \text{ continuous} \]
\begin{proof}
Consider $f_n(x) = \frac{x^2}{(1+x^2)^n}$.
	Clearly, $f_n : \mathbb{R} \to \mathbb{R}$ is continuous for every natural number $n$.\\

\textbf{Case 1 : $x = 0$}\\
Suppose $x = 0$.
Then, $f_n(0) = 0$ and therefore $\displaystyle\lim_{n \to \infty} f_n(0) = 0$. \\

\textbf{Case 2 : $x \ne 0$}\\
	Suppose $x \ne 0$.
	Then $\frac{1}{1+x^2} < 1,\ \forall x \in \mathbb{R},\ (x \ne 0)$.\\
Therefore,
\[ f(x) = \lim_{n \to \infty} f_n(x) = x^2 \lim_{n \to \infty} \left(\frac{1}{1+x^2}\right)^n = x^2 \frac{1}{1-\frac{1}{1+x^2}} = x^2 \frac{1+x^2}{x^2} = 1+x^2 \]
	Clearly from cases $1$ and $2$,\\
	\[ f : \mathbb{R} \to \mathbb{R},\ f(x)= \begin{cases} 0 & x = 0 \\ 1+x^2 & x \ne 0 \end{cases} \quad \text{ is not continuous at } 0 \]
\end{proof}
\subsubsection{integrability} 
\[ \sequence{f_n} \to f, f_n \in \mathscr{R} \nimplies f \in \mathscr{R} \]
\begin{proof}
Consider, $\displaystyle f_m(x) = \lim_{n \to \infty} (\cos m!\pi x)^{2n}$.\\

Suppose $m!x$ is not an integer.
Then $\displaystyle f_m(x) = \lim_{n \to \infty} (y^2)^n = 0$ since $-1<y = \cos m! \pi x <1$.
Suppose $m!x$ is an integer.
Then $\cos m! \pi x = \pm 1$ and $\displaystyle f_m(x) = \lim_{n \to \infty} ((\pm 1)^2)^n = 1$.\\

We know that, if $x \in \mathbb{Q}$, then $x = \pm \frac{p}{q}$ where $p,q \in \mathbb{N}$.
Clearly, for any $m > q$, $m! x$ is an integer and $f_m(x) =1 $ for any $m > q$.
Therefore, $\displaystyle \lim_{m \to \infty} f_m(x) = 1$ if $x \in \mathbb{Q}$.
And $\displaystyle \lim_{m \to \infty} f_m(x) = 0$ if $x \notin \mathbb{Q}$ since $m! x$ is not an integer for any $m \in \mathbb{N}$ and $f_m(x) = 0,\ \forall m \in \mathbb{N}$.\\

Now, $f : \mathbb{R} \to \mathbb{R}$ defined by $\displaystyle f(x) = \lim_{m \to \infty} f_m(x)$ is given by,
	\[ f(x) = \begin{cases} 1 & x \in \mathbb{Q} \\ 0 & x \notin \mathbb{Q} \end{cases} \text{ which is \textbf{everywhere discontinuous.}} \]
	Clearly, $f$ is not Riemann integrable since $\mathbb{Q}, \mathbb{R}-\mathbb{Q}$ are dense in any subinterval.
	Thus, $m_i = 0$ and $M_i = 1$ in any subinterval of any partition $P$.
	\[ \lowint_0^1 f(x)\ dx = 0 \ne 1 = \upint_0^1 f(x)\ dx \]

\begin{important}
Let $m \in \mathbb{N}$.
Consider closed interval $E = [-1,1]$.
Then there exists finitely many rational numbers $x$ on $E$ such that $m!x$ is an integer.
Thus, $f_m(x)$ has at most finitely many discontinuities in any bounded interval. 
\end{important}
\begin{commentary}
$f_m(x) = \cos m! \pi x$ is discontinuous at $S = \left\{ \frac{k}{m!} \in E : k \in \mathbb{Z} \right\}$.
For example, suppose $m = 3$.
Then $S = \left\{ 0,\pm\frac{1}{6},\pm\frac{2}{6},\pm\frac{3}{6},\pm\frac{4}{6},\pm\frac{5}{6},\pm 1 \right\}$.
\end{commentary}
Let $\alpha$ be the identity function.
Then $\alpha$ is continuous at those finite points where $f_m$ is discontinuous.
And $f_m$ are bounded functions, since $|f_m| \le 1$.
Therefore, for any bounded interval $E$, $f_m$ is Riemann integrable on $E$ for each $m$.
However, $f$ is not Riemann integrable.
\end{proof}
\subsubsection{Derivative}
\[ \sequence{f_n} \to f \nimplies \sequence{f_n'} \to f' \]
\begin{proof}
	Consider $f_n(x) = \frac{\sin nx}{\sqrt{n}}$.
	\[ f(x) = \lim_{n \to \infty} f_n(x) = 0,\ \forall x \]
	However, $f_n'(x) = \sqrt{n} \cos nx$.
	And $f'(x) = 0$.
	\[ \lim_{n \to \infty} \frac{d}{dx} f_n(x) = \infty \ne f'(x) = \frac{d}{dx} \lim_{n \to \infty} f_n(x),\ \forall x \]
	Clearly, convergence doesn't preserve derivatives.
\end{proof}
\subsubsection{Integral}
\[ \sequence{f_n} \to f \nimplies \sequence{ \int f_n} \to \int f \]
\begin{proof}
	Consider $f_n : [0,1] \to \mathbb{R}$ defined by $f_n(x) = n^2 x (1-x^2)^n$.\\

	We have,
	\[ \lim_{n \to \infty} \frac{n^\alpha}{(1+p)^n} = 0 \]
	Let $\frac{1}{1+p} = 1-x^2$, then $1+p = \frac{1}{1-x^2}$ and $p = \frac{x^2}{1-x^2} > 0$.\\
	Now, we have
	\[ f(x) = \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} n^2 x(1-x^2)^n = x \lim_{n \to \infty} \frac{n^2}{\left(1+\frac{x^2}{1-x^2}\right)^n} = 0 \]
	And, we have
	\[ \int_0^1 f(x)\ dx = 0 \]
	However,
	\[ \int_0^1 f_n(x)\ dx = n^2 \int_0^1 x(1-x^2)^n\ dx = n^2 \int_0^1 \frac{u^n}{2}\ du = n^2 \left[ \frac{u^{n+1}}{2(n+1)} \right]_0^1 = \frac{n^2}{2n+2} \]
	Clearly, 
	\[ \lim_{n \to \infty} \int_0^1 f_n(x)\ dx = \lim_{n \to \infty} \frac{n^2}{2n+2} = \infty \ne 0 = \int_0^1 f(x)\ dx = \int_0^1 \lim_{n \to \infty} f_n(x)\ dx \]
\end{proof}
\subsection{Uniform Convergence}
Uniform convergence is a partial solution to our main problem.
\begin{definition}
	Let sequence $\sequence{f_n}$ be a sequence of functions on $E$.
	Then $\sequence{f_n}$ converges uniformly to limit function $f$ if for any $\varepsilon > 0$, there exists a natural number $N$ such that for any $n > N$ and $x \in E$, $|f_n(x) - f(x)| \le \varepsilon$.
\end{definition}
\begin{important}
	\[ \forall \varepsilon > 0,\ \exists N \in \mathbb{N},\ \forall n > N,\ \forall x \in E,\ |f_n(x) - f(x)| \le \varepsilon \]
\end{important}
\begin{commentary}
	The difference between pointwise convergence and uniform convergence is that the choice of natrual number $N$ is independent of the choice of $x$ in the case of uniform convergence.
\end{commentary}
\subsubsection{Criterion for Uniform Convergence}
\begin{theorem}[Cauchy criterion]
	The sequence of functions $\sequence{f_n}$ defined on $E$ converges uniformly on $E$ if and only if for any $\varepsilon > 0$, there exists a natural number $N$ such that for any $m,n \ge N$ and $x \in E$, $|f_n(x)-f_m(x)| \le \varepsilon$.
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Let $\varepsilon > 0$.
	Then there exists $N \in \mathbb{N}$ such that $\forall n,m > N$ and $\forall x \in E$, $|f_n(x) - f(x)| \le \frac{\varepsilon}{2}$ and $|f_m(x)-f(x)| \le \frac{\varepsilon}{2}$.
	Therefore, $\forall \varepsilon > 0$ we have, $N \in \mathbb{N}$ such that $\forall n,m > N$ and $\forall x \in E$,
	\[ |f_n(x) - f_m(x)| \le |f_n(x)-f(x)| + |f_m(x)-f(x)| \le \varepsilon \]

	Let $\varepsilon > 0$.
	Suppose there exists $N \in \mathbb{N}$ such that $\forall n,m > N$ and $\forall x \in E$,
	\[ |f_n(x) - f_m(x)| \le |f_n(x)-f(x)| + |f_m(x)-f(x)| \le \varepsilon \]
	By Cauchy criterion, $\sequence{f_n} \to f$ pointwise on $E$.
	It remains to prove that this convergence is uniform on $E$.
	Clearly, above inequality is true for any value of $m$ greater than $N$.
	As $m \to \infty$ we have,
	\begin{align*}
		\lim_{m \to \infty} |f_n(x) - f_m(x)|  \le \varepsilon \\
		\implies \left|f_n(x) - \lim_{m \to \infty} f_m(x)\right| = |f_n(x) - f(x) | & \le \varepsilon
	\end{align*}
	Clearly, the convergence is uniform.
	That is, $\sequence{f_n} \to f$ uniformly on $E$.
\end{proof}

\begin{theorem}[Supremum Test]
	Suppose sequence of function $\sequence{f_n} \to f$ pointwise on $E$.
	Suppose $\displaystyle M_n = \sup_{x \in E} \left|f_n(x) - f(x)\right|$.
	Then $\sequence{f_n} \to f$ uniformly on $E$, if and only if sequence $\sequence{M_n} \to 0$.
\end{theorem}
\begin{proof}
	Define $\displaystyle M_n = \sup_{x \in X} |f_n(x) - f(x)|$.
	Clearly, $M_n \ge 0$.
	Suppose $\sequence{M_n} \to 0$ on $E$.
	Let $\varepsilon > 0$.
	Then there exists $N \in \mathbb{N}$ such that for every $n > N$,  we have $|M_n - 0| = M_n \le \varepsilon$.
	Thus for any $n \ge N$ and $x \in E$,
	\[ |f_n(x) - f(x)| \le \sup |f_n(x) - f(x)| = M_n \le \varepsilon \]
	Therefore, $\sequence{f_n} \to f$ uniformly on $E$.\\

	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Let $\varepsilon > 0$.
	Then, there exists $N \in \mathbb{N}$ such that for every $n \ge N$ and $x \in E$, $|f_n(x) -f(x)| \le \varepsilon$.
	This inequality is true for every $x \in E$.
	Thus,
	\[ \sup_{x \in E} \left| f_n(x) - f(x) \right| = M_n \le \varepsilon,\quad \forall n \ge N \]
	Therefore, $\sequence{M_n} \to 0$ on $E$.
\end{proof}

\begin{theorem}[Weierstrass M-test]
	Suppose $\sequence{f_n}$ is a sequence of functions on $E$.
	Suppose $|f_n(x)| \le M_n$.
	Then $\sum f_n$ converges on $E$ if $\sum M_n$ converges.
\end{theorem}
\begin{proof}
	Suppose $|f_n| \le M_n$.
	Then $M_n \ge 0$.
	Suppose $\sum M_n$ converges.
	Let $\sequence{s_n}$ be the sequence of partial sums.
	Let $\varepsilon > 0$.
	By Cauchy criterion, there exists $N \in \mathbb{N}$ such that $\forall n,m \ge N$, $|s_n - s_m| \le \varepsilon$.
	Thus,
	\[ |s_n - s_m| = \left| \sum_{j = 1}^n M_j - \sum_{j = 1}^m M_j \right| =  \sum_{j = n+1}^m M_j \le \varepsilon,\quad ( m > n )\]
	Let $\sequence{S_n}$ be the sequence of partial sums for $\sum f_n$.
	Clearly, $\forall n,m \ge N$ and $\forall x \in E$, we have $|S_n-S_m| \le \varepsilon$ since
	\[ |S_n - S_m| = \left|\sum_{j = 1}^n f_j(x) - \sum_{j = 1}^m f_j(x) \right| = \left| \sum_{j = n+1}^m f_j(x) \right| \le \sum_{j = n+1}^m M_j \le \varepsilon \]
	By Cauchy criterion for uniform convergence, the sequence of partial sums, $\sequence{S_n}$ converges uniformly on $E$.
	Therefore, the series of functions, $\sum f_n$ converges uniformly on $E$.
\end{proof}

\subsection{Uniform Convergence and Continuity}
\begin{theorem} %7.11
	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Let $x$ be a limit point of $E$.
	Suppose $\displaystyle \lim_{t \to x} f_n(t) = A_n$.
	Then $\sequence{A_n}$ converges, and 
	$\displaystyle \lim_{t \to x} f(t) = \lim_{n \to \infty} A_n$.
	In other words,
	\[ \lim_{t \to x} \lim_{n \to \infty} f_n(t) = \lim_{n \to \infty} \lim_{t \to x} f_n(t) \]
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n} \to f$ uniformly on $E$.
	Suppose $\displaystyle \lim_{t \to x} f_n(t)  = A_n$ on $E$.
	Let $\varepsilon > 0$.
	By Cauchy criterion for uniform convergence, there exists $N \in \mathbb{N}$ such that for every $n \ge N$ and every $t \in E$, $|f_n(t)-f_m(t)| \le \varepsilon/3$.
	As $t \to x$, we have $|A_n - A_m| \le \varepsilon/3$.
	Clearly, $\sequence{A_n}$ is Cauchy and $\sequence{A_n} \to A$.\\

	Now fix a natural number $N$ such that for every $n \ge N$ and $t \in E$, $|f_n(t) - f(t) | < \varepsilon /3$ and $|A_n-A| \le \varepsilon/3$.
	Also, choose a neighbourhood $V$ of $x$ such that $|f_n(t) - A_n | \le \varepsilon/3$.
	Then, $\forall x \in V \cap E$ we have
	\[ |f(t) - A| \le |f(t) -f_n(t)| + |f_n(t)-A_n| + |A_n - A| \le \varepsilon \]
	Therefore, $\displaystyle \lim_{t \to x} f(t) = A$.
	\[ \lim_{t \to x} \lim_{n \to \infty} f_n(t) = \lim_{t \to x} f(t)  = \lim_{n \to \infty} A_n = \lim_{n \to \infty} \lim_{t \to x} f_n(t) \]
\end{proof}

\begin{theorem} %7.12
	If $\sequence{f_n}$ is a sequence of continuous functions on $E$, and if $f_n \to f$ uniformly on $E$, then $f$ is continuous on $E$.
\end{theorem}
\begin{important}
	Uniform convergence preserves continuity.
\end{important}
\begin{proof}
	Suppose the sequence of continuous functions, $\sequence{f_n} \to f$ uniformly on $E$.
	Since $f_n$ is continuous $\lim_{t \to x} f_n(t) = f_n(x),\ \forall x \in E$.
	Since the convergence is uniform, the order of limits can be interchanged.
	\[ f(x) = \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \lim_{t \to x} f_n(t) = \lim_{t \to x} \lim_{n \to \infty} f_n(t) = \lim_{t \to x} f(t) \]
	Therefore, the limit function $f$ is continuous.
\end{proof}
\begin{remark}
	Suppose a sequence of continuous functions $\sequence{f_n}$ converges to a function $f$.
	Function $f$ being continuous doesn't imply that the convergence is uniform.
\end{remark}
\begin{proof}
	Consider $f_n : (0,1) \to \mathbb{R}$ defined by $f_n(x) = \frac{1}{nx+1}$.
	Then, $\sequence{f_n} \to 0$.
	Clearly, $f_n,0$ are continuous functions on $(0,1)$.
	However, the convergence is not uniform.
\end{proof}

\begin{theorem} %7.13
	Suppose $K$ is compact, and
	\begin{enumerate}
		\item $\sequence{f_n}$ is sequence of continuous functions on $K$
		\item $\sequence{f_n}$ converges pointwise to a continuous function $f$ on $K$.
		\item $f_n(x) \ge f_{n+1}(x),\ \forall x \in K$
	\end{enumerate}
	Then, $\sequence{f_n} \to f$ uniformly on $K$.
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n} \to f$ pointwise on $K$.
	Let function $g_n = f_n - f$.
	Then, $g_n$ is continuous, $\sequence{g_n} \to 0$ pointwise and $g_n \ge g_{n+1}$.
  	If sequence $\sequence{g_n} \to 0$ uniformly, then sequence $\sequence{f_n} \to f$ uniformly.\\

	Let $\varepsilon > 0$.
	Let $K_n$ be the set of all points $x \in K$ such that $g_n(x) \ge \varepsilon$.
	Then $K_n$ is closed, since $g_n$ is continuous.
	And $K_n$ is compact since $K_n$ is a closed subset of a compact set $K$.\\

	Suppose $x \in K_{n+1}$.
	Then $g_{n+1}(x) \ge \varepsilon$.
	We have, $g(x) \ge g_{n+1}(x)$.
	Thus, $g(x) \ge \varepsilon$.
	Therefore, $K_n \supset K_{n+1} \supset \dots$.\\

	Let $x \in K$.
	We have $\sequence{g_n(x)} \to 0$ pointwise.
	Then there exists $N \in \mathbb{N}$ such that $\forall n > N$, $g_n(x) < \varepsilon$.
	Thus, $x \notin K_n,\ \forall n \ge N$.
	Therefore $\cap K_n$ is empty.\\

	Clearly, $K_N$ is empty for some $N \in \mathbb{N}$.
	Thus, $\forall n \ge N$ and $\forall x \in K$ we have, $0 \le g_n(x) < \varepsilon$.
	In other words, $\sequence{g_n}$ converges to $0$ uniformly on $K$.
	Therefore, $\sequence{f_n}$ converges to $f$ uniformly on $K$.
\end{proof}

\begin{definition} %7.14
	Let $X$ be a metric space.
	Let $\mathscr{C}(X)$ be the set of all complex valued, continuous, bounded functions on $X$.
	Let $f \in \mathscr{C}(X)$.
	Then \textbf{supremum norm} on $\mathscr{C}(X)$ is defined by
	\[ \| f \| = \sup_{x \in X} |f(x)| \]
	And, $\mathscr{C}(X)$ together with \textbf{distance function} $d : \mathscr{C}(X) \times \mathscr{C}(X) \to \mathbb{R}$ defined by $d(f,g) = \| f-g \|$ is a metric space.
\end{definition}

\begin{commentary}
	Let $\mathscr{C}(X)$ be the set of all complex valued, continuous, bounded functions on metric space $X$.
	Let $f \in \mathscr{C}(X)$.
	Then $f$ is bounded, $|f| < \infty$.
	Thus, $\sup |f(x)|$ exists.
	Therefore, $\| f \|$ is well-defined.\\

	And, $\| f \| = \sup |f(x)| \ge 0$ since $|f(x)| \ge 0$.
	Let $f,g \in \mathscr{C}(X)$.
	Then $h = f+g \in \mathscr{C}(X)$ since sum of continuous functions is functions and sum of bounded functions is bounded.
	Also, we have
	\[ \| h \| = \sup_{x \in X} |(f+g)(x)| \le \sup_{x \in X} |f(x)| + \sup_{x \in X} |g(x)| = \| f \| + \| g \| \]
	Therefore, $\| . \| : \mathscr{C}(X) \to \mathbb{R}$ is a norm on $\mathscr{C}(X)$.\\

	Consider the function $d : \mathscr{C}(X) \times \mathscr{C}(X) \to \mathbb{R}$ defined by $d(f,g) = \| f-g \|$.
	Clearly, $d(f,g) \ge 0$ since $|(f-g)(x)| \ge 0$.
	And,
	\[ d(f,g) = 0 \iff |(f-g)(x)| = 0, \forall x \in X \iff f =g  \]
	Also we have,
	\[ d(f,g) \le \| (f-h)+(h-g) \| \le \| f-h \| + \| h-g \| =  d(f,h) + d(h,g) \]
	Therefore, the function $d$ is a distance function/metric on $\mathscr{C}(X)$.
\end{commentary}
\begin{remark}
\begin{important}
	A sequence $\sequence{f_n}$ converges to $f$ in metric space $\mathscr{C}(X)$ if and only if $\sequence{f_n}$ converges to $f$ uniformly on $X$.
\end{important}
\end{remark}
\begin{proof}
	\begin{align*}
		\sequence{f_n} \to f \text{ in } \mathscr{C}(X) & \iff \forall \varepsilon > 0,\ \exists N \in \mathbb{N},\ \forall n > N, d(f_n,f) \le \varepsilon\\
		& \iff \forall n > N, \| f_n - f \| \le \varepsilon \\
		& \iff \forall n > N, \sup |f_n(x)-f(x)| \le \varepsilon \\
		& \iff \forall n > N, \forall x \in X,\ |f_n(x)-f(x)| \le \varepsilon \\
		& \iff \sequence{f_n} \to f \text{ uniformly on } X
	\end{align*}
\end{proof}

\begin{definition}[uniformly closed]
	Closed subset of $\mathscr{C}(X)$ is \textbf{uniformly closed} since every convergent sequence in it corresponds to a uniform convergent sequence in $X$.
\end{definition}

\begin{definition}[uniform closure]
	Let $\mathscr{A} \subset \mathscr{C}(X)$.
	Then its closure, $\bar{\mathscr{A}}$ is the \textbf{uniform closure} of $\mathscr{A}$.
\end{definition}

\begin{definition}[complete metric space]
	A metric space is \textbf{complete} if every Cauchy sequence in it converges.
\end{definition}

For example, $\mathbb{R}$ is complete.
But $\mathbb{Q}$ is not complete since a sequence of rational numbers converging to $\pi$ in $\mathbb{R}$ is Cauchy sequence in $\mathbb{Q}$ which doesn't converge to any point in $\mathbb{Q}$.

\begin{theorem} %7.15
	Let $X$ be a metric space.
	Let $\mathscr{C}(X)$ be the set of all complex valued, continuous, bounded functions on $X$.
	Let $f,g \in \mathscr{C}(X)$.
	Define norm $\| f \| = \sup_{x \in X} |f(x)|$ and metric $d(f,g) = \| f-g \|$.
	Then metric space $\entity{\mathscr{C}(X),d}$ is a complete metric space.
\end{theorem}
\begin{proof}
	Let sequence of functions $\sequence{f_n}$ be a Cauchy sequence in $\mathscr{C}(X)$.
	Let $\varepsilon > 0$.
	Then there exists a natural number $N$ such that for any $n,m > N$, $d(f_n,f_m) < \varepsilon$.
	That is, 
	\[ d(f_n,f_m) = \sup_{x \in X} |f_n(x) - f_m(x)| < \varepsilon \]
	%Thus, the  inequality is true for any $x \in X$.
	%\[ \forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n,m > N, \forall x \in X, |f_n(x)-f_m(x)| <\varepsilon \]
	%Thus, $\sequence{f_n} \to f$ uniformly on $X$.
	Define $M_n = \sup_{x \in X} |f_n(x)|$.
	Then,
	\[ |M_n - M_m| = \left| \sup_{x \in X} |f_n(x)| - \sup_{x \in X}|f_m(x)| \right| \le \sup_{x \in X} |f_n(x) - f_m(x) | < \varepsilon \]
	We have, $\sequence{M_n}$ is a Cauchy sequence in $\mathbb{R}$.
	We know that, every Cauchy sequence in $\mathbb{R}$ is convergent in $\mathbb{R}$ since $\mathbb{R}$ is complete.
	Thus, $\sequence{M_n}$ converges.\\

	We have, corresponding  sequence $\sequence{f_n}$ in $X$ such that sequence $\sequence{M_n}$ defined by $M_n = \sup_{x \in X} |f_n(x)|$ converges.
	Therefore, $\sequence{f_n}$ converges to $f$ uniformly on $X$ by criterion of uniform convergence.\\

	We have, $f_n$ are continuous, bounded functions on $X$.
	And $\sequence{f_n} \to f$ uniformly on $X$.
	\[ \forall \varepsilon > 0, \exists N \in \mathbb{N}, \forall n > N, \forall x \in X, |f_n(x) - f_m(x)| < \varepsilon \]
	Since, the convergence is uniform, the limit function $f$ is continuous and \begin{important}bounded\end{important}.
	Therefore, $\sequence{f_n}$ in $\mathscr{C}(X)$ converges to $f \in \mathscr{C}(X)$.\\

	The Cauchy sequence $\sequence{f_n}$ is chosen arbitrarily.
	Thus, every Cauchy sequence in $\mathscr{C}(X)$ is convergent.
	Therefore, $\mathscr{C}(X)$ is complete.
\end{proof}

\hrule
\begin{remark}\cite[Exercise 7.1]{rudin}
	Let $\sequence{f_n}$ be a sequence of bounded functions on a metric space $X$.
	Suppose $\sequence{f_n} \to f$ uniformly on $X$.
	Then $f$ is bounded.
\end{remark}
\begin{proof}
	Suppose the sequence of bounded functions, $\sequence{f_n}$ converges to $f$ uniformly on $X$.
	Consider $\sequence{g_n}$ where $g_n = |f_n - f|$
	Then, $\sequence{g_n}$ converges to $0$ uniformly.
	Suppose $f$ is unbounded.
	Then $g_n$ is an unbounded sequence which doesn't converge.
	This is a contradiction.
	Therefore, $f$ is bounded.
\end{proof}
\hrule

\subsection{Uniform Convergence and Integration}
\begin{theorem} %7.16
	Let $\alpha$ be monotonically increasing on $[a,b]$.
	Suppose $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Suppose $\sequence{f_n} \to f$ uniformly on $[a,b]$.
	Then $f \in \mathscr{R}(\alpha)$ on $[a,b]$.
	And,
	\[ \int_a^b f\ d\alpha = \lim_{n \to \infty} \int_a^b f_n \ d\alpha \]
\end{theorem}
\begin{proof}
	Let $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$.
	Suppose sequence $\sequence{f_n} \to f$ uniformly on $[a,b]$.
	Define
	\[ \varepsilon_n = \sup_{x \in [a,b]} |f_n(x) - f(x)| \]
	Then, $ -\varepsilon_n \le f - f_n \le \varepsilon_n,\quad \forall x \in [a,b] $. 
	And,
	\[ f_n - \varepsilon_n \le f \le f_n + \varepsilon_n \]
	Since $f_n \in \mathscr{R}(\alpha)$, we have $f_n+\varepsilon, f_n - \varepsilon \in \mathscr{R}(\alpha)$.
	From the definition of lower and upper integrals,
	\[ \int_a^b (f_n - \varepsilon_n)\ d\alpha \le \lowint_a^b f\ d\alpha \le \upint_a^b f\ d\alpha \le \int_a^b (f_n + \varepsilon_n)\ d\alpha  \]
	Thus,
	\[ 0 \le \upint_a^b f\ d\alpha - \lowint_a^b f\ d\alpha \le \int_a^b 2\varepsilon_n\ d\alpha = 2\varepsilon_n \left[ \alpha(b)-\alpha(a) \right] \]
	Clearly, $\varepsilon_n \to 0$ as $n \to \infty$.
	Then, $f \in \mathscr{R}(\alpha)$ on $[a,b]$ since the lower and upper integrals of $f$ with respect to $\alpha$ are equal.\\
	
	Also we have, $ 0 \le |f-f_n| \le \varepsilon_n $ for every $x \in [a,b]$.\\
	Therefore,
	\[ 0 \le \left| \int_a^b f\ d\alpha - \int_a^b f_n\ d\alpha \right| \le \int_a^b |f-f_n|\ d\alpha \le \int_a^b \varepsilon_n\ d\alpha = \varepsilon_n \left[ \alpha(b) - \alpha(a) \right] \]
	As $n \to \infty$, we have $\varepsilon_n \to 0$.
	Thus,
	\[ \int_a^b f\ d\alpha = \lim_{n \to \infty} \int_a^b f_n\ d\ \alpha \]
\end{proof} 

\begin{corollary}
	If $f_n \in \mathscr{R}(\alpha)$ on $[a,b]$ and if
	\[ f(x) = \sum_{n = 1}^\infty f_n(x) \]
	the series converging uniformly on $[a,b]$, then
	\[ \int_a^b f\ d\alpha = \sum_{n=1}^\infty \int_a^b f_n \ d\alpha \]
	In other words, the series may be integrated term by term.
\end{corollary}
\begin{proof}
	Suppose $f_n \in \mathscr{R}(\alpha)$ and $\sum f_n$ converges to $f$ uniformly on $[a,b]$.
	Let $\sequence{S_n}$ be the sequence of partial sums converging to $f$ uniformly on $[a,b]$.
	We have, $S_n \in \mathscr{R}(\alpha)$ on $[a,b]$ by linearity of the integral and mathematical induction.
	\[ \sum_{j = 1}^n \int_a^b f_n\ d\alpha = \int_a^b \sum_{j = 1}^n f_j\ d\alpha = \int_a^b S_n\ d\alpha \]
	Since the uniform limit function of integrable functions is integrable,
	\[ \sum_{n = 1}^\infty f_n = \lim_{n \to \infty} S_n  = f \in \mathscr{R}(\alpha) \text{ and } \lim_{n \to \infty} \int_a^b S_n\ d\alpha = \int_a^b f\ d\alpha \]
	By the definition of sum of series,
	\[ \sum_{n = 1}^\infty \int_a^b f_n\ d\alpha = \lim_{n \to \infty} \sum_{j = 1}^n \int_a^b f_j\ d\alpha = \lim_{n \to \infty} \int_a^b S_n\ d\alpha = \int_a^b f\ d\alpha = \int_a^b \sum_{n = 1}^\infty f_n\ d\alpha \]
	Thus, the series may be integrated term by term.
\end{proof}

\subsection{Uniform Convergence and Differentiation}
\begin{theorem} %7.17
	Suppose $\sequence{f_n}$ is a sequence of functions, differentiable on $[a,b]$ and $\sequence{f_n(x_0)}$ converges for some $x_0 \in [a,b]$.
	If $\sequence{f_n'}$ converges uniformly on $[a,b]$, then $\sequence{f_n}$ converges uniformly on $[a,b]$ to a function $f$.
	And,
	\[ f'(x) = \lim_{n \to \infty} f_n'(x) \]
\end{theorem}
\begin{proof}
	Suppose $\sequence{f_n(x_0)} \to f(x_0)$.
	Suppose $\sequence{f_n'}$ converges uniformly on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists a natural number $N$ such that
	\[ \forall n,m > N,\ |f_n(x_0) - f_m(x_0)| < \frac{\varepsilon}{2} \text{ and}\] 
	\[ \forall n,m > N,\ |f_n'(t) - f_m'(t)| < \frac{\varepsilon}{2(b-a)} \]
	Let $x,t \in [a,b]$.
	\begin{align*}
		|f_n(x)-f_m(x)-f_n(t)+f_m(t)| & = |(f_n-f_m)(x) - (f_n-f_m)(t)| 
		\intertext{By mean value theorem, there exists $y \in (x,t)$ such that}
		|f_n(x)-f_m(x)-f_n(t)+f_m(t)| & = |(x-t)\ (f_n-f_m)'(y)|\\
		& \le \frac{|x-t|\varepsilon}{2(b-a)} \le \frac{\varepsilon}{2}
	\end{align*}
	since $|(f_n-f_m)'(x)| = |f_n'(x) - f_m'(x)| \le \frac{\varepsilon}{2(b-a)}$. \\

	And the inequality is true of $t = x_0$.
	Thus,
	\begin{align*}
		|f_n(x)-f_m(x)| &
		= |f_n(x)-f_m(x) -f_n(x_0)+f_m(x_0)+f_n(x_0)-f_m(x_0| \\
		& \le |f_n(x)-f_m(x) -f_n(x_0)+f_m(x_0)| + |f_n(x_0)-f_m(x_0| \le \varepsilon
	\end{align*}
	Thus, $\forall \varepsilon > 0$, we have a natural number $N$ such that $\forall n,m > N$, $\forall x \in [a,b]$, $|f_n(x)-f_m(x)| \le \varepsilon$.
	In other words, $\sequence{f_n} \to f$ uniformly on $[a,b]$.\\

	Fix $x \in [a,b]$.
	Define functions $\phi_n,\phi$ on $[a,b]$ except at $t = x$,
	\[ \phi_n(t) = \frac{f_n(t)-f_n(x)}{t-x} \quad \text{ and } \quad \phi(t) = \frac{f(t)-f(x)}{t-x}\]
	Then,
	\[ \lim_{n \to \infty} \phi_n(t) = \lim_{n \to \infty} \frac{f_n(t) - f_n(x)}{t-x} = \frac{\displaystyle \lim_{n \to \infty} f_n(t) - \displaystyle \lim_{n \to \infty} f_n(x)}{t-x} = \frac{f(t) - f(x)}{t-x} = \phi(t)\] 
	since sequence $\sequence{f_n} \to f$ uniformly on $[a,b]$.
	Clearly, $\sequence{\phi_n}$ converges to $\phi$ pointwise on $[a,b] - \{ x \}$.\\

	We have,
	\[ \lim_{t \to x}\phi_n(t) = \lim_{t \to x} \frac{f_n(t) - f_n(x)}{t-x} = f_n'(x) \]
	\[ \lim_{t \to x} \phi(t) = \lim_{t \to x} \frac{f(t)-f(x)}{t-x} = f'(x) \]
	Also we have,
	\[ |\phi_n(t)-\phi_m(t)| = \frac{|f_n(t)-f_n(x)-f_m(t) + f_m(x)|}{t-x} \le \frac{\varepsilon}{2(b-a)} \]
	That is $\forall n,m > N$, $|\phi_n(t)-\phi_m(t)| < \varepsilon$.
	Now, by Cauchy criterion for uniform convergence, sequence $\sequence{\phi_n}$ converges to $\phi$ uniformly on $[a,b]-\{x\}$.\\

	We know that continuity is preserved under uniform convergence.\\
	Therefore,
	\[ f'(t) = \lim_{t \to x} \phi(t) = \lim_{t \to x} \lim_{n \to \infty} \phi_n(t) = \lim_{n \to \infty} \lim_{t \to x} \phi_n(t) = \lim_{n \to \infty} f_n'(x) \]

\end{proof}

\begin{theorem} %7.18
	There exists a real continuous function on the real line which is nowhere differentiable.
\end{theorem}
\begin{proof}
	Define $\phi : [-1,1] \to \mathbb{R}$ by $\phi(x) = |x|$.
	Extend $\phi$ from $[-1,1]$ to $\mathbb{R}$ such that $\phi(x+2) = \phi(x)$.
	Then $|\phi(s)-\phi(t)| \le |s-t|,\ \forall s,t \in \mathbb{R}$.\\

\begin{figure}[h]
	\begin{tikzpicture}
		\draw[-latex] (-7,0) -- (7,0);
		\draw[-latex] (0,0) -- (0,2);
		\draw (-6,0) -- (-5,1) -- (-4,0) -- (-3,1) -- (-2,0) -- (-1,1) -- (0,0) -- (1,1) -- (2,0) -- (3,1) -- (4,0) -- (5,1) -- (6,0);
		\draw (-6,-0.3) node{$-6$};
		\draw (-5,-0.3) node{$-5$};
		\draw (-4,-0.3) node{$-4$};
		\draw (-3,-0.3) node{$-3$};
		\draw (-2,-0.3) node{$-2$};
		\draw (-1,-0.3) node{$-1$};
		\draw (0,-0.3) node{$0$};
		\draw (1,-0.3) node{$1$};
		\draw (2,-0.3) node{$2$};
		\draw (3,-0.3) node{$3$};
		\draw (4,-0.3) node{$4$};
		\draw (5,-0.3) node{$5$};
		\draw (6,-0.3) node{$6$};
	\end{tikzpicture}
	\caption{Graph of $\phi$}
\end{figure}

	Define $f_n(x) = \left(\frac{3}{4}\right)^n \phi(4^n x)$.
	Then $|f_n| \le \left(\frac{3}{4}\right)^n$.
	By Weierstrass M test, $\displaystyle \sum_{n = 0}^\infty f_n$ converges uniformly on $\mathbb{R}$.
	Consider the function $f : \mathbb{R} \to \mathbb{R}$ defined by $\displaystyle f(x) = \sum_{n = 0}^\infty \left(\frac{3}{4}\right)^n \phi(4^n x)$.
	This sum function $f$ is continuous since the convergence is uniform and $f_n$ are all continuous.\\

	Fix a real number $x$ and a positive integer $m$.
	Define $\displaystyle \delta_m = \pm \frac{1}{2}\ 4^{-m}$ such that no integer lies between $4^m x$ and $4^m(x+\delta_m)$.
	This is possible since $|4^m (x+\delta_m) - 4^m x| = |4^m \delta_m| = \frac{1}{2}$.\\

	\begin{commentary}
		The choice of sign of $\delta_m$ is made depending on the value of $4^m x$.
		Suppose $x = 1.1$ and $m = 3$.
		Then $4^m x = 70.4$.
		Now $\delta = \frac{1}{2}\ 4^{-m} = \frac{1}{128}$ so that $4^m(x+\delta_m)= 70.9$.
		Suppose $x = 1.2$ and $m = 3$.
		Then $4^m x = 76.8$.
		Now $\delta = -\frac{1}{2}\ 4^{-m} = -\frac{1}{128}$ so that $4^m(x+\delta) = 76.3$.\\
	\end{commentary}

	Define
	\[\gamma_n = \frac{\phi(4^n(x+\delta_m)) - \phi(4^n x)}{\delta_m} \]

	If $n > m$, then $4^n \delta_m$ is an even integer and $\gamma_n = 0$.\\
	If $n \le m$, then 
	\[ |\gamma_n| = \left|\frac{\phi(4^n(x+\delta_m)) - \phi(4^n x)}{\delta_m} \right| \le \frac{4^n \delta_m }{\delta_m} \le 4^n \]
	since $|\phi(s)-\phi(t)| \le |s-t|$.\\
	In particular, if $n = m$, then
	\[ |\gamma_m| = \left| \frac{\phi(4^m(x+\delta_m))-\phi(4^m x)}{\delta_m} \right| = 4^m \]
	since $|\phi(4^m (x+\delta_m)) - \phi(4^m x)| = |4^m \delta| = \frac{1}{2}$ as there are no integers between $4^m(x+\delta_m)$ and $4^m x$.\\

	From the definition of $\gamma_n$ we have,
	\begin{align*}
	 \left| \frac{f(x+\delta_m) - f(x)}{\delta_m} \right| 
		& = \left| \sum_{n = 0}^m \left( \frac{3}{4} \right)^n \gamma_n \right| \\
		& = \left| 3^m + \sum_{n = 0}^{m-1} \left( \frac{3}{4} \right)^n \gamma_n \right| \\
		& \ge 3^m - \sum_{n = 0}^{m-1} 3^n = \frac{3^m+1}{2}
	\end{align*}
	Therefore, the function $f$ is not differentiable at $x$ since the following limit does not exist as $m \to \infty$.
	\[ \lim_{\delta_m \to 0} \left| \frac{f(x+\delta_m) - f(x)}{\delta_m} \right| \ge \lim_{m \to \infty} \frac{3^m+1}{2} \]
	Since the choice of $x$ is arbitrary, the function $f$ is nowhere differentiable.
\end{proof}

\pagebreak
{\Large Module 4 : Weierstrass Approximation \& Some Special Functions}
\section{Equicontinuous Family of Functions}
\begin{definition}
	Let $\sequence{f_n}$ be a sequence of functions defined on $E$.\\
	{\color{red}
	Sequence $\sequence{f_n}$ is a \textbf{bounded} sequence if every functions in the sequence is bounded.
	\[ \forall n \in N, \exists M \in \mathbb{R} \text{ such that } \forall x \in E, |f_n(x)|<M \]

	Sequence $\sequence{f_n}$ is a \textbf{pointwise bounded} sequence if there exists a function $\phi(x)$ such that $|f_n(x)| < \phi(x)$.
	In other words, sequence $\sequence{f_n}$ is a pointwise bounded sequence if $\sequence{f_n(x)}$ is bounded for every $x \in E$
	\[ \forall x \in E, \exists M \in \mathbb{R} \text{ such that } \forall n \in \mathbb{N}, |f_n(x)| < M \]
	}

	Sequence $\sequence{f_n}$ is a \textbf{uniformly bounded} sequence if there exists a real number $M$ such that $|f_n(x)| < M$ for every $x \in E$ and $n \in \mathbb{N}$.
	\[ \exists M \in \mathbb{R} \text{ such that } \forall x \in E, \forall n \in \mathbb{N}, |f_n(x)| < M \]
\end{definition}
\subsection{Two Problems}
\begin{enumerate}
	\item Whether uniform bounded sequence of uniformly bounded functions have a convergent subsequence ?
		\textbf{NO}.\\
		Sequence $\sequence{f_n}$ where $f_n(x) = \sin nx$ is uniformly bounded.
		But it doesn't have a convergent subseqeunce.
		\begin{proof}
			Suppose $\sequence{\sin nx}$ has a convergent subsequence, say $\sequence{\sin n_kx}$.
			Then by Cauchy criterion, $(\sin n_k x - \sin n_{k+1}x) \to 0$ as $n_k \to \infty$.
			\[ \lim_{n \to \infty} \sin n_k x - \sin n_{k+1}x = 0 \]
			\[ \lim_{n \to \infty} (\sin n_k x - \sin n_{k+1}x)^2 = 0 \]
			By Lebesgue dominated convergence theorem,
			\[ \lim_{n \to \infty} \int_0^{2\pi} (\sin n_kx - \sin n_{k+1}x)^2 dx = \int_0^{2\pi} \lim_{n \to \infty} (\sin n_kx - \sin n_{k+1}x)^2 dx = 0 \]
			However, the integral on the left evaluates $2\pi$ which is a contradiction.
			Clearly, the uniformly bounded sequence $\sequence{\sin nx}$ of continuous functions on compact interval $[0,2\pi]$ does not even imply existence of a subsequence which converges pointwise.
		\end{proof}
	\item Whether every uniformly bounded, convergent sequence has a  uniformly convergent subsequence ?
		\textbf{NO}.\\
		Consider,
		\[ f_n(x) = \frac{x^2}{x^2+(1-nx)^2} \]
		Sequence $\sequence{f_n}$ is a uniformly bounded sequence of functions on compact interval $[0,1]$.
		doesn't have a convergent subsequence.
		\begin{proof}
			Suppose $\sequence{f_n}$ has a convergent subsequence $\sequence{f_{n_k}} \to f$.
			Then $\displaystyle \lim_{n \to \infty} f_{n_k}(x) = f(x)$.
			We have,
			\[ \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \frac{x^2}{x^2+(1-nx)^2} = 0 \]
			However,
			\[ \lim_{n \to \infty} f_n \left(\frac{1}{n}\right) = \lim_{n \to \infty} \frac{\left(\frac{1}{n}\right)^2}{\left(\frac{1}{n}\right)^2+0} = 1 \ne 0 = \left(\lim_{n \to \infty} f_n\right)(x) \] 
			Clearly, the sequence of functions $\{ f_n\}$ is a uniformly bounded sequence of continuous functions on a compact interval $[0,1]$.
			Therefore, uniformly bounded, convergent sequence on compact set doesn't necessarily have a uniformly convergenct subsequence.
		\end{proof}
\end{enumerate}

\begin{definition}
	Let $E$ be a subset of a metric space $X$.
	A family $\mathscr{F}$ of complex functions on $E$ is \textbf{equicontinuous} if for any $\varepsilon > 0$, there exists $\delta > 0$ such that $|f(x)-f(y)| < \varepsilon$ whenever $d(x,y) < \delta$ for any $x,y \in E$ and $f \in \mathscr{F}$.
	\[ \forall \varepsilon > 0, \exists \delta > 0 \text{ such that } \forall f \in \mathscr{F}, |f(x)-f(y)| < \varepsilon \text{ whenever } d(x,y) < \delta \]
	\begin{commentary}
		In equicontinuity, the choice of $\delta$ is independent of the choice of $f$.
	\end{commentary}
\end{definition}

\begin{theorem}
	If $\sequence{f_n}$ is a sequence of pointwise bounded, complex functions on countable set $E$, then it has a subsequence $\sequence{f_{n_k}}$ such that $\sequence{f_{n_k}(x)}$ converges for every $x \in E$.
\end{theorem}
\begin{proof}
	Let $E$ be a countable set.
	Let $\sequence{f_n}$ be a sequence of pointwise bounded, complex functions.
	Let $\sequence{x_i}$ be a sequence in $E$.
	Then $\sequence{f_n(x_1)}$ is a bounded sequence of complex numbers.
	By Bolzano-Weierstrass theorem,  $\sequence{f_n(x_1)}$ has a subsequence $S_1$, say $\sequence{f_{1,k}(x_1)}$ which converges at $x_1 \in E$.
	\[ S_1 : f_{1,1}\ f_{1,2}\ f_{1,3} \dots \]

	Consider, the subsequence $\sequence{f_{2,k}(x_2)}$ of the sequence $\sequence{f_n(x_2)}$ with the same values for $k$ as in subsequence $\sequence{f_{1,k}(x_2)}$.
	Again, $\sequence{f_{2,k}(x_2)}$ is a bounded complex sequence.
	And by Bolzano-Weierstrass theorem, $\sequence{f_{2,k}(x_2)}$ has a convergent subsequence $S_2$, say $\sequence{f_{2,k'}(x_2)}$ which is a subsequence of $\sequence{f_2(x_2)}$.
	And more importantly, $S_2$ is a subsequence of $S_1$ such that $S_2$ converges for both $x_1,x_2 \in E$.
	\[ S_2 : f_{2,1}\ f_{2,2}\ f_{2,3} \dots \]

	Continuing like this, we get a sequence of subsequences
	\begin{align*}
		S_1 \quad & : f_{n,1}\quad f_{n,2}\quad f_{n,3}\quad \dots \\
		S_2 \quad & : f_{n,1}\quad f_{n,2}\quad f_{n,3}\quad \dots \\
		 & \dots \\
		S_n \quad & : f_{n,1}\quad f_{n,2}\quad f_{n,3}\quad \dots
	\end{align*}

	Consider the diagonal sequence, $S : f_{1,1}\ f_{2,2}\ f_{3,3} \dots $.
	We know that discarding finitely many first terms won't affect convergence of sequences.
	And for any natural number $n$, we have can obtain a subsequence of $S_n$ by discarding a finite number of first terms from $S$.
	Thus, we have sequence $S$ which converges for $x_1,x_2,\dots,x_n \in E$ since $S_n$ converges.
	Therefore, as $n \to \infty$ we have a convergent subsequence $S$ of $\sequence{f_n}$ which converges for every $x \in E$.
\end{proof}

\begin{theorem}
	Let $K$ be a compact metric space.
	If $\sequence{f_n}$ is a sequence of pointwise bounded, continuous, complex valued functions on $K$ converges uniformly on $K$, then $\sequence{f_n}$ is equicontinuous on $K$.
\end{theorem}
\begin{proof}
	Let $K$ be a compact metric space.
	Let $\varepsilon > 0$.
	Let $\sequence{f_n}$ converges uniformly on $K$.
	Then,
	\[ \forall \varepsilon > 0,\ \exists N \in \mathbb{N} \text{ such that } \forall n > N,\ \forall x \in K,\ \| f_n(x)-f_N(x) \| < \varepsilon \]

	We have continuous functions on compact sets are uniformly continuous.
	Thus for any $\varepsilon > 0$ there exist $\delta > 0$ such that $|f_j(x) - f_j(y)| < \varepsilon$ whenever $d(x,y) < \delta$.
	Thus, $|f_N(x) - f_N(y)| \le \varepsilon$.\\

	Let $1 \le j \le N$.
	Since the continuity is uniform, there exists $\delta > 0$ such that $|f_j(x) - f_j(y)| < \varepsilon$.\dag\footnote{
		For each functions $f_j$, given $\varepsilon > 0$, there exists $\delta_j > 0$ satisfying the $\varepsilon$-$\delta$ condition for uniform continuity.
		Define $\delta = \min \{ \delta_j : j = 1,2,\dots,N \}$, then for this $\delta$ the condition is satified by functions $f_1,f_2,\dots,f_N$.}\\

	Let $n > N$ and $d(x,y) < \delta$.
	Then,
	\[ |f_n(x)-f_n(y)| \le |f_n(x)-f_N(x)| + |f_N(x) - f_N(y)| + |f_N(y) - f_n(y)| < 3\varepsilon \]

	Therefore, for any $\varepsilon > 0$ there exists $\delta > 0$ such that $\forall n \in \mathbb{N},\ |f_n(x) - f_m(x)| < \varepsilon$ whenever $d(x,y)< \delta$.
	\[ \forall \varepsilon > 0,\ \exists \delta > 0,\ \forall n \in \mathbb{N},\!\! \underset{d(x,y) < \delta}{\forall} \!\!\!\! y \in K,\ |f_n(x)-f_n(y)| < \varepsilon \]
	That is, the sequence $\sequence{f_n}$ is equicontinuous on $K$.
\end{proof}

\begin{theorem}
	Let $K$ a compact.
	If $\sequence{f_n}$ be a sequence of pointwise bounded, equicontinuous, complex functions on $K$, then
	\begin{enumerate}
		\item $\sequence{f_n}$ is uniformly bounded on $K$.
		\item $\sequence{f_n}$ has a uniformly convergent subsequence.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $\sequence{f_n}$ be sequence of point-wise bounded, equicontinuous, complex functions on compact set $K$.
	Since $f_n$ are equicontinuous, given $\varepsilon > 0$, there exists $\delta > 0$ such that $\forall n \in \mathbb{N},\ |f_n(x)-f_n(y)| < \varepsilon$ whenever $d(x,y) < \delta$.\\

	Let $\mathcal{C}$ be a cover of $K$ of open balls of radius $\delta$.
	Since, $K$ is compact this cover has a finite subcover, say open balls with center $p_j,\ j = 1,2,\dots,r$.
	Thus there exists finitely many points, $p_j$ such that every point in $K$ is sufficiently close one of them.
	Then for any $x \in K$, there exists some $p_j$ such that $d(x,p_j) < \delta$.\\

	Since $\sequence{f_n}$ is point-wise bounded, there exists $M_j$ such that $|f_n(p_j)| < M_j$.
	Define $M = \max \{ M_1, M_2, \dots, M_r\}$. 
	Then 
	\begin{align*}
		|f_n(x)| & = |f_n(x)-f_n(p_j)+f_n(p_j)| \\
		& \le |f_n(x) - f_n(p_j)| + |f_n(p_j)| \\
		& \le \varepsilon + M
	\end{align*}
	Therefore, $\sequence{f_n}$ is uniformly bounded.\\

	\hrule \vspace{1em}

	Let $\varepsilon > 0$.
	Choose $\delta > 0$ such that $|f_n(x)-f_n(y)| < \varepsilon$ whenever $d(x,y) < \delta$.
	Since $K$ is compact, $K$ has a countable dense subset, say $E = \{x_1,x_2,\dots\}$.
	That is, given $\delta > 0$, for any $x \in K$, there exists $x_j \in E$ such that $d(x,x_j) < \delta$.
	Clearly, $K$ has a cover of open balls with center $x_j$s and radius $\delta$.
	Since $K$ is compact, there exists finitely many points $x_1,x_2,\dots,x_m \in E$ such that $d(x,x_m) < \delta$.\\

	Since $E$ is countable and $\sequence{f_n}$ is point-wise bounded, $\sequence{f_n}$ on $E$ has a subsequence, say diagonal sequence $f_{n_i} = g_i$ which converges for any $x_j \in E$.
	Thus, by Cauchy criterion there exists integer $N$ such that 
	\[ \forall i,j \ge N,\ |g_i(x_s) - g_j(x_s)| < \varepsilon,\quad s = 1,2,\dots,m \]

	Let $x \in K$.
	Let $x_s \in E$ such that $d(x,x_s) < \delta$.
	Then,
	\[ |g_i(x) - g_j(x)| \le |g_i(x)-g_i(x_s)| + |g_i(x_s) - g_j(x_s)| + |g_j(x_s) - g_j(x)| \le 3\varepsilon \]
	That is, $\sequence{g_i}$ is uniformly convergent on $K$.
	Therefore, $\sequence{f_n}$ has a uniformly convergent subseqeunce $\sequence{f_{n_k}}$.
\end{proof}

\begin{theorem}[Weierstrass' Theorem]
	If $f$ is a continuous, complex function on $[a,b]$, there exists a sequence of polynomials $P_n$ such that
	\[ \lim_{n \to \infty} P_n(x) = f(x) \]
	uniformly on $[a,b]$.
	If $f$ is real, the $P_n$ may be taken real.
\end{theorem}
\begin{important}
In other words, any continuous function has a polynomial approximation.
\end{important}
\begin{proof}
	Without loss of generality, suppose $[a,b] = [0,1]$ and $f(0) = f(1) = 0$.\\

	\textbf{Step 1 : WLoG $f(0) = f(1) = 0$}
	\begin{commentary}
	If theorem is true for continuous functions satisfying $f(0) = f(1) = 0$, then it is true for any continuous function.\\
	\end{commentary}

	Let $f$ be any continuous function on $[0,1]$.
	Then there exists a function $g : [0,1] \to \mathbb{R}$ be defined by,
	\[ g(x) = f(x)-f(0) - x[f(1) - f(0)] \]
	such that $g(0) = g(1) = 0$.
	Suppose $g$ can be expressed as limit function of a uniformly convergent sequence of polynomials, say $P_n$.
	We have,
	\[ (f-g)(x) = x[f(1) - f(0)] + f(0) \]
	is a polynomial, say $P(x)$.
	Then $P_n+P \to f$ uniformly as $n \to \infty$, since $P_n \to g$ uniformly and $P$ is a constant polynomial independent of $n$.
	\[ \lim_{n \to \infty} (P_n+P)(x) = \lim_{n \to \infty} P_n(x) + P(x) = g(x) + (f-g)(x) = f(x) \]
	Therefore, it is enough to prove the theorem is true for any continuous function $f$ satisfying $f(0) =f(1) = 0$.\\

	\hrule \vspace{1em}

	\textbf{Step 2 : Construction of $P_n(x)$}\\
	Since $f$ is continous in $[0,1]$, $f$ is uniformly continuous in $[0,1]$.
	Extend $f$ such that $f(x) = 0, \ \forall x \notin [0,1]$.
	Then,  $f$ is uniformly continuous on $\mathbb{R}$.
	Define $Q_n(x) = c_n(1-x^2)^n$ such that
	\[ \int_{-1}^1 Q_n(x) \ dx = 1 \]
	Then we have,
	\begin{align*}
		\int_{-1}^1 (1-x^2)^n \ dx 
		& = 2\int_0^1 (1-x^2)^n \ dx \ \text{ since } (1-x^2)^n \text{ is even} 
		\intertext{We have, Bernouli's inequality. $(1+x)^r \ge (1+rx),\ \forall x \ge -1,\ \forall r \ge 0$}
		\int_{-1}^1 (1-x^2)^n \ dx 
		& \ge 2\int_0^{1/\sqrt{n}} (1-nx^2) \ dx \\
		& \ge 2 \left(\frac{1}{\sqrt{n}} - \frac{n}{3n\sqrt{n}} \right) = \frac{4}{3\sqrt{n}} > \frac{1}{\sqrt{n}}
	\end{align*}
	Clearly, $c_n < \sqrt{n}$. \\

	For $\delta > 0$, $Q_n(x) \le \sqrt{n}(1-\delta^2)^n$ for $\delta \le |x| \le 1$.	
	Then $Q_n \to Q$ uniformly for all such that $\delta \le |x| \le 1$.
	Define $P_n : [0,1] \to \mathbb{R}$ defined by 
	\[ P_n(x) = \int_{-1}^1 f(x+t) \ Q_n(t) \ dt \]
	Then,
	\begin{align*}
		P_n(x) & = \int_{-1}^1 f(x+t) \ Q_n(t) \ dt \\
		& = \int_{-x}^{1-x} f(x+t) \ Q_n(t) \ dt \text{ since } f \text{ vanishes outside } [0,1]\\
		& = \int_0^{1} f(t) \ Q_n(t-x) \ dt 
	\end{align*}
	{\color{red}
	Continuous function $f$ is uniformly continuous on compact interval $[0,1]$.
	Thus $f$ is Riemann integrable on $[0,1]$.
	And $Q_n(t-x) = c_n [1-(t+x)^2]^n$.
	From integration by parts, we know that $\sequence{P_n}$ is a sequence of polynomials.
	\begin{align*}
		P_n(x) 
		& = \int_0^1 f(t) Q_n(t+x) dt \\
		& = \left[f(t)Q_n'(t+x)\right]_0^1 - \int_0^1 Q_n'(t+x) \int_0^1 f(t) dt\\
		& = f(1)Q_n'(1+x) -f(0)Q_n'(x) - [F(1)-F(0)]\int_0^1 Q_n'(t+x) dt \\
		& = f(1)Q_n'(1+x) - f(0)Q_n'(x) - [F(1)-F(0)] [Q_n(1+x) - Q_n(x)]
	\end{align*}
	}
	And for each natural number $n$, we have $P_n(x)$ is real, if $f$ is real.\\

	\hrule \vspace{1em}

	\textbf{Step 3 : $P_n \to f$ uniformly}\\
	Let $\varepsilon > 0$.
	Since extended $f$ is uniformly continuous on real line, there exists $\delta > 0$ such that $|f(y)-f(x)| < \frac{\varepsilon}{2}$ whenever $|y-x| < \delta$.
	\begin{align*}
		|P_n(x)-f(x)| 
		& = \left| \int_{-1}^1 f(x+t)Q_n(t)\ dt - f(x)\int_{-1}^1 Q_n(t)\ dt \right| \\
		& \le \int_{-1}^1 |f(x+t)-f(x)| Q_n(t)\ dt 
		\intertext{Let $M =\sup f(x)$. Then $|f(x+t)-f(x)| \le 2M$. And we have an upper bound for the value $Q_n(x)$ for $\delta \le |x| \le 1$. Therefore, we split the domain of integral into three parts so that we may apply uniform continuity on the middle part and bound of $Q_n$ on other two parts.}
		|P_n(x)-f(x)| 
		& \le 2M \int_{-1}^{-\delta} Q_n(t)dt + \frac{\varepsilon}{2} \int_{-\delta}^{\delta} Q_n(t)dt+2M\int_{\delta}^1 Q_n(t)dt 
		\intertext{We have an upper bound for $Q_n$, say $Q_n(x) \le \sqrt{n}(1-\delta^2)^n$ for $\delta \le |x| \le 1$.}
		|P_n(x)-f(x)| 
		& \le 2M\sqrt{n}(1-\delta^2)^n \int_{-1}^{-\delta} dt + \frac{\varepsilon}{2} \int_{-\delta}^{\delta} Q_n(t)dt+2M \sqrt{n} (1-\delta^2)^n \int_{\delta}^1 dt \\
		& \le 4M\sqrt{n}(1-\delta^2)^n + \frac{\varepsilon}{2} \quad \text{ since } 2(1-\delta) < 2 \\
		& \le \varepsilon \text{ for sufficiently large n}
	\end{align*}
	Therefore, there exists $N \in \mathbb{N}$ such that $\forall n > N,\ |P_n(x) - f(x)| < \varepsilon$.
	In other words, $P_n \to f$ uniformly on $[0,1]$. 
\end{proof}

\begin{corollary}
	For every interval $[-a,a]$ there is a sequence of real polynomials $P_n$ such that $P_n(0) = 0$ and
	\[ \lim_{n \to \infty} P_n(x) = |x| \]
	uniformly on $[-a,a]$.
\end{corollary}
\begin{proof}
	By Weierstrass theorem, there exists a sequence $\sequence{P_n^\ast(x)}$ of real polynomials which converges to $|x|$ uniformly on $[-a,a]$.
	Thus, $P_n^\ast(0) \to 0$ as $n \to \infty$.
	Define $P_n(x) = P_n^\ast(x) - P_n^\ast(0)$.
	Clearly, $P_n(0) = 0$ and the sequence $\sequence{P_n(x)}$ converges uniformly on $[-a,a]$.
	And,
	\[ \lim_{n \to \infty} P_n(x) = \lim_{n \to \infty} P_n^\ast(x) - \lim_{n \to \infty}P_n^\ast(0) = |x| \]	
	Therefore, $P_n(0) = 0$ and $P_n(x) \to |x|$ as $n \to \infty$.
\end{proof}

\begin{remark}[Out of Syllabus]
	On the other side of this corollary, we have Stone-Weierstrass theorem which study functions that doesn't vanish anywhere.\\

	Let $\mathscr{A}$ be an algebra of functions defined on compact set $K$ that separates points and vanishes nowhere.
	By Stone-Weierstrass theorem,\dag\footnote{
		\cite[\S7.32]{rudin} Let $\mathscr{A}$ be a family of functions defined on compact set $K$.\\
		$\mathscr{A}$ separates points if $\forall x \in K,\ \exists f,g \in \mathscr{A}$ such that $f(x) \ne g(x)$.\\
		$\mathscr{A}$ vanishes at a point $x \in K$ if $\forall x \in K,\ \exists f \in \mathscr{A}$ such that $f(x) \ne 0$ }
	any continuous function $f$ on $K$  has a sequence of functions in $\mathscr{A}$ which converges to $f$ uniformly on $K$.
	The algebra of even polynomials doesn't separate points since $P_n(x) = P_n(-x)$.
\end{remark}

%\chapter{Weierstrass Approximation \& Some Special Functions
\subsection{Some special functions}
\begin{definition}[analytic function]
	A function $f$ is (real) analytic if it can be represented by a power series.
	\[ f(x) = \sum_{j=1}^\infty c_n x^n \]
\end{definition}

\begin{remark}
	The open interval in which a power series $\sum c_n x^n$ converges is the \textbf{interval of convergence}.
\end{remark}

\begin{theorem}
	Suppose series $\sum c_n x^n$ converges for $|x| < R$.
	Suppose function $f : (-R,R)$ is defined by \[ f(x) = \sum_{n = 0}^\infty c_n x^n,\ |x| < R \]
	Then,for any $\varepsilon > 0$, the series $\sum c_n x^n$ converges uniformly on $[-R+\varepsilon,R-\varepsilon]$.
	Also the function $f$ is continuous and differentiable in $(-R,R)$ and
	\[ f'(x) = \sum_{n = 1}^\infty nc_n x^{n-1},\quad |x| < R \]
\end{theorem}
\begin{proof}
	Let $\varepsilon > 0$.
	Then for $|x| \le R - \varepsilon$ we have $|c_n x^n| \le |c_n (R-\varepsilon)^n|$.\\

	We have, 
	\[ \limsup_{n \to \infty} |R-\varepsilon| \sqrt[n]{|c_n|} = (R-\varepsilon) \limsup_{n \to\infty} \sqrt[n]{|c_n|} = 0 < 1 \]

	By raio test, the series $\sum c_n (R-\varepsilon)^n$ converges absolutely.
	By Weierstrass M test, series $\sum c_n x^n$ converges uniformly on $[-R+\varepsilon,R-\varepsilon]$.
	In the same fashion, the series $\sum nc_n x^{n-1}$ converges uniformly on $[-R+\varepsilon,R-\varepsilon]$ since $\displaystyle \limsup_{n \to \infty} \sqrt[n]{n|c_n|} = 0$.\\

	Let $x_0 \in (-R,R)$.
	Then there exists $\varepsilon > 0$ such that $x_0 \in [-R+\varepsilon, R-\varepsilon]$.
	The series $\sum n c_n x^{n-1}$ converges to $f'(x)$ uniformly on $[-R+\varepsilon,R-\varepsilon]$ and $f(x_0) = \sum c_n x_0^n$.
	Thus, $f$ is differentiable on $[-R+\varepsilon,R-\varepsilon]$ and
	\[ f'(x) = \lim_{n \to \infty} \sum_{k=1}^n k c_k x^{k-1} = \sum_{n = 1}^\infty n c_n x^{n-1} \]
	We know that $f$ is continuous at a point if it is differentiable at that point.
	Thus, $f$ is continuous on $[-R+\varepsilon,R-\varepsilon]$.
\end{proof}

\begin{corollary}
	Suppose $\displaystyle \sum_{n = 0}^\infty c_n x^n$ converges for $|x| < R$ and function $f$ is defined by $f(x) = \displaystyle \sum_{n = 0}^\infty c_n x^n$.
	Then $f$ has derivatives of all ordres, say $f^{(k)}(x)$ given by
	\[ f^{(k)}(x) = \sum_{n=k}^\infty n(n-1)\dots(n-k+1)c_n x^{n-k} \]
	In particular,
	\[ f^{(k)}(0) = k!\ c_k \]
\end{corollary}
\begin{proof}
	Let $f(x) = \sum c_n x^n$.
	Then, we have 
	\[ f'(x) = \sum_{n = 1}^\infty n c_n x^n \]
	By mathematical induction, we have
	\[ f^{(k)}(x) = \sum_{n = k}^\infty n(n-1)\dots(n-k+1) c_n x^{n-k} \]
	When $x = 0$, we get
	\[ f^{(k)}(0) = \sum_{n = k}^\infty n(n-1)\dots(n-k+1) c_n 0^{n-k} = {\color{red}k!\ c_k} \]
\end{proof}

\begin{theorem}
	Suppose $\displaystyle \sum_{n = 0}^\infty c_n$ converges.
	Define \[ f(x) = \sum c_n x^n,\quad -1<x<1 \]
	Then,
	\[ \lim_{x \to 1} f(x) = \sum_{n=0}^\infty c_n \]
\end{theorem}
\begin{proof}
	Let $s_n = c_0 + c_1 + \dots + c_n$ and $s_{-1} = 0$.
	Suppose $\sum c_n$ converges, then $s_n$ converges, say $s_n \to s$.
	Let $\varepsilon > 0$.
	Then there exists $N \in \mathbb{N}$ such that $\forall n > N,\ |s-s_n| < \frac{\varepsilon}{2}$.\\

	Also we have,
	\begin{align*}
		\sum_{n=0}^m c_n x^n
		& = \sum_{n=0}^m (s_n-s_{n-1})x^n \\
		& = \sum_{n=0}^m s_n x^n - xs_{n-1}x^{n-1} \\
		& = \sum_{n=0}^m s_nx^n - x\sum_{n=0}^{m}s_{n-1}x^{n-1} \\
		& = \sum_{n=0}^m s_nx^n - x\sum_{n=-1}^{m-1}s_nx^n \\
		& = \sum_{n=0}^{m-1} s_nx^n + s_mx^m - x\sum_{n=0}^{m-1}s_nx^n \text{ since $s_{-1} = 0$} \\
		& = (1-x)\sum_{n=0}^{m-1} s_nx^n + s_mx^m \\
		\lim_{m \to \infty} \sum_{n=0}^m c_n x_n
		& = (1-x) \lim_{m \to \infty} \sum_{n=0}^{m-1} s_nx^n + \lim_{m \to \infty} s_m x^m  \\
		f(x) & = (1-x) \sum_{n=0}^\infty s_nx^n \text{ since $s_m \to s$, $|x|<1$ and $x^m \to 0$}
	\end{align*}

	{\color{red}
	We know that,
	\[ (1-x)\sum_{n=0}^m x^n = (1-x)(1+x+x^2+\dots+x^m) = 1-x^{m+1} \]
	Thus,
	\[ \lim_{m \to \infty} (1-x)\sum_{n=0}^m x^n = \lim_{m \to \infty} 1-x^{m+1} = 1 \]
	}
	Thus,
	\begin{align*}
		|f(x) - s| 
		& = \left| (1-x)\sum_{n=0}^\infty s_nx^n - s(1-x)\sum_{n=0}^\infty x^n \right| \\
		& = (1-x) \left| \sum_{n=0}^\infty (s_n-s)x^n \right| \\
		& \le (1-x) \sum_{n=0}^\infty |(s_n-s)x^n| \\
		& \le (1-x)\sum_{n=0}^\infty |s_n-s|\ |x|^n  \\
		& {\color{red}\le (1-x)\sum_{n=0}^N |s_n-s|\ |x|^n +  \frac{\varepsilon}{2} (1-x) \sum_{n=N+1}^\infty x^n} \\
	\end{align*}
	{ \color{red}
	Let $1-x < \delta < 1$.\dag\footnote{
		In earlier version, the term $|s_n-s|$ was neglected.
		Corrected as per the seminar by Haripriya}
	\[ \lim_{x \to 1} |f(x) - s| \le \lim_{\delta \to 0} \delta \sum_{n=0}^N |s_n -s| (1-\delta)^n + \frac{\varepsilon}{2} = 0 \]
	}	
	Therefore, 
	\[ f(1) = \lim_{x \to 1} f(x) = s = \sum_{n = 0}^\infty c_nx^n \]
\end{proof}

\begin{theorem}
	Given a double sequence $\sequence{a_{ij}}$.
	Suppose that,
	\[ \sum_{j=1}^\infty |a_{ij}| = b_i \]
	and $\sum b_i$ converges.
	Then,
	\[ \sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij} = \sum_{j=1}^\infty \sum_{i=1}^\infty a_{ij} \]
\end{theorem}
\begin{proof}
	\textbf{Step 1 : Construction of $f_i$}\\
	Given series $\displaystyle \sum_{j=1}^\infty |a_{ij}|$ converges to $b_i$.
	Let $E = \{ x_0,x_1,x_2,\dots \}$ be a countable set such that $x_n \to x_0$ as $n \to \infty$.
	Define sequence of functions $\sequence{f_i}$ on $E$ such that 
	\[ f_i(x_n) = \sum_{j=1}^n a_{ij},\ \forall n \in \mathbb{N} \quad \text{ and } \quad f_i(x_0) = \sum_{j=1}^\infty a_{ij} \]
	Clearly, $f_i(x_0) = b_i$ and $f_i(x_n) \to f_i(x_0)$ as $n \to \infty$.\\

	\textbf{Step 2 : $f_i$ is continuous at $x_0$}\\
	We have, $f_i(x_n) \to f_i(x_0)$.
	{\color{red}Then \dag\footnote{
		The method of contradiction was an unnecessary complication.
		Corrected as per the seminar by Mekha},
	\[ \lim_{x_n \to x_0} f_i(x_n) = \lim_{n \to \infty} \sum_{i=1}^n a_{ij} = \sum_{i=1}^\infty a_{ij} = f_i(x_0) \]
	}
	Therefore, function $f_i$ is continuous at $x_0$.\\

	\textbf{Step 3 : Construction of $g$}\\
	Given $|f_i(x)| < b_i$ and $\sum b_i$ converges.
	Thus, $\sum f_i(x)$ converges.
	Define $g : E \to \mathbb{R}$ such that
	\[ g(x) = \sum_{i = 1}^\infty f_i(x) \]
	Since $f_i$ are continuous functions defined on a countable set $E$, the convergence is uniform and the sum $g$ is continuous.
	And we have,
	\[ \lim_{n \to \infty} \lim_{m \to \infty} \sum_{i=1}^m f_i(x_n) = \lim_{x_n \to x_0} g(x_n) = g(x_0) = \lim_{m \to \infty} \sum_{i=1}^m f_i(x_0) = \lim_{m \to \infty} \lim_{n \to \infty} \sum_{i = 1}^m f_i(x_n) \]
	{\color{red}
	Therefore,
	\[ \sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij} = \sum_{j=1}^\infty \sum_{i=1}^\infty a_{ij} \]
	}
\end{proof}

\begin{theorem}[Taylor]
	Suppose $f(x) = \sum c_n x^n$, the series converging in $|x| < R$.
	If $-R < a < R$, then $f$ can be expanded in a power series about the point $x = a$ which converges in $|x-a| < R-|a|$.
	And,
	\[ f(x) = \sum_{n = 0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n,\quad |x-a| < R-|a|\]
\end{theorem}
\begin{proof}
	Suppose 
	\begin{align*}
		f(x)
		& = \sum_{n=0}^\infty c_n[(x-a)+a]^n \\
		& = \sum_{n=0}^\infty c_n \sum_{m=0}^n  \binom{n}{m} a^{n-m}(x-a)^m 
		%& = \lim_{k \to \infty} \sum_{n=0}^k c_n \sum_{m=0}^n  \binom{n}{m} a^{n-m}(x-a)^m 
		\intertext{Changing the order of summation, we may combine coefficients of $(x-a)^m$.}
		%& = \lim_{k \to \infty} \sum_{m=0}^k \sum_{n = m}^k \binom{n}{m} c_n a^{n-m} (x-a)^m \\
		& = \sum_{m=0}^\infty \sum_{n = m}^\infty \binom{n}{m} c_n a^{n-m} (x-a)^m
	\end{align*}

	Therefore, it is enough to prove that the order of summation can be changed.
	We know that the order of summation can be changed if,
	\[ \sum_{n = 0}^\infty \sum_{m = 0}^n \left| c_n \binom{n}{m} a^{n-m} (x-a)^m \right| \text{ converges} \]
	We know that,
	\[ \sum_{n = 0}^\infty \sum_{m = 0}^n \left| c_n \binom{n}{m} a^{n-m} (x-a)^m \right| = \sum_{n = 0}^\infty |c_n|\ (|x-a|+|a|)^n \]
	and it converges if $|x-a|+|a| < R-|a|$.\\

	Also we know that, if $f(x) = \sum c_n x^n$ converges in $|x|<R$, then the convergence is uniform in $[-R+\varepsilon,R-\varepsilon]$ and it is differentiable in $(-R,R)$.
	And the derivatives are given by,
	\[ f^{(m)}(a) = \sum_{n=m}^\infty \frac{n!}{(n-m)!}c_n a^{n-m} = m! \sum_{n=m}^\infty \binom{n}{m} c_n a^{n-m} \]
	Thus,
	\[ f(x) = \sum_{m = 0}^\infty \sum_{n = m}^\infty c_n\binom{n}{m} a^{n-m} (x-a)^m = \sum_{m=0}^\infty \frac{f^{(m)}(a)}{m!}(x-a)^m \]
\end{proof}

\begin{theorem}
	Suppose the series $\sum a_n x^n$ and $\sum b_n x^n$converge in the segment $S = (-R,R)$.
	Let $E$ be the set of all $x \in S$ at which
	\[ \sum_{n = 0}^\infty a_n x^n = \sum_{n = 0}^\infty b_n x^n \]
	If $E$ has a limit point on $S$, then $a_n = b_n$.
\end{theorem}
\begin{important}
	In other words, If two power series coverges to the same function in $(-R,R)$, then the series are identical.
\end{important}
\begin{proof}
	---continue---
\end{proof}

\begin{doubt}
Whether the power series representation is unique ?
\end{doubt}

\begin{theorem}
	Let $e^x$ be defined on $\mathbb{R}$.
	Then
	\begin{enumerate}
		\item $e^x$ is continuous and differentiable for all $x$.
		\item $(e^x)' = e^x$
		\item $e^x$ is a strictly increasing function
		\item $e^{x+y} = e^x e^y$
		\item $e^x \to +\infty$ as $x \to \infty$ an d $e^x \to 0$ as $x \to \infty$ and $e^x \to 0$ as $x \to -\infty$
		\item $\displaystyle \lim_{x \to \infty} x^ne^{-x} = 0$ for every $n$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $\displaystyle E(z) = \sum_{k=0}^\infty \frac{z^k}{k!}$.\\

	We know that,	
	\[ E(1) = \sum_{k=0}^\infty \frac{1^n}{n!} = e \quad \text{and} \quad E(0) = 1 \]

	And,
	\begin{align*}
		E(z)E(w) & = \sum_{n=0}^\infty \frac{z^n}{n!} \sum_{m=0}^\infty \frac{w^m}{m!} \\
		& = \sum_{n=0}^\infty \sum_{k=0}^\infty \frac{z^k w^{n-k}}{k!(n-k)!}  \\
		& = \sum_{n=0}^\infty \frac{1}{n!} \sum_{k=0}^\infty \binom{n}{k} z^k w^{n-k} \\
		& = \sum_{n=0}^\infty \frac{(z+w)^n}{n!} \\
		& = E(z+w)
	\end{align*}

	Then we have,
	\[ E(z)E(-z) = E(0) = 1 \]

	We $E(1) = e > 1$.
	Let $M$ be any integer.
	Then $E(M) = e^M > M$.
	Thus there exists $x \in \mathbb{R}$ such that $E(x) > M$.
	Therefore, $E(x) \to +\infty$ as $x \to +\infty$.\\

	From definition, $E(x) > 0$ for any $x > 0$.
	And for any $x < 0$, $-x > 0$ and $E(-x) > 0$.
	Therefore, $E(x) = 1/E(-x) > 0$.
	Thus, $E(z) > 0$ for any $x \in \mathbb{R}$.
	Let $\varepsilon > 0$.
	Then there exists an integer $M$ such that $\varepsilon > 1/M$.
	We know that there exists $x \in \mathbb{R}$ such that $E(x) > M$.
	Then, $0 < E(-x) = 1/E(x) < 1/M < \varepsilon$.
	Thus, $E(x) \to 0$ as $x \to -\infty$.\\

	\begin{align*}
		E'(z) & = \lim_{h \to 0} \frac{E(z+h)-E(z)}{h} \\
		& = \lim_{h \to 0} \frac{E(z)E(h) - E(z)}{h} \\
		& = E(z) \lim_{h \to 0} \frac{E(h)-1}{h} \\
		& = E(z) \lim_{h \to 0} (h+\frac{h^2}{2!}+\frac{h^3}{3!} + \dots )/ h \\
		& = E(z) 
	\end{align*}
	Thus, $E(x)$ is differentiable at every $x \in \mathbb{R}$.
	Therefore, $E(x)$ is continuous at every $x \in \mathbb{R}$.
	Thus, the function $E : \mathbb{R} \to (0,\infty)$ is strictly increasing.\\

	By mathematical induction
	\[ E(z_1 + z_2 + \dots + z_n) = E(z_1) E(z_2) \dots E(z_n) \]
	Thus,
	\[ E(n) = E(1)^n = e^n,\ \forall n \in \mathbb{N} \]
	Also we have,
	\[ E(-1) = \frac{1}{E(1)} = E(1)^{-1} = e^{-1} \]

	\[ E(-n) = E(-1)^n = \frac{1}{E(n)^n} = E(1)^{-n} = e^{-n},\ \forall n \in \mathbb{N} \]
	Thus,
	\[ E(n) = e^n,\ \forall n \in \mathbb{Z} \]

	Let $p \in \mathbb{Q}$.
	Then $p = n/m$.
	And,
	\[ E(p)^m = E(mp) = E(n) = e^n \implies E(p) = e^p,\ \forall p \in \mathbb{Q} \]

	Let $x \in \mathbb{R}$.
	Define 
	\[ E(x) = \sup_{p < x,\ p \in \mathbb{Q}} E(p) \]

	By continuity and monotonicity of $E$, we have $E(x) = e^x,\ \forall x \in \mathbb{R}$.
	Now rewriting the results, we get
	\begin{eqnarray}
		e^{x+y} &= e^x e^y \\
		\frac{d}{dx} e^x &= e^x 
	\end{eqnarray}

	We have,
	\begin{align*}
		e^x 
		& = \sum_{k=0}^\infty \frac{x^k}{k!} \\
		& > \frac{x^{n+1}}{(n+1)!} \\
		x^n e^{-x}
		& < \frac{(n+1)!}{x} \\
		\lim_{x \to \infty} x^n e^{-x} & < (n+1)! \lim_{x \to \infty} \frac{1}{x} = 0,\ \forall n \in \mathbb{N}
	\end{align*}
	Thus, $x^n e^{-x} \to 0$ as $x \to \infty$.
	As $x \to +\infty$, $e^{x} \to +\infty$ faster than any positive power $x$.
	\begin{important}
		This is no surprise since $E(1) = e > 1 = E(0)$.
	\end{important}
\end{proof}

\begin{remark}
	The function $E : \mathbb{R} \to (0,\infty)$ is a bijection.
	Thus, there exists an inverse function $L : (0,\infty) \to \mathbb{R}$ such that $E \circ L = I_{(0,\infty)}$ and $L \circ E = I_\mathbb{R}$ such that 
	\[ L(y) = \int_1^y \frac{1}{x} dx \]
	
	Furthermore, $L(x) \to +\infty$ as $x \to +\infty$ and $L(x) \to 0$ as $x \to -\infty$.
\end{remark}
\begin{proof}
	Let $u,v \in (0,\infty)$.
	Then there exists $x,y \in \mathbb{R}$ such that $E(x) = u$ and $E(y) = v$.
	Thus,
	\[ L(uv) = L(E(x)E(y)) = L(E(x+y)) = x+y \]

	Let $y = E(x)$.
	We have, $L(E(x)) = x$.
	And $L'(E(x)) E'(x) = yL'(y) = 1$ since $E'(x) = E(x) = y$.
	Thus, $L'(y) = 1/y$.\\

	From fundamental theorem for calculus,we have
	\[ \int_a^b f(t)\ dt = F(b) - F(a) \]
	where $f = F'$ on $(a,b)$.
	We have $E(x)$ is strictly monotonic and continuous, thus $L(x)$ is monotonic and continuous on $(0,\infty)$.
	And $L(x)$ is Riemann integrable on any interval $[a,b]$ subset of $(0,\infty)$.
	Put $F = L$.
	Then,
	\[ \int_a^b L'(x) dx = \int_a^b \frac{dx}{x} = L(b) - L(a) \]
	We know that $L(1) = 0$.
	Put $a = 1$  and $b = y$.
	Then, \[ L(y) = \int_1^y \frac{dx}{x} \]

	By mathematical induction,
	\[ L(x^m) = mL(x) \ \forall m \in \mathbb{N} \]
	\[ L(x) = L(x^\frac{m}{m}) = L(x^\frac{1}{m})^m = mL(x^\frac{1}{m}) \implies L(x^\frac{1}{m}) = \frac{1}{m}L(x) \]

	Thus, $x^n = E(nL(x))$ and $x^\frac{1}{m} = E(\frac{1}{m}L(x))$.
	And,
	\[ x = E(L(x)) = E(L(\frac{1}{m}x)^m) = E(mL(\frac{1}{m}x))\]
	Therefore,
	\[ x^\alpha = E(\alpha L(x)),\quad \forall x \in \mathbb{R},\ x > 0,\  \forall \alpha \in \mathbb{Q} \]
	Rewriting the relation with $\log x$ instead of $L(x)$ and $e^x$ instead of $E(x)$, we get $x^\alpha = e^{\alpha \log x}$ for any rational $\alpha$.\\

	We have,
	\[ \frac{d}{dx}x^\alpha = \frac{d}{dx} E(\alpha L(x)) = E'(\alpha L(x)) \alpha L'(x) = E(\alpha L(x)) \frac{\alpha}{x} = \alpha x^{\alpha - 1} \]
	Suppose $\alpha \ne -1$.
	For irrational values of $x$, $L(x)$ can be obtained from,
	\[ E(\alpha L(x)) = \int_1^x y^\alpha dy \]

	We have,
	\begin{align*}
		x^{-\alpha}\log x & = x^{-\alpha} \int_1^x t^{-1}\ dt \\
		& < x^{-\alpha} \int_t^x t^{\varepsilon-1}\ dt \\
		& < x^{-\alpha} \frac{x^\varepsilon -1}{\varepsilon}
	\end{align*}
	Clearly, 
	\[ \lim_{x \to +\infty} x^{-\alpha} \log x  = 0 \]
	As $x \to +\infty$, we have $\log x \to +\infty$ slower than any positive power of $x$.
\end{proof}


\begin{theorem}
	\begin{enumerate}
		\item The function $E$ is periodic with period $2\pi i$.
		\item The functions $C$ and $S$ are periodic with period $2\pi i$.
		\item If $0 < t < 2\pi$, then $E(it) \ne 1$.
		\item If $z$ is a complex number with $|z| =1$, there is a unique $t \in [0,2\pi]$ such that $E(it) = z$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $\displaystyle E(z) = \sum_{k=0}^\infty \frac{z^n}{n!}$.
\end{proof}

\begin{theorem}
	Suppose $a_0,a_1,\dots,a_n$ are complex numbers.
	\[ P(z) = \sum_{n=0}^\infty a_k z^k \]
	Then $P(z) = 0$ for some complex number $z$.
\end{theorem}
\begin{proof}
	---continue---
\end{proof}


