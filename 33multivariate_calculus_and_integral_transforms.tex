%Text Books : \cite{apostol}, \cite{rudin}

%Module 1:
%The Weirstrass theorem, other forms of Fourier series, the Fourier integral theorem, the exponential form of the Fourier integral theorem, integral transforms and convolutions, the convolution theorem for Fourier transforms.
%(Chapter 11 Sections 11.15 to 11.21 of \cite{apostol}) (20 hours.)
%Module 2:
%Multivariable Differential Calculus, The directional derivative, directional derivatives and continuity, the total derivative, the total derivative expressed in terms of partial derivatives, An application of complex- valued functions, the matrix of a linear function, the Jacobian matrix, the matrix form of the chain rule. Implicit functions and extremum problems, the mean value theorem for differentiable functions,
%(Chapter 12 Sections. 12.1 to 12.11 of \cite{apostol}) (22 hours.)
%Module 3: 
%A sufficient condition for differentiability, a sufficient condition for equality of mixed partial derivatives, functions with non-zero Jacobian determinant, the inverse function theorem ,the implicit function theorem, extrema of real- valued functions of one variable, extrema of real-valued functions of several variables.
%Chapter 12 Sections-. 12.12 to 12.13 of \cite{apostol} 
%Chapter 13 Sections-. 13.1 to 13.6 of \cite{apostol} (28 hours.)
%Module 4:
%Integration of Differential Forms Integration, primitive mappings, partitions of unity, change of variables, differential forms.
%(Chapter 10 Sections. 10.1 to 10.14 of \cite{rudin}) (20 hours)

%Module 1 - \cite{apostol} 11
%Module 2 - \cite{apostol} 12
%Module 3 - \cite{apostol} 12, 13
%Module 4 - \cite{rudin} 10
%\cite{apostol}
%\chapter{The Real and Complex Number Systems*}
%\chapter{Some Basic Notations of Set Theory*}
%\chapter{Elements of Point Set Topology*}
%\chapter{Limits and Continuity*}
%\chapter{Derivatives*}
%\chapter{Functions of Bounded Variation and Rectifiable Curves*}
%\chapter{The Riemann-Stieltjes Integral*}
%\chapter{Infinite Series adn Infinite Products*}
%\chapter{Sequences of Functions*}
%\chapter{The Lebesgue Integral*}

%\chapter{Fourier Series and Fourier Integrals}
\section{Integral Transforms}
\subsection{The Weierstrass Approximation Theorem}
\begin{important}
Every continuous, real valued function on a compact interval has a polynomial approximation.
	\cite[Theorem 11.17]{apostol}
\end{important}
\begin{theorem}[Weierstrass]
Let $f$ be a real-valued, continuous function on a compact interval $[a,b]$.
Then for every \(\epsilon > 0\), there is a polynomial $p$ such that \(|f(x)-p(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{theorem}
\begin{synopsis}
Given a real-valued continuous function on compact interval $[a,b]$, we can construct a real-valued, continous function $g$ on $\mathbb{R}$ which is periodic with period $2\pi$.
We have, if \(f \in L(I)\) and $f$ is bounded almost everywhere in $I$, then \(f \in L^2(I)\).
	\cite[Theorem 10.52]{apostol}.
By Fejer's theorem (\cite[Theorem 11.15]{apostol}), the fourier series generated by $g$ (\cite[definition 11.3]{apostol}) converges to the Cesaro sum (\cite[Definition 8.47]{apostol}), which is $g$ itself in this case.
Thus for any \(\epsilon > 0\), there is a finite sum of trignometric functions.
The power series expansions of trignometric functions (\cite[definition 9.27]{apostol}) being uniformly convergent, there exists a polynomial $p_m$ which approximates $g$.
And we can construct $p$ (polynomial approximation of $g$) using $p_m$.
\end{synopsis}
\begin{proof}
Define \(g : \mathbb{R} \to \mathbb{R}\), \[ g(t) = \begin{cases} f(a+(b-a)t/\pi),\ t \in [0,\pi) \\ f(a+(2\pi-t)(b-a)/\pi),\ t \in [\pi,2\pi] \\ g(t - 2n\pi),\ t > 2\pi,\ n \in \mathbb{N} \\ g(t+2n\pi),\ t < 0,\ n \in \mathbb{N} \end{cases}\]

Thus $g$ is a continuous, real-valued, periodic function with period $2\pi$ such that
\begin{equation}
	f(x) = g\left(\frac{\pi (x-a)}{b-a}\right),\ x \in [a,b] \label{equ:fx}
\end{equation}

The fourier series generated by $g$ is given by, \[ g(t) \sim \frac{a_0}{2} + \sum_{k=1}^\infty \left( a_k \cos kt + b_k \sin kt \right)\] \[ \text{ where } a_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos kt\ dt,\ b_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin kt\ dt\]

Let \(\sequence{s_n(t)}\) be the sequence of partial sums of the fourier series generated by $g$.
And \( \sequence{\sigma_n(t)}\)  be the sequence of averages of $s_n(t)$ given by, \[\sigma_n(t) = \frac{1}{n} \sum_{k = 1}^n s_k(t),\text{ where } s_k(t) = \frac{a_0}{2} + \sum_{j = 1}^k \left( a_j \cos jt + b_j \sin jt \right)\]

Function \(f \in L(I)\) being real-valued continuous function on a compact interval, it is bounded and hence is Lebesgue square integrable. ie, \(f \in L^2(I)\).
Thus, \(g \in L^2(I)\).

Since $g$ is continous on $\mathbb{R}$, the function \(s : \mathbb{R} \to \mathbb{R}\) defined by, \[ s(t) = \lim_{h \to 0^+} \frac{g(t+h)-g(t-h)}{2} \] is well-defined on $\mathbb{R}$ and \(s(t) = g(t),\ \forall t \in \mathbb{R}\).

Then by Fejer's Theorem, the sequence \(\sequence{\sigma_n(t)}\) converges uniformly to $g(t)$ for every \(t \in \mathbb{R}\).
Thus, given \(\epsilon > 0\), there exists \(N \in \mathbb{N}\) such that \(\forall t \in \mathbb{R}\), \(|g(t)-\sigma_N(t)| < \frac{\epsilon}{2}\).

We have,
\begin{equation}
	\sigma_N(t) = \sum_{k=0}^N \left(A_k \cos kt + B_k \sin kt \right),\text{ where } A_k, B_k \in \mathbb{R}
	\label{equ:sigmaN}
\end{equation}
By the power series expansion of the trignometric functions about origin,
\begin{equation}
	\cos kt = \sum_{j = 1}^\infty \left(\frac{\cos^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty A_j' t^j \text{ where } A_j' \in \mathbb{R}
	\label{equ:coskt}
\end{equation}
\begin{equation}
	\sin kt = \sum_{j = 1}^\infty \left(\frac{\sin^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty B_j' t^j \text{ where } B_j' \in \mathbb{R}
	\label{equ:sinkt}
\end{equation}

Since the above power series expansions of trignometric functions are uniformly convergent, their finite linear combination \(\sequence{\sigma_N(t)}\) is also uniformly convergent.
ie, Given \(\epsilon > 0\) there exists \(m \in \mathbb{N}\) such that for every \(t \in \mathbb{R}\)
\[\left|\sum_{k = 0}^m C_k t^k - \sigma_N(t)\right| < \frac{\epsilon}{2} \text{ where } C_k \in \mathbb{R}\]

Therefore, \(| p_m(t) - g(t)| \le | p_m(t) - \sigma_N(t) | + |\sigma_N(t) - g(t)| < \epsilon\) where \(p_m(t) = \sum_{k = 0}^m C_k t^k\).
Define \(p : [a,b] \to \mathbb{R}\) by,
\begin{equation}
	p(x) = p_m\left( \frac{\pi(x-a)}{b-a} \right)
	\label{equ:px}
\end{equation}

By equations \ref{equ:fx} and \ref{equ:px}, \(|p(x)-f(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{proof}

\subsection{Other Forms of Fourier Series}

Let \(f \in L([0,2\pi])\), then the fourier series generated by $f$ is given by,
\[ f(x) \sim \frac{a_0}{2}+\sum_{n=1}^\infty \left( a_n \cos nx + b_n \sin nx \right) \]
\[ \text{ where } a_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos nt\ dt,\qquad b_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin nt\ dt \]

By Euler's forumula \(e^{inx} = \cos nx + i\sin nx\).
We have, \(\cos nx = \frac{(e^{inx}+e^{-inx})}{2}\) and \(\sin nx = \frac{(e^{inx}-e^{-inx})}{2i}\)

\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( \alpha_n e^{inx} + \beta_n e^{-inx} \right) \]

\[ \text{ where } \alpha_n = \frac{(a_n - ib_n)}{2} \qquad \beta_n = \frac{(a_n+ib_n)}{2} \]

Therefore, by assigning \(\alpha_0 = a_0/2\), \(\alpha_{-n} = \beta_n\), we get the following exponential form of fourier series generated by $f$,

\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^{inx} \text{ where } \alpha_n = \frac{1}{2\pi} \int_0^{2\pi} f(t)\ e^{-int}\ dt \]

Note : If $f$ is periodic with period $2\pi$, then the interval of integration $[0,2\pi]$ can be replaced with any interval of length $2\pi$.
eg. $[-\pi,\pi]$

\subsubsection{Periodic with period $p$}
Let \(f \in L([0,p])\) and $f$ is periodic with period $p$.
Then
\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( a_n \cos \frac{2\pi nx}{p} + b_n \sin \frac{2\pi nx}{p} \right) \]
\[ \text{ where } a_n = \frac{2}{p} \int_0^p f(t) \cos \frac{2\pi nt}{p}\ dt \qquad b_n = \frac{2}{p} \int_0^p f(t) \sin \frac{2\pi nt}{p}\ dt \]
Therefore, we have the exponential form of the above fourier series given by,
\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^\frac{2\pi inx}{p},\text{ where } \alpha_n = \frac{1}{p} \int_0^p f(t)\ e^\frac{-2\pi int}{p}\ dt \]
	
\subsection{Fourier Integral Theorem}
\begin{theorem}[Fourier Integral Theorem]
Let \(f \in L(-\infty,\infty)\).
Suppose \(x \in \mathbb{R}\) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
\begin{enumerate}
	\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
	\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals \[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \] exists.
\end{enumerate}
Then, 
\[ \frac{f(x+)+f(x-)}{2} = \frac{1}{\pi} \int_0^\infty \int_{-\infty}^\infty f(u)\cos v(u-x)\ du\ dv, \] the integral $\int_0^\infty$ being an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\[ f(x+t)\frac{\sin \alpha t}{\pi t} dt \to f(u)\frac{\sin \alpha(u-x)}{\pi(u-x)} \to \frac{f(u)}{\pi} \int_0^\alpha \cos v(u-x) dv \]
By Riemann-Lebesgue lemma\cite[Theorem 11.6]{apostol},
\[ f \in L(I) \implies \lim_{\alpha \to +\infty} \int_I f(x) \sin \alpha t\ dt = 0 \]
By Jordan's Theorem\cite[Theorem 10.8]{apostol}, if $g$ is of bounded variation on $[0,\delta]$, then
\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
By Dini's Theorem\cite[Theorem 10.9]{apostol}, if the limit $g(x+)$ exists and Lebesgue integral \( \int_0^\delta \frac{g(t)+g(0+)}{t} dt \) exists for some \( \delta > 0 \), then
\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
The order of Lebesgue integrals can be interchanged.\cite[Theorem 10.40]{apostol}

Suppose \(f \in L(X)\) and \(g \in L(Y)\).
Then \[ \int_X f(x) \left(\int_Y g(y) k(x,y) dy \right) dx = \int_Y g(y) \left( \int_X f(x) k(x,y) dx \right) dy \]
\end{synopsis}
\begin{proof}
	Consider \( \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \).
	We prove that this integral is equal to the either sides.
	\[ \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt = \int_{-\infty}^{-\delta} + \int_{-\delta}^0 + \int_0^{-\delta}  + \int_{\delta}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \] 
	We have, function \( \frac{f(x+t)}{\pi t} \) is bounded on \( (-\infty,-\delta)\cup(\delta,\infty) \), hence \( \frac{f(x+t)}{\pi t} \) is Lebesgue integrable on \( (-\infty,-\delta) \cup (\delta,\infty) \).
	
	By Riemann Lebesgue lemma, 
	\[ \frac{f(x+t)}{\pi t} \in L(-\infty,-\delta) \implies \int_{-\infty}^{-\delta} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0, \]
	\[ \frac{f(x+t)}{\pi t} \in L(\delta,\infty) \implies \int_{\delta}^{\infty} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0 \]

	\paragraph{Case 1}
	Suppose $f$ is of bounded variation on $[x-\delta,x+\delta]$, put \( g(t) = f(x+t) \) then $g$ is of bounded variation on $[-\delta,\delta]$.
	Thus $g$ is of bounded variation on $[0,\delta]$.
	Then by Jordan's Theorem
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	\paragraph{Case 2}
	Suppose both the limits $f(x+)$ and $f(x-)$ exists and both Lebesgue integrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and } \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \]
	exists.

	Thus, we have $f(x+)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \) exists.
	Put \( g(t) = f(x+t) \), then \( g(0+) = f(x+) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	Similarly, $f(x-)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \) exists.
	Put \( g(t) = f(x-t) \), then \( g(0+) = f(x-) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\begin{align*}
		\lim_{\alpha \to +\infty} \frac{2}{\pi}\int_{-\delta}^0 f(x+t)\frac{\sin \alpha t}{t} dt 
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta f(x-\tau) \frac{\sin \alpha \tau}{\tau} d\tau\\
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(\tau) \frac{\sin \alpha \tau}{\tau} d\tau = g(0+) = f(x-)
	\end{align*}

	Then by either cases,
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt  
		& = \lim_{\alpha \to +\infty} \int_{-\delta}^0 + \int_0^\delta f(x+t) \frac{\sin \alpha t}{\pi t} dt \\
		& = \frac{f(x+)+f(x-)}{2}
	\end{align*}

	We have, \( \int_0^\alpha \cos v(u-x) dv = \frac{\sin v(u-x)}{u-x} \).
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt 
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \frac{ \sin \alpha (u-x)}{u-x} du,\ (\text{put }u = x+t)\\
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \left( \int_0^\alpha \cos v(u-x) dv \right) du\\
		& = \lim_{\alpha \to +\infty} \int_0^\alpha \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv,\\
		& \text{since, the order of Lebesgue integrals can be reversed.}\\
		& = \int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv\\
		\text{where, } \int_0^\infty \text{ is not }& \text{a Lebesgue integral, but an improper Riemann integral }
	\end{align*}
	Therefore,
	\begin{align*}
		\int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt \\
		& =  \frac{f(x+)+f(x-)}{2}
	\end{align*}
\end{proof}

\begin{remark}
	If a function $f$ on $(-\infty,\infty)$ is non-periodic, then it may not have a fourier series represenation.
	In such cases, we have fourier intergral representaion.
\end{remark}

\subsection{Exponential form of Fourier Integral Theorem}
Let \( f \in L(-\infty,\infty) \).
Suppose \( x \in \mathbb{R} \) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
\begin{enumerate}
	\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
	\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \] exists.
\end{enumerate}
Then, \[ \frac{f(x+)+f(x-)}{2} = \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)}\ du\right) dv \]
\begin{proof}
Let \( F(v) = \int_{-\infty}^\infty f(u) \cos v(u-x) du \).
Then \( F(v) = F(-v) \) and 
\begin{align*}
	\lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) dv
	& = \lim_{\alpha \to \infty} \frac{1}{\pi} \int_0^\alpha \int_{-\infty}^\infty f(u) \cos v(u-x) du dv\\
	& = \frac{f(x+)+f(x-)}{2}
\end{align*}
Let \( G(v) = \int_{-\infty}^\infty f(u) \sin v(u-x) du \).
Then \( G(v) = -G(-v) \) and
\[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha G(v) dv = 0 \]
Thus \[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) + iG(v) dv = \frac{f(x+)+f(x-)}{2} \]
\end{proof}

\subsection{Integral Transforms}
\begin{definition}
	\textbf{Integral transform} $g(y)$ of $f(x)$ is a Lebesgue integral or Improper Riemann integral of the form
	\[ g(y) = \int_{-\infty}^\infty K(x,y) f(x)\ dx \], where $K$ is the kernal of the transform.
	We write \( g = \mathscr{K}(f) \).
\end{definition}

\begin{remark}
	Integral transforms(operators) are linear operators.
	 ie, \( \mathscr{K}(af_1 + bf_2) = a\mathscr{K}f_1 + b\mathscr{K}f_2 \)
\end{remark}

\begin{remark} A few commonly used integral transforms,
\begin{enumerate}
	\item Exponential Fourier Transform $\mathscr{F}$,
		\[ \mathscr{F}f = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	\item Fourier Cosine Transform $\mathscr{C}$,
		\[ \mathscr{C}f = \int_0^\infty \cos xy f(x)\ dx \]
	\item Fourier Sine Transform $\mathscr{S}$,
		\[ \mathscr{S}f = \int_0^\infty \sin xy f(x)\ dx \]
	\item Laplace Transform $\mathscr{L}$,
		\[ \mathscr{L}f = \int_0^\infty e^{-xy} f(x)\ dx \]
	\item Mellin Transform $\mathscr{M}$,
		\[ \mathscr{M}f = \int_0^\infty x^{y-1}f(x)\ dx \]
\end{enumerate}
\end{remark}

\begin{remark} Suppose \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_{-\infty}^\infty e^{-ixy}f(x)\ dx = \int_0^\infty e^{-ixy}f(x)\ dx = \int_0^\infty \cos xy \ f(x)\ dx + i \int_0^\infty \sin xy \ f(x)\ dx \]
	\[ \mathscr{F}f = \mathscr{C}f + i\mathscr{S}f \]

	Therefore Fourier Cosine $\mathscr{C}$ and Sine $\mathscr{S}$ transforms are special cases of fourier integral transform, $\mathscr{F}$ provided $f$ vanishes on negative real axis.
\end{remark}

\begin{remark} Let \( y = u+iv \), \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_0^\infty e^{-xy}f(x) = \int_0^\infty e^{-xu}e^{-ixv}f(x)\ dx = \int_0^\infty e^{-ixv} \phi_u(x) dx \]
	where \( \phi_u(x) = e^{-xu}f(x) \).
	\[ \mathscr{L}f = \mathscr{F}\phi_u \]
	Therefore Laplace transform, $\mathscr{L}$ is a special case of Fourier integral transform, $\mathscr{F}$.
\end{remark}

\begin{remark} Let \( g(y) = \mathscr{F}f(x) \).
	\[ g(y) = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	Suppose $f$ is continuous at $x$, then by fourier integral theorem,
	\begin{align*}
		f(x)	& = \frac{1}{2\pi} \int_{-\infty}^\infty \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)} du \right) dv\\
			& = \int_{-\infty}^\infty e^{-ivx} \left( \frac{1}{2\pi} \int_{-\infty}^\infty e^{ivu} f(u)\ du \right) dv\\
			& = \int_{-\infty}^\infty g(v) e^{-ivx} dv = \mathscr{F}g \text{ where } g(v) = \frac{1}{2\pi}\int_{-\infty}^\infty f(u) e^{ivu} du 
	\end{align*}
	The above function $g(v)$ gives the \textbf{inverse fourier transformation} of $f$.

	Let $g$ be fourier transform of $f$, then $f$ is uniquely determined by its fourier transform $g$ by,
	\[ f(x) = \mathscr{F}^{-1}g(y) = \frac{1}{2\pi} \lim_{\alpha \to \infty} \int_{-\alpha}^\alpha g(y) e^{ixy} dy \]
\end{remark}

\begin{enumerate}
	\setcounter{enumi}{5}
	\item Inverse Fourier Transform $\mathscr{F}^{-1}$,
		\[ \mathscr{F}^{-1}f = \int_{-\infty}^\infty \frac{e^{ixy}}{2\pi}f(x)\ dx \]
\end{enumerate}

\subsection{Convolutions}
\begin{definition}
	Let \( f,g \in L(-\infty,\infty) \).
	Let $S$ be the set of all points $x$ for which the Lebesgue integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists.
	Then the function \( h : S \to \mathbb{R} \) is a \textbf{convolution} of $f$ and $g$.
	And \( h = f \ast g \).
\end{definition}

\begin{remark}
	Convolution operator is commutative.
	ie, \( h = f \ast g = g \ast f \)
	\begin{commentary}
		(hint : take $u = x-t$)
	\end{commentary}
	% $u = x-t \implies du = -dt$
	% sign is reversed as the order of limits are switched.
	% ie, $t = \infty \to u = -\infty$
\end{remark}

\begin{remark}
	Suppose $f,g$ vanishes on negative real axis, then
	\[ h(x) = \int_{-\infty}^\infty f(t)\ g(x-t)\ dt = \int_{-\infty}^0 + \int_0^x  + \int_x^\infty f(t)\ g(x-t)\ dt = \int_0^x f(t)\ g(x-t)\ dt \] 
\end{remark}

\begin{remark}
	Singularity of convolution is a point at which the convolution integral fails to exists.
\end{remark}

\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and either $f$ or $g$ is bounded in $\mathbb{R}$.
	Then the convoluton integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists for every \( x \in \mathbb{R} \) and the function $h$ so defined is bouned in $\mathbb{R}$.
	In addition, if the bounded function is continuous on $\mathbb{R}$, then $h$ is continuous and \( h \in L(\mathbb{R}) \).
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}
	If $f,g$ are both unbounded, the convolution integral may not exist.
	\[ \text{ eg: } f(t) = \frac{1}{\sqrt{t}},\ g(t) = \frac{1}{\sqrt{1-t}} \]
\end{remark}

\begin{theorem}
	Let \( f,g \in L^2(\mathbb{R}) \).
	Then the convolution integral $f \ast g$ exists for each \( x \in \mathbb{R} \) and the function \( h : \mathbb{R} \to \mathbb{R} \) defined by \( h(x) = f \ast g (x) \) is bounded in $\mathbb{R}$.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\subsection{The Convolution Theorem for Fourier Tranforms}
\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and at least one of $f$ or $g$ is continuous and bounded on $\mathbb{R}$.
	Let \( h = f \ast g \).
	Then for every real $u$,
	\[ \int_{-\infty}^\infty h(x) e^{-ixu} dx = \left( \int_{-\infty}^\infty f(t) e^{-itu} dt \right) \left( \int_{-\infty}^\infty g(y) e^{-iyu} dy \right) \]
	The integral on the left exists both as a Lebesgue integral and an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}[Application of Convolution Theorem]
	\[ B(p,q) = \frac{\Gamma{p} \Gamma{q}}{\Gamma{p+q}},\text{ where } B(p,q) = \int_0^1 x^{p-1} (1-x)^{q-1} dx,\ \Gamma{p} = \int_0^\infty t^{p-1} e^{-t} dt \]
\end{remark}

\section{Multivariate Differential Calculus}

In this chapter, we deal with real functions of several variables.
Instead of $\mathbf{c}$, we write \( \bar{c} \in \mathbb{R}^n \), then \( \bar{c} = (c_1, c_2, \dots, c_n) \) where \( c_j \in \mathbb{R} \) for every \(j = 1,2, \dots, n\).
Again, suppose \(f : \mathbb{R}^n \to \mathbb{R}^m\) and \(f(\bar{x}) = \bar{y}\), then \(\bar{y} = (y_1, y_2, \dots, y_m)\) where each $y_k$ is real.
The unit co-ordinate vector, $\bar{u}_k$ is given by \( {u_k}_j = \delta_{j,k} \)

\subsection{Directional Derivative}
\textsl{Motivation : The existence of all partial derivatives of a multivariate real function $f$ at a point $\bar{c}$ doesn't imply the continuity of $f$ at $\bar{c}$.
	Thus, we need a suitable generalisation for the partial derivative which could characterise continuity.
	And directional derivative is such an attempt.}

\begin{definition}[Directional Derivative]
	Let \(S \subset \mathbb{R}^n\) and \(f : S \to \mathbb{R}^m\).
	Let $\bar{c}$ be an interior points of $S$ and \( \bar{u} \in \mathbb{R}^n \), then there exists an open ball $B(\bar{c},r)$ in $S$.
	Also for some $\delta > 0$ the line segment \( \alpha : [0,\delta] \to S \) given by \( \alpha(t) = \bar{c}+t\bar{u} \) lie in $B(\bar{c},r)$.
	Then the \textbf{directional derivative} of $f$ at an interior point $\bar{c}$ in the direction $\bar{u}$ is given by
	\[ f'(\bar{c},\bar{u}) = \lim_{h \to 0} \frac{f(\bar{c}+h\bar{u}) - f(\bar{c})}{h} \]
\end{definition}

\begin{remark}
	The direction derivative of $f$ at an interior point $\bar{c}$ in the direction $\bar{u}$ exists only if the above limit exists.
\end{remark}

\begin{remark}Example, \cite[Exercise 12.2a]{apostol}

Suppose \(\bar{x},\bar{a},\bar{c},\bar{u} \in \mathbb{R}^n\).
Let \(f : \mathbb{R}^n \to \mathbb{R}\) such that \(f(\bar{x}) = \bar{a}\cdot\bar{x}\).
Then \[ f'(\bar{c},\bar{u}) = \lim_{h \to 0} \frac{\bar{a}\cdot(\bar{c}+h\bar{u}) - \bar{a} \cdot \bar{c}}{h} = \bar{a} \cdot \bar{u}\]
\end{remark}

\begin{remark}[Properties] Let \(f : S \to \mathbb{R}^m \), where \( S \subset \mathbb{R}^n\)
\begin{enumerate}
	\item \( f'(\bar{c},\bar{0}) = \bar{0} \)\\
	\textsl{Note : The zero vectors belongs to $\mathbb{R}^n, \mathbb{R}^m$ respectively.}
	\item \( f'(\bar{c},\bar{u}_k) = \frac{\partial f}{\partial u_k}(\bar{c}) = D_k f(\bar{c}) \), the $k^{th}$ partial derivative of $f$.
	\item Let \( f = (f_1, f_2, \dots, f_m), \text{ such that } f(\bar{c}) = \left(f_1(\bar{c}),f_2(\bar{c}),\dots,f_m(\bar{c})\right) \).
	Then, 
		\[ \exists f'(\bar{c},\bar{u}) \iff \forall k, \exists f_k'(\bar{c},\bar{u}) \text{ and } f'(\bar{c},\bar{u}) = \left(f_1'(\bar{c},\bar{u}),f_2'(\bar{c},\bar{u}),\dots,f_m'(\bar{c},\bar{u})\right) \]
	ie, Directional derivative of $f$ exists iff directional derivative of each component function $f_k$ exists.
	And the components of the directional derivatives of $f$ are the directional derivaties of the components of $f$.

	Thus \( D_k f(\bar{c}) = \left(D_k f_1(\bar{c}),D_k f_2(\bar{c}),\dots,D_k f_m(\bar{c}) \right) \) holds.
	\item Let \( F(t) = f(\bar{c}+t\bar{u}) \), then \( F'(0) = f'(\bar{c},\bar{u}) \) and \( F'(t) = f'(\bar{c}+t\bar{u},\bar{u}) \)
	\item Let \( f(\bar{c}) = \bar{c} \cdot \bar{c} = \|\bar{c}\|^2 \), and \(F(t) = f(\bar{c}+t\bar{u}) \), then \( F'(t) = 2\bar{c} \cdot \bar{u}+2t\|\bar{u}\|^2 \) and \( F'(0) = f'(\bar{c},\bar{u}) = 2\bar{c} \cdot \bar{u} \)
	\item Let \(f\) be linear, then \( f'(\bar{c},\bar{u}) = f(\bar{u}) \)
	\item Existence of all partial derivatives doesn't imply existence of all directional derivatives.
	\[ f(x,y) = \begin{cases} x+y \qquad \text{ if } x = 0 \text{ or } y = 0 \\ 1 \qquad \qquad \text{otherwise} \end{cases} \]
	For above \(f\), directional derivatives exists only along the co\nobreakdash-ordinates (ie, partial derivatives).
	\item Existence of all directional derivatives doesn't imply continuity.
	\[ f(x,y) = \begin{cases} xy^2(x^2+y^4) \qquad x \ne 0 \\ 0 \hspace{2.5cm} x = 0 \end{cases} \]
	Above \(f\) is discontinuous at \((0,0)\), however all directional derivatives exists and has finite value.
	\end{enumerate}
\end{remark}

\subsection{Total Derivative}
We may define a total derivative \( T_c(h) = hf'(c) \) in the case of real-functions of single variable as follows :-

\[ \text{Let }E_c(h) = \begin{cases} \frac{f(c+h)-f(c)}{h} - f'(c),\qquad h \ne 0 \\ 0,\hspace{3.4cm} h = 0 \end{cases} \]
Then, \( f(c+h) = f(c) + hf'(c) + hE_c(h) \) and as \( h \to 0 \), \( E_c(h) \to 0\).
Also \( T_c(h) = f'(c)h \) is a linear function of $h$.
ie, \( T_c(ah_1+bh_2) = aT_c(h_1)+bT_c(h_2) \).
Now, we will define a total derivative of multivariate function that has these two properties.

\begin{definition}[Total Derivative]
	The function \( f: \mathbb{R}^n \to \mathbb{R}^m \) is \textbf{differentiable} at $\bar{c}$
	if there exists a \texttt{linear} function \( T_{\bar{c}} : \mathbb{R}^n \to \mathbb{R}^m \)
	such that \( f(\bar{c}+\bar{v}) = f(\bar{c}) + T_{\bar{c}}(\bar{v}) + \|\bar{v}\| E_{\bar{c}}(\bar{v}) \)
	where \( E_{\bar{c}}(\bar{v}) \to \bar{0} \) as \(\bar{v} \to \bar{0}\).
\end{definition}

\begin{remark}
	The linear function $T_{\bar{c}}$ is the total derivative of $f$ at $\bar{c}$, \( T_{\bar{c}}(\bar{0}) = \bar{0} \) and the condition above gives the First Order Taylor's Formula for \( f(\bar{c}+\bar{v})-f(\bar{c}) \).
\end{remark}

\begin{remark}[Properties] Let \( f : \mathbb{R}^n \to \mathbb{R}^m \) and \( f'(\bar{c})(\bar{v}) = T_{\bar{c}}(\bar{v}) \) be the total derivative of $f$ at $\bar{c}$ evaluated at $\bar{v}$.
Then, 
\begin{enumerate}
	\item \( f'(\bar{c})(\bar{v}) = f'(\bar{c},\bar{u}) \)
	\item If $f$ is differentiable at $\bar{c}$, then $f$ is continuous at $\bar{c}$.
	\item \( f'(\bar{c})(\bar{v}) = v_1 D_1 f(\bar{c}) + v_2 D_2 f(\bar{c}) + \dotsb + v_n D_n f(\bar{c}) \)
	\end{enumerate}
\end{remark}

\begin{note}
The above $f'$ is a function from $\mathbb{R}^n$ to the set of all linear functions \( \mathscr{L} = \{ h : \mathbb{R}^n \to \mathbb{R}^m\} \).
$f'(\bar{c})$ is a linear function (in fact, total derivative $T_{\bar{c}}$) which maps $\bar{v}$ into the directional derivatives of $f$ at $\bar{c}$ in the direction $\bar{v}$.
This notation generalises $f'$ for univariate $f$ as well.(put $n=m=1$)

In this subject, we use the following notations,
\begin{description}
	\item[$D_kf(\bar{c})$] partial derivative
	\item[$f'(\bar{c},\bar{v})$] directional derivative
	\item[$f'(\bar{c})(\bar{v})$] total derivative
	\item[$\nabla{}f(\bar{c})$] gradient vector
\end{description}
\end{note}

\begin{theorem}
If $f$ is differentiable at $\bar{c}$ with total derivative $T_{\bar{c}}$, then for every $\bar{u} \in \mathbb{R}^n$, $T_{\bar{c}}(\bar{u}) = f'(\bar{c},\bar{u})$.
( ie, \( f'(\bar{c})(\bar{v}) = f'(\bar{c},\bar{v}) \) )
\end{theorem}
\begin{proof}
For \( \bar{v} = \bar{0}$, we have $T_{\bar{c}}(\bar{0}) = 0 = f'(\bar{c},\bar{0}) \).

Suppose \( \bar{v} \ne \bar{0} \), then put \( \bar{v} = h\bar{u} \).
Since $f$ is differentiable at $\bar{c}$, $f$ has total derivative at $\bar{c}$.
That is, there exists a linear function $T_{\bar{c}}$ such that \( f(\bar{c}+h\bar{u}) = f(\bar{c}) + T_{\bar{c}}(h\bar{u}) + \|h\bar{u}\|E_{\bar{c}} (h\bar{u}) \) where $E_{\bar{c}}(h\bar{u}) \to \bar{0}$ as $h\bar{u} \to \bar{0}$.
\begin{align*}
	\implies  & f(\bar{c}+h\bar{u}) = f(\bar{c}) + hT_{\bar{c}}(\bar{u}) + |h|\|\bar{u}\|E_{\bar{c}} (h\bar{u}),\ E_{\bar{c}}(h\bar{u}) \to \bar{0} \text{ as } h\bar{u} \to \bar{0} \\
	\implies  & \frac{f(\bar{c}+h\bar{u}) - f(\bar{c})}{h} = T_{\bar{c}}(\bar{u}) + \frac{|h|\|\bar{u}\|E_{\bar{c}}(h\bar{u})}{h},\  E_{\bar{c}}(h\bar{u}) \to \bar{0} \text{ as } h \to 0 \\
	\implies  & \lim_{h \to 0} \frac{f(\bar{c}+h\bar{u}) - f(\bar{c})}{h} = T_{\bar{c}}(\bar{u}) + \lim_{h \to 0} \frac{|h|\|\bar{u}\|E_{\bar{c}}(h\bar{u})}{h} \\
	\implies & f'(\bar{c},\bar{u}) = T_{\bar{c}}(\bar{u})
\end{align*}
\end{proof}

\begin{note}
$T_{\bar{c}}$ is linear, however $E_{\bar{c}}$ is not linear.
Thus $E_{\bar{c}}(h\bar{u}) \ne h E_{\bar{c}} (\bar{u})$.

As $h \to 0$, $h\bar{u} \to \bar{0}$ and \( E_{\bar{c}}(h\bar{u}) \to \bar{0}$.
Since the order of the function \( E_{\bar{c}}(h\bar{u}) \) is much smaller than that of $h$, the limit on the right converges to 0.
\end{note}

\begin{theorem}
If $f$ is differentiable at \( \bar{c} \), then $f$ is continuous at \( \bar{c} \).
\end{theorem}
\begin{proof}
Let $\bar{v} \ne 0$, then
\begin{align*}
	\bar{v} = v_1 \bar{u}_1 & + v_2 \bar{u}_2 + \dotsb + v_n \bar{u}_n,\\
	\bar{v} \to \bar{0} \implies & \forall j,\ v_j \to 0 \\
	T \text{ is linear }\implies & T_{\bar{c}} (\bar{v}) = v_1 T_{\bar{c}}(\bar{u}_1) + v_2 T_{\bar{c}}(\bar{u}_2) + \dotsb + v_n T_{\bar{c}}(\bar{u}_n)\\
	\text{Thus, } & T_{\bar{c}}(\bar{v}) \to \bar{0} \text{ as } \bar{v} \to 0
\end{align*}
Since $f$ differentiable at \( \bar{c} \), there exists linear function $T_{\bar{c}}$ such that
\begin{align*}
	f(\bar{c}+\bar{v}) & =  f(\bar{c}) + T_{\bar{c}}(\bar{v}) + \|v\|E_{\bar{c}}(\bar{v}) \\
	\implies & \lim_{\bar{v} \to \bar{0}} f(\bar{c}+\bar{v}) = f(\bar{c}) + \lim_{\bar{v} \to \bar{0}} T_{\bar{c}}(\bar{v}) + \lim_{\bar{v} \to \bar{0}} \|v\|E_{\bar{c}}(\bar{v})\\
	\implies & \lim_{\bar{v} \to \bar{0}} f(\bar{c}+\bar{v}) = f(\bar{c})
\end{align*}
\end{proof}

\begin{theorem}
Let $S \subset \mathbb{R}^n$ and $f : S \to \mathbb{R}^m$ be differentiable at an interior point $\bar{c}$ of $S$, where $S \subseteq \mathbb{R}^n$.
If $\bar{v} = v_1\bar{u}_1+v_2\bar{u}_2 + \dotsb + v_n\bar{u}_n$, then
\[ f'(\bar{c})(\bar{v}) = \sum_{k=1}^n v_k D_k f(\bar{c}) \]
In particular, if $f$ is real-valued $(m = 1)$ we have, $f'(\bar{c})(\bar{v}) = \nabla{}f(\bar{c}).\bar{v}$
\end{theorem}
\begin{proof}
Suppose $f : S \to \mathbb{R}^m$ is differentiable at $\bar{c}$, then there exists a linear function $f'(\bar{c}) : S \to \mathbb{R}^m$ such that $f(\bar{c}+\bar{v}) = f(\bar{c}) + f'(\bar{c})(\bar{v}) + \|\bar{v}\| E_{\bar{c}}(\bar{c})$ where $E_{\bar{c}} \to \bar{0}$ as $\bar{v} \to \bar{0}$.
\begin{align*}
	f'(\bar{c})(\bar{v}) & = f'(\bar{c})\left(\sum_{k=1}^n v_k \bar{u}_k\right)\\
	& = \sum_{k=1}^n v_k f'(\bar{c})(\bar{u}_k), \text{ since $f'(\bar{c})$ is linear}\\
	& = \sum_{k=1}^n v_k D_k f(\bar{c}), \text{ since $f'(\bar{c})(\bar{u}_k) = f'(\bar{c},\bar{u}_k) = D_k f(\bar{c})$}\\
	\intertext{Let $m = 1$, then $f : S \to \mathbb{R}$ }
	f'(\bar{c})(\bar{v}) & = \sum_{k=1}^n v_k D_k f(\bar{c}) = \nabla{}f(\bar{c}).\bar{v}\\
	& \text{ since } \nabla{}f(\bar{c}) = \left( D_1f(\bar{c}),\ D_2f(\bar{c}),\ \dots ,\ D_nf(\bar{c}) \right)
\end{align*}
\end{proof}

\begin{remark}
Let $f : S \to \mathbb{R}$, then $f(\bar{c} +\bar{v}) = f (\bar{c}) + \nabla{}f(\bar{c}).\bar{v} + o(\|\bar{v}\|)$ as $\bar{v} \to \bar{0}$.
\end{remark}

\begin{remark}[Complex-valued Functions]
\end{remark}

\subsection{Matrix of Linear Function}
Let $T : \mathbb{R}^n \to \mathbb{R}^m$ be a linear function.
Let $\{\bar{u}_1,\ \bar{u}_2,\ \dots,\ \bar{u}_n\}$ be standard basis for $\mathbb{R}^n$ and  $\{\bar{e}_1,\ \bar{e}_2,\ \dots,\ \bar{e}_m\}$ be standard basis for $\mathbb{R}^m$.
Let $\bar{v} \in \mathbb{R}^n$, then $\bar{v} = \sum_{k=1}^n v_k\bar{u}_k$ and $T(\bar{v}) = \sum_{k=1}^n v_k T(\bar{u}_k)$ and
\begin{commentary}
\begin{align*}
	T(\bar{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} T(\bar{u}_1) \\ T(\bar{u}_2) \\ \dots \\ T(\bar{u}_n) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}\bar{e}_1+t_{21}\bar{e}_2+\dotsb+t_{m1}\bar{e}_m \\ t_{12}\bar{e}_1+t_{22}\bar{e}_2+\dotsb+t_{m2}\bar{e}_m \\ \dots \\ t_{1n}\bar{e}_1+t_{2n}\bar{e}_2+\dotsb+t_{mn}\bar{e}_m \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \dots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \dots & t_{m1} \\ t_{12} & t_{22} & \dots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \dots & t_{mn} \end{bmatrix} \begin{bmatrix} \bar{e}_1 \\ \bar{e}_2 \\ \dots \\ \bar{e}_m \end{bmatrix} \\
	\intertext{ We may take the transpose,}
	T(\bar{v}) & = \begin{bmatrix} \bar{e}_1 & \bar{e}_2 & \dots & \bar{e}_m \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \dots & t_{1n} \\ t_{21} & t_{22} & \dots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \dots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \dots \\ v_n  \end{bmatrix} 
\end{align*}
\end{commentary}
\[ T(\bar{v}) = T \left( \sum_{k=1}^n v_k \bar{u}_k \right) = \sum_{k=1}^n v_k T(\bar{u}_k) = \sum_{k=1}^n v_n \sum_{j=1}^m t_{kj}\bar{e}_j \]
Thus matrix of $T$ is given by, $m(T) = (t_{ik})$ where $T(\bar{u}_k) = \sum_{k=1}^n t_{ik}\bar{e}_i$.
\begin{commentary}
\begin{remark}[Example]
Let $T : \mathbb{R}^3 \to \mathbb{R}^2$ defined by $T(x,y,z)=(2x+y,y-z)$.
\begin{align*}
	T(1,2,3) = & T((1,0,0) + 2(0,1,0) + 3(0,0,1)) \\
	= & T(\bar{u}_1+2\bar{u}_2+3\bar{u}_3) \\
	= & T(\bar{u}_1) + 2T(\bar{u}_2) + 3T(\bar{u}_3) \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} T(\bar{u}_1) \\ T(\bar{u}_2) \\ T(\bar{u}_3) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} (2,0) \\ (1,1) \\ (0,-1) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2(1,0) \\ (1,0)+(0,1) \\ -1(0,1) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2\bar{e}_1 \\ \bar{e}_1+\bar{e}_2 \\ -\bar{e}_2 \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 1 & 1 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} \bar{e}_1 \\ \bar{e}_2 \end{bmatrix} \\
	= & 4\bar{e}_1-\bar{e}_2 = 4(1,0) - 1(0,1) = (4,-1)
\end{align*}
\[ \text{ In the above case, }m(T) = \begin{bmatrix} 2 & 0 \\ 1 & 1\\ 0 & -1 \end{bmatrix}\]
Using the matrix of linear function $m(T)$, we can compute the image of any point in $R^3$ by matrix multiplication.
\end{remark}
\end{commentary}

\subsubsection{Matrix of the composition of two linear functions}
Let $T : \mathbb{R}^n \to \mathbb{R}^m$ and $S : \mathbb{R}^m \to \mathbb{R}^p$ be two linear functions with domain of $S$ containing the range of $T$ (so that $S \circ T$ is well defined).
Then $S \circ T : \mathbb{R}^n \to \mathbb{R}^p$ is defined by
\[ S \circ T(\bar{x}) = S(T(\bar{x})),\ \forall \bar{x} \in \mathbb{R}^n\]
Since $S,T$ are linear, $S \circ T$ is also linear.
\begin{align*}
	S \circ T(a \bar{x} + b \bar{y}) = & S(T(a \bar{x} + b\bar{y})) = S(a T(\bar{x}) + b T(\bar{y})) = a S(T(\bar{x}))) + b S(T(\bar{y})) \\
	= & a S \circ T(\bar{x}) + b S \circ T(\bar{y}),\ \forall a,b \in \mathbb{R},\ \forall \bar{x},\bar{y} \in \mathbb{R}^n
\end{align*}
Let $\{\bar{u}_1,\bar{u}_2,\dots,\bar{u}_n\}$ be the standards basis for $\mathbb{R}^n$, $\{\bar{e}_1,\bar{e}_2,\dots,\bar{e}_m\}$ be the standards basis for $\mathbb{R}^m$ and $\{\bar{w}_1,\bar{w}_2,\dots,\bar{w}_p\}$ be the standards basis for $\mathbb{R}^p$.
Let $\bar{v} \in \mathbb{R}^n$, then $\bar(v) = \sum_{i=1}^n v_i\bar{u}_i$, and $S \circ T(\bar{v})=\sum_{i=1}^n v_n S \circ T(\bar{u}_i)$
\begin{commentary}
\begin{align*}
	S \circ T(\bar{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} S \circ T(\bar{u}_1) \\ S \circ T(\bar{u}_2) \\ \dots \\ S \circ T(\bar{u}_n) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} S(t_{11}\bar{e}_1 + \dotsb + t_{m1}\bar{e}_m) \\ S(t_{12}\bar{e}_1 + \dotsb + t_{m2}\bar{e}_m) \\ \dots \\ S(t_{1n}\bar{e}_1 + \dotsb + t_{mn}\bar{e}_m) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}S(\bar{e}_1) + \dotsb + t_{m1}S(\bar{e}_m) \\ t_{12}S(\bar{e}_1) + \dotsb + t_{m2}S(\bar{e}_m) \\ \dots \\ t_{n1}S(\bar{e}_1) + \dotsb + t_{mn}S(\bar{e}_m) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \dots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \dots & t_{m1} \\ t_{12} & t_{22} & \dots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \dots & t_{mn} \end{bmatrix} \begin{bmatrix} S(\bar{e}_1) \\ S(\bar{e}_2) \\ \dots \\ S(\bar{e}_m) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \dots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \dots & t_{m1} \\ t_{12} & t_{22} & \dots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \dots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11}\bar{w}_1+s_{12}\bar{w}_2+\dotsb+s_{1p}\bar{w}_p \\ s_{12}\bar{w}_1+s_{22}\bar{w}_2+\dotsb+s_{p2}\bar{w}_p \\ \dots \\ s_{1m}\bar{w}_1+ s_{2m}\bar{w}_2+\dotsb+s_{pm}\bar{w}_p \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \dots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \dots & t_{m1} \\ t_{12} & t_{22} & \dots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \dots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11} & s_{21} & \dots & s_{p1} \\ s_{12} & s_{22} & \dots & s_{p2} \\ \vdots & \vdots & \ddots & \vdots \\ s_{1m} & s_{2m} & \dots & s_{pm} \end{bmatrix} \begin{bmatrix} \bar{w}_1 \\ \bar{w}_2 \\ \vdots \\ \bar{w}_p \end{bmatrix}\\
	\intertext{We may take transpose,}
	S \circ T(\bar{v}) & = \begin{bmatrix} \bar{w}_1 & \bar{w}_2 & \vdots & \bar{w}_p \end{bmatrix} \begin{bmatrix} s_{11} & s_{12} & \dots & s_{1m} \\ s_{21} & s_{22} & \dots & s_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ s_{p1} & s_{p2} & \dots & s_{pm} \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \dots & t_{1n} \\ t_{21} & t_{22} & \dots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \dots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \dots \\ v_n  \end{bmatrix} 
\end{align*}
Remember : Given $T : \mathbb{R}^n \to \mathbb{R}^m$, then we may take $m(T)$ either as $m \times n$ matrix or $n \times m$ matrix.
Since, we chose $m \times n$, $m(S \circ T) = m(S)m(T)$.
Otherwise, $m(S \circ T) = m(T)m(S)$.
This may change for different authors.
\end{commentary}

Suppose $m(S) = (s_{ij})$ and $m(T) = (t_{ij})$ respectively.
Then
\[ S(e_k) = \sum_{i=1}^p s_{ik} \bar{w}_i,\ k=1,2,\dots,m \text{ and }\]
\[ T(u_j) = \sum_{k=1}^m t_{kj} \bar{e}_k,\ j=1,2,\dots,n \]
\begin{align*}
	(S \circ T)(\bar{u}_j) = & S(T(\bar{u}_j)) = S\left(\sum_{k=1}^m t_{kj}\bar{e}_k\right) = \sum_{k=1}^m t_{kj}S(\bar{e}_k) \\ 
	= & \sum_{k=1}^m t_{kj}\left( \sum_{i=1}^p s_{ik} \bar{w}_i\right) = \sum_{i=1}^p \left(\sum_{k=1}^m s_{ik}t_{kj}\right)\bar{w}_i
\end{align*}
Therefore, $m(S \circ T) = \sum_{k=1}^m s_{ik}t_{kj} = (s_{ik})(t_{kj}) =  m(S)m(T)$.

\subsection{The Jacobian Matrix}
Let $\bar{u}_1, \bar{u}_2, \dots, \bar{u}_n$ be the unit co-ordinate vectors in $\mathbb{R}^n$ and $\bar{e}_1, \bar{e}_2, \dots, \bar{e}_m$ be the unit co-ordinate vectors in $\mathbb{R}^m$.
Let function $f : \mathbb{R}^n \to \mathbb{R}^m$ be differentiable at $\bar{c} \in \mathbb{R}^n$.
Then there exists a linear function $T = f'(\bar{c}) : \mathbb{R}^n \to \mathbb{R}^m$ such that $f(\bar{c}+\bar{v}) = f(\bar{c})+f'(\bar{c})(\bar{v}) + \|\bar{v}\|+E_{\bar{c}}(\bar{v})$.
We have, $T(\bar{u}_k) = f'(\bar{c})(\bar{u}_k) = f'(\bar{c},\bar{u}_k) = D_kf(\bar{c}) = D_k \sum_{i=1}^m f_i(\bar{c})\bar{e}_i$.

Clearly, the matrix of total derivative $T$, $m(T) = (t_{ik}) = (D_k f_i(\bar{c}))$.
This matrix is called Jacobian matrix of $f$ at $\bar{c}$ and is denoted by $Df(\bar{c})$.
\[ Df(\bar{c}) = \begin{bmatrix} D_1 f_1(\bar{c}) & D_2 f_1(\bar{c}) & \dots & D_n f_1(\bar{c}) \\ D_1 f_2(\bar{c}) & D_2 f_2(\bar{c}) & \dots & D_n f_2(\bar{c}) \\ \vdots & \vdots & \ddots & \vdots \\ D_1 f_m(\bar{c}) & D_2 f_m(\bar{c}) & \dots & D_n f_m(\bar{c}) \end{bmatrix} \]

\subsubsection{Properties of Jacobian matrix}
\begin{enumerate}
	\item $k$th row of $Df(\bar{c})$ is gradient vector of $f_k$
		\[\nabla f_k(\bar{c}) = (D_1f_k(\bar{c}),D_2f_k(\bar{c}),\dots,D_nf_k(\bar{c}))\]
	\item When $m = 1$, $Df(\bar{c}) = \nabla f(\bar{c})$.
	\item $f'(\bar{c})(\bar{v}) = \sum\limits_{k=1}^m \left(\nabla f_k(\bar{c}) \cdot \bar{v}\right) \bar{e}_k$
	\item $\|f'(\bar{c})(\bar{v})\| \le M\|v\|$ where $M = \sum\limits_{k=1}^m \|\nabla f_k(\bar{c})\| $, by property (3)
	\item $f'(\bar{c})(\bar{v}) \to \bar{0}$ as $\bar{v} \to \bar{0}$, by property (4)
\end{enumerate}

\subsubsection{Chain Rule}
\begin{commentary}
Chain Rule for real function : $\frac{d F \circ G}{dx}(x) = \frac{d}{dy}F(y) \frac{d}{dx}G(x) = F'(y)\ G'(x)$

For example : $\frac{d}{dx} (ax+3)^3 = \frac{d}{dy}y^3 \frac{d}{dx} \left(ax+3\right) = 3ay^2 = 3a(ax+3)^2$
\end{commentary}
\begin{theorem}
Let $g$ be differentiable at $\bar{a}$, with total derivative $g'(\bar{a})$ and $\bar{b} = g(\bar{a})$.
Let $f$ is differentiable at $\bar{b}$, with total derivative $f'(\bar{b})$.
Then $h = f \circ g$ is differentiable at $\bar{a}$ with total derivative $h'(\bar{a}) = f'(\bar{b}) \circ g'(\bar{a})$.
\begin{commentary}
Try to read $h'(\bar{a}) = H$, $f'(\bar{b}) = F$, $g'(\bar{a}) = G$, then $H = F \circ G \implies H(x) = F(G(x))$
In other words, $h'(\bar{a})(\bar{v}) = f'(\bar{b}) \circ g'(\bar{a})\ (\bar{v}) = f'(\bar{b})(g'(\bar{a})(\bar{v}))$.
\end{commentary}
\end{theorem}
\begin{proof}
Given $\epsilon > 0$, let $y \in \mathbb{R}^p$ such that $\|y\| < \epsilon$.
Let $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^p \to \mathbb{R}^n$, then $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.

We have, $h(\bar{a}+\bar{y})-h(\bar{a}) = f(g(\bar{a}+\bar{y})) - f(g(\bar{a})) = f(\bar{b}+\bar{v}) - f(\bar{b})$ where $\bar{b} = g(\bar{a})$, and  $\bar{v} = g(\bar{a}+\bar{y})-g(\bar{a})$.

Since $g$ is differentiable at $\bar{a}$, $g$ satisfies first-order Taylor's formula.
\begin{align*}
	g(\bar{a}+\bar{y}) & =  g(\bar{a}) + g'(\bar{a})(\bar{y}) + \|\bar{y}\| E_{\bar{a}}(\bar{y}) \text{ where } E_{\bar{a}} \to \bar{0} \text{ as } \bar{y} \to \bar{0} \\
	\implies & \bar{v} = g(\bar{a}+\bar{y})-g(\bar{a}) = g'(\bar{a})(\bar{y}) + \|\bar{y}\| E_{\bar{a}}(\bar{y})
\end{align*}
Clearly, as $\bar{y} \to \bar{0} \implies \bar{v} \to g'(\bar{a})(\bar{0}) = \bar{0}$.
Again,  we have $f$ is differentiable at $\bar{b}$, thus $f$ satisfies first-order Taylor's formula.
\[ f(\bar{b}+\bar{v}) = f(\bar{b}) + f'(\bar{b})(\bar{v}) + \|\bar{v}\| E_{\bar{b}}(\bar{v}) \text{ where } E_{\bar{b}} \to \bar{0} \text{ as } \bar{v} \to \bar{0} \]
\begin{align*}
	\implies f(\bar{b}+\bar{v}) - f(\bar{b}) & = f'(\bar{b})(\bar{v}) + \|\bar{v}\| E_{\bar{b}}(\bar{v}) \\
	& = f'(\bar{b})\left( g'(\bar{a})(\bar{y}) + \|\bar{y}\|E_{\bar{a}}(\bar{y}) \right) + \|\bar{v}\| E_{\bar{b}}(\bar{v}) \\
	& = f'(\bar{b})(g'(\bar{a})(\bar{y})) + \|\bar{y}\| E(\bar{y}) \\
	& \text{ where } E(\bar{y}) = f'(\bar{b})(E_{\bar{a}}(\bar{y})) + \frac{\|\bar{v}\|}{\|\bar{y}\|} E_{\bar{b}}(\bar{v}),\ \bar{y} \ne \bar{0}
\end{align*}
\[ \implies h(\bar{a}+\bar{y})-h(\bar{a}) = f(\bar{b}+\bar{v}) - f(\bar{b}) = f'(\bar{b})(g'(\bar{a})(\bar{y})) + \|\bar{y}\|E(\bar{y}) \]

Since $f'(\bar{b})$ and $g'(\bar{a})$ are linear, their composition is also linear.
Therefore, $h$ is differentiable at $\bar{a}$ with a linear, total derivative $h'(\bar{a}) = f'(\bar{b}) \circ g'(\bar{a})$ as it satisfies first-order Taylor's formula if $E_{\bar{y}} \to \bar{0}$ as $\bar{y} \to \bar{0}$.

We have, $\|\bar{v}\| \le \|g'(\bar{a})(\bar{y})\| + \|\bar{y}\|\ \|E_{\bar{a}}(\bar{y})\| \le M\|y\| + \|E_{\bar{a}}(\bar{y})\|\ \|\bar{y}\|$.
\[ \implies \frac{\|\bar{v}\|}{\|\bar{y}\|} \le M + \|E_{\bar{a}}(\bar{y})\| \]

Thus, $\bar{v} \to \bar{0}$ as $\bar{y} \to \bar{0}$.
Then $f'(\bar{b})(\bar{v}) \to f'(\bar{b})(\bar{0}) = \bar{0}$.
And $E_{\bar{a}}(\bar{y}) \to \bar{0}$.
Therefore, $E(\bar{y}) \to \bar{0} + M\bar{0} = \bar{0}$ as $\bar{y} \to \bar{0}$.
\end{proof}

\subsubsection{Matrix form of the chain rule}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$, $g : \mathbb{R}^p \to \mathbb{R}^n$.
And $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.
Suppose $g$ is differentiable at $\bar{a} \in \mathbb{R}^p$ and $f$ is differentiable at $g(\bar{a}) = \bar{b} \in \mathbb{R}^n$.
Then $h$ is differentiable at $\bar{a}$ and the Jacobian matrix of $h$ is given by the chain rule,
\[ Dh(\bar{a}) = Df(\bar{b})Dg(\bar{a}) \text{ where } h = f \circ g,\ \bar{b} = g(\bar{a})\]
In other words,
\[ D_jh_i(\bar{a}) = \sum_{k=1}^n D_k f_i(\bar{b}) D_j g_k(\bar{a}),\ i=1,2,\dots,m,\ j=1,2,\dots,p \]

For $m=1$, $D_j h(\bar{a}) = \sum\limits_{k=1}^n D_kf(\bar{b}) D_jg_k(\bar{a})$

For $m=1$ and $p=1$, $h'(\bar{a}) = \sum\limits_{k=1}^n Df(\bar{b}) g_k'(\bar{a}) = \nabla f(\bar{b}) \cdot Dg(\bar{a})$

\begin{theorem}
Let $f$ and $D_2f$ be continuous functions on a rectangle $[a,b] \times [c,d]$.
Let $p$ and $q$ be differentiable on $[c,d]$, where $p(y) \in [a,b]$ and $q(y) \in [c,d]$ for each $y \in [c,d]$.
Define $F$ by the equation,
\[ F(y) = \int_{p(y)}^{q(y)} f(x,y) dx,\ y \in [c,d] \]
Then $F'(y)$ exists for each $y \in (c,d)$ and is given by,
\[ F'(y) = \int_{p(y)}^{q(y)} D_2 f(x,y) dx + f((q,y),y)q'(y) - f(p(y),y)p'(y) \]
\end{theorem}
\begin{commentary}
The following two theorems are required for proving the theorem on differentiating an integral.
\end{commentary}
\begin{theorem}
Let $\alpha$ be of bounded variation on $[a,b]$ and assume that $f \in \mathcal{R}(\alpha)$ on $[a,b]$.
\[ \text{ Define } F(x) = \int_a^x f\ d\alpha,\ x \in [a,b] \]
Then $F$ is of bounded variation on $[a,b]$ and $F$ is continuous at $x$ if $\alpha$ is continuous at $x$.
If $\alpha$ is increasing on $[a,b]$, then the derivative $F'(x)$ exists at each $x \in (a,b)$ where $\alpha'(x)$ exists and where $f$ is continuous.
And 
\[ F'(x) = f(x)\alpha'(x) \]
\label{thm:paralimit}
\end{theorem}
\begin{theorem}
Let $Q = \{ (x,y) : a \le x \le b,\ c \le y \le d \}$.
Assume that $\alpha$ is of bounded variation on $[a,b]$ and for each $y \in [c,d]$, assume that the integral
\[ F(y) = \int_a^b f(x,y)\ d\alpha(x) \]
exists.
If the partial derivative $D_2f$ is continuous on $Q$, the derivative $F'(y)$ exists for each $y \in (c,d)$ and is given by
\[ F'(y) = \int_a^b D_2f(x,y)\ d\alpha(x) \]
\label{thm:fixedlimit}
\end{theorem}
\begin{proof}
Let $G(x_1,x_2,x_3) = \int_{x_1}^{x_2} f(t,x_3)\ dt$.
Then we may write $F(y)$ in terms of $G$.
That is, $F(y) = G(p(y),q(y),y)$.\\
\begin{commentary}Step 1 : 1-D Chain Rule \end{commentary}

By 1-dimensional chain rule, we have 
\begin{align*}
	F'(y) & = \frac{dF}{dy} = \frac{\partial G}{\partial p} \frac{dp}{dy} + \frac{\partial G}{\partial q} \frac{dq}{dy} + \frac{\partial G}{\partial y} \\
	& = D_1 G\ p'(y) + D_2 G\ q'(y) + D_3 G
\end{align*}
\begin{commentary}Step 2 : $D_1 G$ \end{commentary}

	Since the variable of differentition is present in the limit of the integral, we use theorem \ref{thm:paralimit} to compute the derivative of the integral.
	We may write,
	\begin{equation}
		G(p(y),q(y),y)  = -\int_{q(y)}^{p(y)} f(t,y)\ dt
	\end{equation}
	We are differentiating (partially) with respect to $p(y)$.
	Thus $q(y)$, $y$ are constants for this differentiation.
	\begin{align*}
		G(x,a,y) & = -H(x) = -\int_a^x f(t,y)\ dt\\
		\implies D_1 G & = -H'(x)= -f(x,y). \\
		\text{Thus, } D_1 G & = -f(p(y),y)
	\end{align*}
\begin{commentary}Step 3 : $D_2 G$ \end{commentary}

	Again, variable of differentiation is present in the limit of the integral.
	Thus, we write,
	\begin{equation}
		G(p(y),q(y),y) = \int_{p(y)}^{q(y)} f(t,y)\ dt
	\end{equation}
	Now we are differentiating (partially) with respect to the the second component of $G$ which is $q(y)$.
	Clearly, $p(y)$ and $y$ are treated as constants.
	\begin{align*}
		G(a,x,y) & = H(x) = \int_a^x f(t,y)\ dt \\
		\implies D_2 G & = H'(x) = f(q(y),y)
	\end{align*}
\begin{commentary}Step 4 : $D_3 G$ \end{commentary}

	Now the variable of integration is not affecting the limits of the integral.
	Also it is given that $D_2 f$ is continuous on $[a,b] \times [c,d]$.
	We write
	\begin{align*}
		G(a,b,x) & = H(x) = \int_a^b f(t,x)\ dt \\
		\implies D_3 G & = H'(x) = \int_a^b D_2 f(t,x)\ dt \\
		\text{Thus, } D_3 G & = \int_{p(y)}^{q(y)} f(t,y)\ dt
	\end{align*}
\end{proof}

\subsubsection{The mean-value theorem for differentiable functions}
\begin{theorem}[Mean-Value]
Let $S$ be an open subset of $\mathbb{R}^n$.
Assume $f : S \to \mathbb{R}^m$ is differentiable at each point of $S$.
Let $\bar{x}$, $\bar{y}$ be two points in $S$ such that $L(\bar{x},\bar{y}) = \{ t\bar{x}+(1-t)\bar{y} : t \in [0,1] \}$ is subset of $S$.
Then for every $\bar{a} \in \mathbb{R}^m$, there exists a point $\bar{z} \in L(\bar{x},\bar{y})$ such that
\[ \bar{a}.\left( f(\bar{y})-f(\bar{x}) \right) = \bar{a}.f'(\bar{z})(\bar{y}-\bar{x}) \]
\end{theorem}
\begin{proof}
Let $\bar{u} = \bar{y}-\bar{x}$.
We have $S$ is open subset and $L(\bar{x},\bar{y}) \subset S$, thus there exists $\delta > 0$ such that $\bar{x}+t\bar{u} \in S, \forall t \in (-\delta,1+\delta)$.
\begin{commentary} In other words, the `Line segment $L(\bar{x},\bar{y})$' is properly contained in $S$, in such a way that extending the Line from $\bar{x}$ to $\bar{y}$ a little bit extra one either sides is still contained in $S$.\end{commentary}

Let $\bar{a} \in \mathbb{R}^m$ and $F : (-\delta,1+\delta) \to \mathbb{R}$ defined by $F(t) = \bar{a}.f(\bar{x}+t\bar{u})$.
Then $F$ is differentiable at each $t \in (-\delta,1+\delta)$ and the derivative $F'(t) = \bar{a}.f'(\bar{x}+t\bar{u},\bar{u})$, the directional derivative of $f(\bar{x}+t\bar{u})$ with respect to $\bar{u}$.

\[ f'(\bar{x}+t\bar{u},\bar{u}) = f'(\bar{x}+t\bar{u})(\bar{u}) \implies F'(t) = \bar{a}.f'(\bar{x}+t\bar{u})(\bar{u}) \]
By 1-dimensional mean-value theorem, we have
\[ \exists \theta \in (0,1) \text{ such that } F(1) - F(0) = F'(\theta) \]
By definition of $F$, $F(1) = \bar{a}.f(\bar{x}+\bar{u}) = \bar{a}.f(\bar{y})$.
And $F(0) =\bar{a}.f(\bar{x})$.
Therefore,
\[ F'(\theta) = F(1) - F(0) = \bar{a}.f(\bar{y}) - \bar{a}.f(\bar{x}) = \bar{a}.(f(\bar{y})-f(\bar{x})) \]
We also have,
\[ F'(\theta) = \bar{a}.f'(\bar{x}+\theta \bar{u})(\bar{u}) = \bar{a}.f'(\bar{z})(\bar{y}-\bar{x}), \text{ where } \bar{z} = \bar{x}+\theta \bar{u} \in L(\bar{x},\bar{y}) \]
\end{proof}

\begin{remark}
Suppose $S$ is convex in $\mathbb{R}^m$.
Then for every pair of points $\bar{x},\bar{y} \in S$, $L(\bar{x},\bar{y}) \subset S$.
Thus Mean-value theorem holds for all $\bar{x},\bar{y} \in S$.
\end{remark}

\section{Multivariate Calculus}
\subsection{A sufficient condition for differentiability}
\begin{theorem}
Suppose one of the partial derivatives $D_1f,D_2f,\dots,D_nf$ exists at $\bar{c}$.
And the remaining $n-1$ partial derivatives exists in some $n$-ball $B(\bar{c})$ and are continuous at $\bar{c}$.
Then $f$ is differentiable at $\bar{c}$.
\end{theorem}
\begin{proof}
\begin{commentary}Step 1 : Real-valued function\end{commentary}

We claim that the function $f : \mathbb{R}^n \to \mathbb{R}^m$ is differentiable at $\bar{c}$ iff each component $f_k$ is differentiable at $\bar{c}$.

Suppose $f$ is differentiable at $\bar{c}$, then there exists a linear, total derivative function $f'(\bar{c})$ satisfying first-order Taylor's formula at $\bar{c}$.

ie, $f(\bar{c}+\bar{v}) = f(\bar{c}) + f'(\bar{c})(\bar{v}) + \|\bar{v}\| E_{\bar{c}}(\bar{v})$ where $E_{\bar{c}}(\bar{v}) \to \bar{0}$ as $\bar{v} \to \bar{0}$.
\begin{align*}
	f(\bar{c}+\bar{v}) & = \left( f_1(\bar{c}+\bar{v}), f_2(\bar{c}+\bar{v}), \dots, f_m(\bar{c}+\bar{v}) \right)\\
	f(\bar{c}) & = \left( f_1(\bar{c}), f_2(\bar{c}), \dots, f_m(\bar{c}) \right) \\
	f'(\bar{c})(\bar{v}) & = \left( f'_1(\bar{v}), f'_2(\bar{v}), \dots, f'_m(\bar{v}) \right) \\
	E_{\bar{c}}(\bar{v}) & = \left( E_1(\bar{v}), E_2(\bar{v}), \dots, E_m(\bar{v}) \right)
\end{align*}
where each component of the error function $E_k(\bar{v}) \to 0$ as $\bar{v} \to \bar{0}$.
Also since $f'(\bar{c})$ is linear, each of its components $f'_k : \mathbb{R}^n \to \mathbb{R}$ are linear.
\begin{align*}
	f(\bar{c}+\bar{v}) = & \left( f_1(\bar{c}+\bar{v}), f_2(\bar{c}+\bar{v}), \dots, f_m(\bar{c}+\bar{v}) \right) \\
	= & \left( f_1(\bar{c}), f_2(\bar{c}), \dots, f_m(\bar{c}) \right) + \left( f'_1(\bar{v}), f'_2(\bar{v}), \dots, f'_m(\bar{v}) \right) \\
	& + \|\bar{v}\|\left(E_1(\bar{v}), E_2(\bar{v}), \dots, E_m(\bar{v}) \right) \text{ where } E_k(\bar{v}) \to 0 \text{ as } \bar{v} \to \bar{0}\\
	= & \left( f_1(\bar{c}), f_2(\bar{c}), \dots, f_m(\bar{c}) \right) + \left( f'_1(\bar{v}), f'_2(\bar{v}), \dots, f'_m(\bar{v}) \right) \\
	& + \left(\|\bar{v}\|E_1(\bar{v}), \|\bar{v}\|E_2(\bar{v}), \dots, \|\bar{v}\|E_m(\bar{v}) \right) \text{ where } E_k(\bar{v}) \to 0 \text{ as } \bar{v} \to \bar{0}\\
	= & \left( f_1(\bar{c}) + f'_1(\bar{v}) + \|\bar{v}\|E_1(\bar{v}), \dots, f_m(\bar{c}) + f'_m(\bar{v}) + \|\bar{v}\|E_m(\bar{v}) \right) 
\end{align*}

Thus first-order Taylor's forumula for $f$ at $\bar{c}$ gives first-order Taylor's forumula for each of its components $f_k$.
ie, $f_k(\bar{c}+\bar{v}) = f_k(\bar{c}) + f'_k(\bar{v} + \|\bar{v}\|E_k(\bar{v})$ where $E_k(\bar{v}) \to 0$ as $\bar{v} \to \bar{0}$.
Therefore, $f_k$ are differentiable at $\bar{c}$ for $k = 1,2,\dots, m$.

Suppose each component $f_k$ of $f$ are differentiable at $\bar{c}$.
Then there exists linear, total derivative functions $f'_k$ satisfying first-order Taylor's formula at $\bar{c}$.
ie, $f_k(\bar{c}+\bar{v}) = f_k(\bar{v}) + f'_k(\bar{v}) + \|\bar{v}\|E_k(\bar{v})$ where $E_k(\bar{v}) \to 0$ as $\bar{v} \to \bar{0}$.

Define $E_{\bar{c}}(\bar{v}) = \left( E_1(\bar{v}), E_2(\bar{v}), \dots, E_k(\bar{v}) \right)$.
Then $E_{\bar{c}}(\bar{v}) \to \bar{0}$ as $\bar{v} \to \bar{0}$.
Therefore, there exists a linear, total derivative function $f'(\bar{c}) = \left( f'_1, f'_2, \dots, f'_m \right)$ satisfying first-order Taylor's formula at $\bar{c}$.

Thus, if each (real-valued) component function $f_k$ are differentiable, then $f$ is also differentiable.
Therefore, it is sufficient to prove the theorem for a real-valued function.\\
\begin{commentary}Step 2: Telescopic Sum\end{commentary}

Assume (without loss of generality) that $D_1f$ exists at $\bar{c}$ and $D_2f,D_3f,\dots,D_nf$ exist and continuous in some $n$-ball $B(\bar{c})$.
\begin{commentary} Suppose $D_rf$ exists at $\bar{c}$ and all partial derivatives execept $D_rf$ are continuous.
	Then $v_0 = \bar{0}$, $v_1 = y_r\bar{u}_r$, $v_2 = y_r\bar{u}_r + y_1\bar{u}_1, \dotsc$.
Then the following proof can be applied without any loss of generality.
\end{commentary}

Let $\bar{v} = \lambda\bar{y}$ where $\bar{y} = \frac{\bar{v}}{\|\bar{v}\|}$.
Clearly, $\|\bar{y}\| = 1$ and $\lambda = \| \bar{v} \|$.
Choose $\lambda > 0$ such that $\bar{c}+\bar{v} \in B(\bar{c})$ and all the partial derivatives $D_2f, D_3f, \dots, D_nf$ exists and are continuous in $B(\bar{c})$.

	We have, $\bar{y} = (y_1, y_2, \dots, y_n) = y_1 \bar{u}_1 + y_2 \bar{u}_2 + \dotsb + y_n \bar{u}_n$.

	Define $\bar{v}_0 = \bar{0},\ \bar{v}_1 = y_1\bar{u}_1, \dots, \ \bar{v}_n = y_1 \bar{u}_1 + y_2 \bar{u}_2 + \dotsb + y_n \bar{u}_n$.
\begin{align*}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = & ( f(\bar{c}+\lambda{} \bar{v}_n) - f(\bar{c}+\lambda{} \bar{v}_{n-1}) ) \\
	& + ( f(\bar{c}+\lambda{} \bar{v}_{n-1}) - f(\bar{c}+\lambda{} \bar{v}_{n-2}) ) \\
	& + \dotsb + ( f(\bar{c}+\lambda{} \bar{v}_1) - f(\bar{c}+\lambda{} \bar{v}_0) ) \\
	& = \sum_{k = 1}^n f(\bar{c} + \lambda{} \bar{v}_k) - f(\bar{c} + \lambda{} \bar{v}_{k-1}) \\
	& = \sum_{k = 1}^n f(\bar{c} + \lambda{} \bar{v}_{k-1} + \lambda{} y_k \bar{u}_k) - f(\bar{c} + \lambda{} \bar{v}_{k-1})
\end{align*}
\begin{commentary}Step 3 : Mean-value theorem\end{commentary}

Define $\bar{b}_k = \bar{c}+\lambda{}\bar{v}_{k-1}$.
Then we have
\begin{equation}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = \sum_{k = 1}^n f(\bar{b}_k + \lambda{}y_k\bar{u}_k)-f(\bar{b}_k)
\end{equation}
We know that all partial derivatives exists in $B(\bar{c})$.
%Thus $f$ is continuous in $B(\bar{c})$.
Therefore by 1-dimensional mean-value theorem we have,
\[ f(\bar{b}_k+\lambda{}y_k\bar{u}_k) - f(\bar{b}_k) = \lambda{}y_kD_kf(\bar{a}_k) \text{ where } \bar{a}_k \in L(\bar{b}_k,\bar{b}_k+\lambda{}y_k\bar{u}_k) \]
\begin{equation}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = \lambda{} \sum_{k = 1}^n y_kD_kf(\bar{a}_k) \text{ where } \bar{a}_k \in L(\bar{b}_k,\bar{b}_k+\lambda{}y_k\bar{u}_k)
\end{equation}
\begin{commentary}Step 4 : Continuity of partial derivatives in $B(\bar{c})$\end{commentary}

As $\lambda{} \to 0,\ \bar{v} \to \bar{0}$.
And both $\bar{b}_k,\ \bar{b}_k+\lambda{}y_k\bar{u}_k \to \bar{c}$.
Clearly, $\bar{a}_k$ in the line between $\bar{b}_k$ and $\bar{b}_k+\lambda{} y_k \bar{u}_k$ also converges to $\bar{c}$.

For $k \ge 2$, $D_kf$ are continuous in the $n$-ball $B(\bar{c})$.
Thus $D_kf(\bar{a}_k) \to D_kf(\bar{c})$.
We may write, $D_kf(\bar{a}_k) = D_kf(\bar{c}) + E_k(\lambda)$ where $E_k(\lambda) \to \bar{0}$ as $\lambda{} \to 0$.
Also, since $D_1 f$ exists, $D_1f(\bar{c}+\lambda{} y_1\bar{u}_1) \to D_1f(\bar{c})$ as $\lambda{} \to 0$.
\begin{commentary}
\[ \text{ Remember : } D_1 f(\bar{c}) = \lim_{h \to 0} \frac{f(\bar{c}+h\bar{u}_1) - f(\bar{c})}{h} \]
\end{commentary}
\begin{align*}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = & \lambda{} \sum_{k = 1}^n y_kD_kf(\bar{c}) + \lambda{} \sum_{k = 1}^n y_kE_kf(\lambda{})\\
	= & \nabla f(\bar{c}) \cdot \bar{v} + \|\bar{v}\|E(\lambda) \\
	& \text{ where } E(\lambda{}) = \sum_{k = 1}^n y_k E_k(\lambda{}) \to \bar{0} \text{ as } \bar{v} \to \bar{0}
\end{align*}
That is, we have a linear function which satisfies first-order Taylor's formula at $\bar{c}$.
Therefore, $f$ is differentiable.
\end{proof}

\subsection{Sufficient conditions for the equality of mixed partial derivatives}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$.
Then $D_r f$ and $D_k f$ are two partial derivatives of $f$.
And $D_{r,k} f = D_r(D_k f)$ and $D_{k,r} f = D_k (D_r f)$.
\[ D_{r,k} f = \frac{\partial^2 f}{\partial x_r \partial x_k} = \frac{\partial}{\partial x_r} \frac{\partial f}{\partial x_k} \text{ and } D_{k,r} f = \frac{\partial^2 f}{\partial x_k \partial x_r} = \frac{\partial}{\partial x_k} \frac{\partial f}{\partial x_r}  \]
\begin{commentary}
	There are two sufficient conditions for the equality of these mixed partial derivatives in our scope.
\begin{enumerate*}
	\item differentiability of $D_k f$ or
	\item continuity of $D_{r,k} f$ and $D_{k,r}$
\end{enumerate*}
at $\bar{c}$ where the mixed partial derivatives are to be equal.
\end{commentary}

\subsubsection{Differentiability}
\begin{theorem}
Suppose $D_r f$ and $D_k f$ exists in an $n$-ball about $\bar{c}$ and are both differentiable at $\bar{c}$.
Then $D_{r,k} f = D_{k,r} f$.
\end{theorem}
\begin{proof}
\begin{commentary}Step 1 : Real-valued function\end{commentary}\\
	It is sufficient to prove the theorem for real-valued functions.
	Let $f : \mathbb{R}^n \to \mathbb{R}^m$, then $f(\bar{c}) = \left( f_1(\bar{c}),f_2(\bar{c}),\dots,f_m(\bar{c}) \right)$.
	And
	\[ D_k f(\bar{c}) = \left( D_k f_1(\bar{c}), D_k f_2(\bar{c}), \dots, D_k f_m(\bar{c}) \right) \]
	Thus it is sufficient to prove that $D_{r,k}f_j(\bar{c}) = D_{k,r}f_j(\bar{c}),\ j = 1,2,\dots,m$.
	That is, it is sufficient to prove equality of mixed partial derivatives of a real-valued function $f_j : \mathbb{R}^n \to \mathbb{R}$.
	Also, we will prove it for $n = 2$ and $\bar{c} = (0,0)$.
\begin{commentary}
	Now, we will prove the theorem of a real-valued function $f : \mathbb{R}^n : \mathbb{R}$.\\
\end{commentary}
\begin{commentary}Step 2 : $\nabla(h)$\end{commentary}

Let $f : \mathbb{R}^n \to \mathbb{R}$.
Suppose that the partial derivatives $D_k f$, $D_r f$ exist in the $n$-ball $B(n)$.
And let $h > 0$ such that the rectangle with vertices $(0,0), (0,h), (h,0), (h,h)$ lies in $B(n)$.

\begin{commentary} Suppose $n=3$, $c = (x,y,z)$, and we want to prove equality of $D_{2,3} f$ and $D_{3,2} f$.
Then we will consider the rectangle with vertices $(x,y,z), (x,y,z+h), (x,y+h,z), (x,y+h,z+h)$.
Again, we are taking $n=2$ and $\bar{c} = (0,0)$, only for the ease of notation as the same proof is applicable for any finite natural number, $n$ and any vector $\bar{c} \in \mathbb{R}^n$.\end{commentary}

	\begin{equation}
		\text{Define }\nabla(h) = f(h,h)-f(h,0)-f(0,h)+f(0,0)
	\end{equation}
\begin{commentary}Step 3 : $D_{1,2} f = \frac{\nabla(h)}{h^2} = D_{2,1} f$\end{commentary}
\begin{equation}
	\text{Define }G(x) = f(x,h)-f(x,0)
\end{equation}
Then  we have, $\nabla(h) = G(h)-G(0)$ and $G'(x) = D_1f(x,h) - D_1f(x,0)$.
By 1-dimensional mean value theorem,
\begin{align*}
	G(h)-G(0) = & hG'(x_1)  \text{ where } x_1 \in (0,h) \\
	= & h\left( D_1f(x_1,h)-D_1f(x_1,0)\right)
\end{align*}
We have $D_1 f$ is differentiable at $(0,0)$.
There exists linear, total derivative function $(D_1f)'(0,0)$ where $(D_1f)'(0,0)(x,y) = \nabla D_1 f(0,0) \cdot{} (x,y)$ satisfiying first-order Taylor's formula at $(0,0)$. 
\begin{commentary}
\[ \text{ Remember :} f'(\bar{c})(\bar{v}) = \sum_{k = 1}^n v_k D_k f(\bar{c}) = \nabla f(\bar{c}) \cdot{} \bar{v} \]
\end{commentary}
\begin{align*}
	D_1 f((0,0) + (x_1,h)) = & D_1 f(0,0) + \nabla D_1 f(0,0) \cdot{} (x_1,h) + \|(x_1,h)\| E_1(h) \\
	& \text{ where } E_1(h) \to 0 \text{ as } h \to 0\\
	D_1 f(x_1,h) = & D_1 f(0,0) + x_1 D_{1,1} f(0,0) + h D_{2,1} f(0,0) + \left|\sqrt{x_1^2+h^2}\right| E_1(h)
\end{align*}

	Similarly,
\begin{align*}
	D_1 f((0,0) + (x_1,0)) = & D_1 f(0,0) + \nabla D_1 f(0,0) \cdot{} (x_1,0) + \|(x_1,0)\| E_2(h) \\
	& \text{ where } E_2(h) \to 0 \text{ as } h  \to 0 \\
	D_1 f(x_1,0) = & D_1 f(0,0) + x_1 D_{1,1} f(0,0) + |x_1| E_2(h)
\end{align*}
	Therefore $\nabla(h) = h (D_1 f(x_1,h) - D_1 f(x_1,0)) = h^2 D_{2,1} f(0,0) + E(h)$ where $E(h) = h|\sqrt{x_1^2+h^2}| E_1(h) - h|x_1|E_2(h)$ and $E(h) \to 0$ as $h \to 0$.

	Since $0 < x_1 < h$, we have
	\[ 0 \le E(h) \le h^2\left(\sqrt{2}E_1(h)-E_2(h)\right) \]

Therefore,
	\[ \lim_{h \to 0} \frac{\nabla(h)}{h^2} \le \lim_{h \to 0} \frac{h^2 D_{2,1} f(0,0)}{h^2} + \lim_{h \to 0} \frac{h^2 (\sqrt{2}E_1(h) - E_2(h))}{h^2} = D_{2,1}f(0,0) \]
\begin{equation}
\lim_{h \to 0} \frac{\nabla(h)}{h^2} = D_{2,1} f(0,0)
\end{equation}

\begin{commentary}
	You may skip the following part.
	And conclude with the last two lines.
\end{commentary}

Similarly, define $H(y) = f(h,y)-f(0,y)$.
Then we have, $\nabla(h) = H(h)-H(0)$ and $H'(y) = D_2f(h,y) - D_2f(0,y)$.
By 1-dimensional mean value theorem,
\begin{align*}
	H(h)-H(0) = & hH'(y_1)  \text{ where } y_1 \in (0,h) \\
	= & h \left( D_2f(h,y_1)-D_2f(0,y_1) \right)
\end{align*}
We have $D_2 f$ is differentiable at $(0,0)$.
Thus there exists a linear, total derivative function $(D_2f)'(0,0)$ where $(D_2f)'(0,0)(x,y) = \nabla D_2 f(0,0) \cdot{} (x,y)$ satisfying first-order Taylor's formula at $(0,0)$.
That is,
\begin{align*}
	D_2 f((0,0)+(h,y_1)) = & D_2 f(0,0) + \nabla D_2 f(0,0) \cdot{} (h,y_1) + \|(h,y_1)\| E_3(h) \\
	& \text{ where } E_3(h) \to 0 \text{ as } h \to 0 \\
	D_2 f(h,y_1) = & D_2 f(0,0) + h D_{1,2} f(0,0) + y_1 D_{2,2} f(0,0) + \left|\sqrt{h^2+y_1^2}\right| E_3(h)
\end{align*}

Again,
\begin{align*}
	D_2 f((0,0)+(0,y_1)) = & D_2 f(0,0) + \nabla D_2 f(0,0) \cdot{} (0,y_1) + |y_1| E_4(h) \\
	& \text{ where } E_4(h) \to 0 \text{ as } h \to 0 \\
	D_2 f(0,y_1) = & D_2 f(0,0) + y_1 D_{2,2} f(0,0) + |y_1| E_4(h)
\end{align*}
Therefore, $\nabla(h) = h( D_2 f(h,y_1) - D_2 f(0,y_1) ) = h^2 D_{1,2} f(0,0) + E'(h)$ where $E'(h) = \left|\sqrt{h^2+y_1^2}\right| E_3(h) - |y_1|E_4(h)$ and $E'(h) \to 0$ as $h \to 0$. And 
\begin{equation}
	\lim_{h \to 0} \frac{\nabla(h)}{h^2}  = D_{1,2} f(0,0)
\end{equation}
Therefore, $D_{1,2} f(0,0) = D_{2,1} f(0,0)$.
\end{proof}

\subsubsection{Continuity}
\begin{theorem}
Suppose $D_r f$ and $D_k f$ exists in an $n$-ball about $\bar{c}$.
And $D_{r,k} f$ and $D_{k,r} f$ are continuous at $\bar{c}$.
Then $D_{r,k} f = D_{k,r} f$.
\end{theorem}
\begin{proof}
We have $D_r f = (D_r f_1 , D_r f_2, \dots, D_r f_m)$.
Therefore, it is sufficient to prove the theorem for real-valued functions.
Suppose $n = 2$, $\bar{c} = (0,0)$ and the partial derivatives $D_1 f$ and $D_2 f$ exist and are continuous in some $2$-ball about $(0,0)$.
Suppose $(h,h)$ lies in that $2$-ball, then $D_1 f(h,h) \to D_1 f(0,0)$ as $h \to 0$.
\end{proof}

\begin{remark} A function $f$ such that $D_{1,2} f \ne D_{2,1} f$.
\[ \text{Let, }f(x,y) = \begin{cases} \frac{xy(x^2-y^2)}{x^2+y^2} & (x,y) \ne (0,0) \\ 0 & (x,y) = (0,0) \end{cases} \]
\begin{align*}
	D_1 f(x,y) = & \frac{\partial}{\partial x} \frac{x^3y-xy^3}{x^2+y^2} \\
	= & \frac{(3x^2y-y^3)(x^2+y^2) - 2x(x^3y-xy^3)}{(x^2+y^2)^2} \\
	= & \frac{x^4y+4x^2y^3-y^5}{(x^2+y^2)^2}\\
	D_1 f_{_{(x = 0)}} = & -y \implies D_{2,1} f_{_{(x = 0)}} = \frac{\partial}{\partial y} -y = -1
\end{align*}
\begin{align*}
	D_2 f(x,y) = & \frac{\partial}{\partial y} \frac{x^3y-xy^3}{x^2+y^2} \\
	= & \frac{(x^3-3xy^2)(x^2+y^2)-2y(x^3y-xy^3)}{(x^2+y^2)^2} \\
	= & \frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2} \\
	D_2 f_{_{(y = 0)}} = & x \implies D_{1,2} f_{_{(y = 0)}}  = \frac{\partial}{\partial x} x = 1
\end{align*}
Therefore, $D_{1,2} f \ne D_{2,1} f$ in the neighbourhood of $(0,0)$.
\begin{commentary} This treatment save a lot of time.
After $D_1 f$, we are planning to perform $D_{2,1} f = D_2 (D_1 f)$ in which the value of $x$ is going to be treated as a constant.
Therefore, we can simplify the expression by substituting $x = 0$ at this stage.
If you are not confident enough to substitute that ``early''.
You may take partial derivative with respect to $y$ and then substitute $x = 0$ and $y = 0$.
Why don't we substitute $y = 0$ before $D_2 f$ is something you should know already !\end{commentary}
\end{remark}

%\chapter{Implicit Functions and Extremum Problems}
\subsection{Implicit Functions and Extremum Problems}
\begin{definition}[Implicit function]
	Let $f$ be a function.
	Consider the equation, $f(\bar{x},\bar{y}) = \bar{0}$.
	If there exists a function $g$ such that $\bar{x} = g(\bar{y})$, then $g$ is an \textbf{implicit form} of $f$ or $g$ is defined implicitly by $f$.
	For example, a linear system of equations $Ax-b = 0$ implicitly defines $x = A^{-1}b$ provided $ A$ has non-zero determinant.
\end{definition}

\begin{definition}[Jacobian Determinant]
	Let $f : \mathbb{R}^n \to \mathbb{R}^n$, then determinant of the Jacobian matrix $Df(\bar{x})$ is the \textbf{Jacobian determinant} of $f$, $J_f(\bar{x})$.
\end{definition}

\begin{commentary}
	\subsubsection*{What is an implicit function ?}
	Consider the equation $\sin(x+y) = \cos(u+v)$.
	We can rewrite this equation as $\sin(x+y)-\cos(u+v) = 0$.
	Now, the equation is of the form $f(x,y,u,v) = 0$.
	Therefore, we have the function $f : \mathbb{R}^4 \to \mathbb{R}$ defined by $f(x,y,u,v) = \sin(x+y)-\cos(u+v)$.

	Let us play around,
	\begin{align*}
		\sin(x+y) = \cos(u+v) & \implies x+y = \sin^{-1}\cos(u+v) \\
		& \implies (x,y) = (\sin^{-1}\cos(u+v)-k,k) \\
		& \implies (x,y) = g(u,v)
		\intertext{ where  $g_1(u,v) = \sin^{-1}\cos(u+v)-k$, and $g_2(u,v) = k$ }
	\end{align*}
	Now we have defined a new function $g : \mathbb{R}^2 \to \mathbb{R}^2$ from the function $f$.
	Therefore, we could say that $f$ defines $g$ implicitly.
	Now we have a few questions to ask about the nature of such implicit functions (or implicitly defined functions),
	\begin{enumerate}
		\item Does there always exists a function for any such combination ?\\
			That is, does there exists a function $h$ such that $(x,u) = h(y,v)$ ?
		\item Suppose $f(x,y,u,v) = 0$. And function $h : \mathbb{R}^2 \to \mathbb{R}^2$ is implicitly defined by $f$. Does there always exists a neighbourhood of $(u,v)$ in which $h$ has continuous partial derivatives ?
		\item What are the properties of $f$ for these implicit functions to have nice properties ?
	\end{enumerate}
	It turns out that non-zero Jacobian determinant is nice property $f$ can have.
\end{commentary}

\begin{theorem}
Let $f : \mathbb{C} \to \mathbb{C}$.
Then $J_f(z) = |f'(z)|^2$.
\end{theorem}
\begin{proof}
Suppose $f : \mathbb{C} \to \mathbb{C}$ where $f(z) = u(z) + iv(z)$ where $u : \mathbb{C} \to \mathbb{R}$ and $v : \mathbb{C} \to \mathbb{R}$.
\begin{commentary}These real-valued functions $u,v$ have respective $u^*,v^*$ multivariate real functions such that $u^* : \mathbb{R}^2 \to \mathbb{R}$, where $u(z) = u^*(x,y)$ and $z = x+iy$.
Let $f(z) = z^2+1$.
Then $u^*(x,y) = x^2-y^2+1$ and $v^*(x,y) = -2xy$.
And theoretically we use derivatives of $u^*$ when we mention derivatives of $u$.\end{commentary}

Then $f$ has a derivative at $z$ only if the partial derivatives $D_1u,D_2u,D_1v,D_2v$ exists at $z$ and satisfies Cauchy-Riemann equations.
ie $D_1u(z) = D_2v(z)$ and $D_1v(z) = -D_2u(z)$.\cite[Theorem 5.22]{apostol}.

Thus we have $f'(z) = D_1u + iD_1v$ \cite[Theorem 12.6]{apostol}\footnote{Prove using first-order Taylor's formula}.
\begin{align*}
	f'(z) & = D_1u(z) + iD_1v(z) \\
	|f'(z)|^2 & = (D_1u(z))^2 + (D_1v(z))^2\\
	\intertext{For ease of representation, we write $D_1u$ instead of $D_1u(z)$}
	|f'(z)|^2 & = (D_1u)^2 + (D_1v)^2
\end{align*}
We also have
\[ J_f(z) = |Df(z)| = \begin{vmatrix} D_1u & D_2u \\ D_1v & D_2v \end{vmatrix} = D_1uD_2v - D_1vD_2u = (D_1u)^2 + (D_1v)^2 \]
Therefore, $J_f(z) = |f'(z)|^2$.
\end{proof}

\subsubsection{Functions with non-zero Jacobian determinant}
\begin{commentary}
That is, $f : \mathbb{R}^n \to \mathbb{R}^n$ such that $J_f \ne 0$ in an $n$-ball. In other words, we have an $n$-ball $B(\bar{x})$ such that $J_f(\bar{y}) \ne 0,\ \forall \bar{y} \in B(\bar{x})$.
\end{commentary}

\begin{theorem}
Let $B$ be an $n$-ball about $\bar{a}$ in $\mathbb{R}^n$, $\partial B$ be its boundary and $\bar{B} = B \cup \partial B$ be its closure.\footnote{$\bar{B}$ : The line above $B$ has a different meaning compare to $\bar{a}$ (situations like this are an abuse of language).}
Let $f : \mathbb{R}^n \to \mathbb{R}^n$ be continuous in $\bar{B}$ and all partial derivatives, $D_jf_i(\bar{x})$ exists for every $\bar{x} \in B$.
Let $f(\bar{x}) \ne f(\bar{a})$ for every $\bar{x} \in \partial B$ and $J_f(\bar{x}) \ne 0$ for every $\bar{x} \in B$.
Then $f(B)$ contains an $n$-ball about $f(\bar{a})$.
\begin{commentary}
	\begin{align*}
		B & = \{ \bar{x} : \| \bar{x} - \bar{a} \| < r \} \\
		\partial B & = \{ \bar{x} : \| \bar{x} - \bar{a} \| = r \} \\
		\bar{B} & = \{ \bar{x} : \| \bar{x} - \bar{a} \| \le r \} 
	\end{align*}
\end{commentary}
\end{theorem}
\begin{proof}
Define $g : \partial B \to \mathbb{R}$ where $g(\bar{x}) = \| f(\bar{x}) - f(\bar{a}) \| $.
We have, $f(\bar{x}) \ne f(\bar{a})$ for every $\bar{x} \in \partial B$, thus $g(\bar{x}) > 0$ for every $\bar{x} \in \partial B$.
Function $f$ is continuous on $\bar{B}$, thus $g$ is continuous on \begin{commentary}$\bar{B}$ and thus $g$ is continuous on its subset\end{commentary} $\partial B$.
Since $\partial B$ is compact, every continuous function on $\partial B$ attains its extrema\footnote{``Every continuous function on a compact set attains its extrema''} and thus $g$ attains its minimum value $m > 0$ somewhere on $\partial B$.

Consider $n$-ball $T$ about $f(\bar{a})$ with radius $\frac{m}{2}$, 
\[ T = B\left(f(\bar{a}),\frac{m}{2}\right) = \left\{ \bar{y} \in \mathbb{R}^n : \| f(\bar{a}) - \bar{y}\| < \frac{m}{2} \right\} \]
Therefore, it is sufficient to prove that $T \subset f(B)$.

Let $\bar{y} \in T$. Define $h : \bar{B} \to \mathbb{R}$ where $h(\bar{x}) = \| f(\bar{x}) - \bar{y} \|$. Again this continuous function $h$ on compact set $\bar{B}$ attains its extrema somewhere on $\bar{B}$. Since $\bar{y} \in T$, $h(\bar{a}) = \| f(\bar{a}) - \bar{y} \| < \frac{m}{2} $. Thus, the minimum of $h$ on $\bar{B}$ is less than $\frac{m}{2}$, \begin{commentary}since $\bar{a} \in \bar{B}$.\end{commentary}

Let $\bar{x} \in \partial B$, then 
\begin{align*}
	h(\bar{x}) = & \| f(\bar{x}) - \bar{y} \| \\
	= &  \| f(\bar{x}) - f(\bar{a}) + f(\bar{a}) - \bar{y} \| \\
	\ge &  \| f(\bar{x}) - f(\bar{a})\| + \| f(\bar{a}) - \bar{y} \| \\ 
	= &  g(\bar{x}) - h(\bar{a}) \\
	> & \frac{m}{2} \text{ since $g(\bar{x}) \ge m$ and $h(\bar{a}) < \frac{m}{2}$}
\end{align*}
	Thus $h$ doesn't attain its minimum on $\partial B$, but at an interior point $\bar{c} \in B$. Consider
	\[ h^2(\bar{x}) = \| f(\bar{x}) - \bar{y}\|^2 = \sum_{r = 1}^n (f_r(\bar{x}) - y_r)^2 \]
	The function $h^2$ also has minimum at the same point $\bar{c}$. Thus all partial derivatives of $h^2$ at $\bar{c}$ are zero. ie, 
	\[ D_k h^2(\bar{c}) = \sum_{r = 1}^n (f_r(\bar{c})-y_r)D_kf_r(\bar{c}) = 0 \]
	This is a system of linear equations with non-zero determinant since $\bar{c} \in B$ and we have $J_f(\bar{c}) \ne 0$. Therefore, $f_r(\bar{c}) = y_r$. That is, $f(\bar{c}) = \bar{y} \in f(B)$. Since $\bar{y} \in T$ is arbitrary, $T \subset f(B)$.
\end{proof}

\begin{theorem}
	Let $A$ be an open subset of $\mathbb{R}^n$ and $f : A \to \mathbb{R}^n$ is continuous and has continuous partial derivatives $D_jf_i$ on $A$. If $f$ is one-to-one on $A$ and $J_f(\bar{x}) \ne 0,\ \forall \bar{x} \in A$, then $f(A)$ is open.
\end{theorem}
\begin{proof}
	Let $\bar{b} \in f(A)$. Then $\bar{b} = f(\bar{a})$ for some $\bar{a} \in A$.
	We have, $f$ is continuous, $f$ has continuous partial derivatives on $A$ and $J_f(\bar{x}) \ne 0$ for every $\bar{x} \in A$.
	Therefore, there exists an open ball $B \subset A$ containing $\bar{a}$ such that $f(B) \subset f(A)$ contains an $n$-ball about $f(\bar{a})$.
	Since $\bar{b} \in f(A)$ is arbitary, every point in $f(A)$ has an $n$-ball containing it in $f(A)$.
	Therefore, $f(A)$ is open.

\begin{commentary} Two assumption in above theorem are trivial.
\begin{enumerate*}
	\item $f$ is continuous in the closed ball, $\bar{B}$.
Set $B$ so chosen that $\bar{B} \subset A$ and $f$ is continuous in $A$.
Thus, $f$ is continuous in $\bar{B}$.
	\item $f$ has different value at boundary compared to center.
ie, $f(\bar{a}) \ne f(\bar{x}),\ \forall x \in \partial B$.
	We have, $f$ is injective on $A$, and $\bar{B} \subset A$.
	Thus $f$ has different values for any two distinct points in it.
Thus, $\forall \bar{x},\bar{y} \in \bar{B},\  \bar{x} \ne \bar{y} \implies \bar{x}, \bar{y} \in A$, and $\bar{x} \ne \bar{y} \implies f(\bar{x}) \ne f(\bar{y})$ \end{enumerate*}
\end{commentary}
\end{proof}

\begin{theorem}
Let $S$ be an open subset of $\mathbb{R}^n$ and $f : S \to \mathbb{R}^n$.
Let components of $f$ has continuous partial derivatives on $S$, $D_jf_i$ and $J_f(\bar{a}) \ne 0$ for some point $\bar{a} \in S$.
Then there is an $n$-ball $B$ about $\bar{a}$ on which $f$ is injective.
\end{theorem}
\begin{proof}
	Let $\bar{z} = (\bar{z}_1,\bar{z}_2,\dots,\bar{z}_n)$ where $\bar{z}_i \in \mathbb{R}^n$.
	ie, $\bar{z} \in \mathbb{R}^{n^2}$.
	Define function $h : \mathbb{R}^{n^2} \to \mathbb{R}$ by $h(\bar{z}) = \det{[D_jf_i(\bar{z}_i)]}$.
	Since $f$ has continuous partial derivatives on $S$, each component of $f$ has continuous partial derivatives in $S$ and thus $h$ is continuous on $S^n$ 
\begin{commentary} 
	which is a subset of $\mathbb{R}^{n^2}$ since $S$ is an open subset of $\mathbb{R}^n$.
\end{commentary}

	Let $\bar{a} \in S$ such that $J_f(\bar{a}) \ne 0$.
\begin{commentary}
	Existence of such a point in $S$ is assumed.
\end{commentary}
	Consider,$\bar{z}_i = \bar{a},\ \forall i$.
	Then $\bar{z} = (\bar{a},\bar{a},\dots,\bar{a})$.
	And $h(\bar{z}) = \det{[D_jf_i(\bar{a})]} = J_f(\bar{a}) \ne 0$.

	Since $h$ is continous and $h(\bar{z}) \ne 0$.
	There exists an $n$-ball $B$ about $\bar{a}$ in $S$ such that $h(\bar{z}) \ne 0$ for $\bar{z}_i \in B$.
	We claim that $f$ is injective on $B$.

	Suppose $f$ is not injective.
	ie, There exists $\bar{x},\bar{y} \in B(\bar{a})$ such that $\bar{x} \ne \bar{y}$ and $f(\bar{x}) = f(\bar{y)}$.
	Open ball $B(\bar{a})$ is a convex set.
	And the line segment $L(\bar{x},\bar{y}) \subset B(\bar{a})$.
	The function $f$ is differentiable on $S$.
	On applying mean-value theorem to each component of $f$, we get
	\[ 0 = f_i(\bar{y})-f_i(\bar{x}) = \nabla f_i(\bar{Z}_i)\cdot(\bar{y}-\bar{x}),\ i=1,2,\dotsc \]
	where $\bar{Z}_i \in L(\bar{x},\bar{y}) \subset B(\bar{a})$.
	Therefore, We have
	\[ \sum_{k=1}^n D_kf_i(\bar{Z}_i)(y_k-x_k) = 0 \]
	The determinant of this system of linear equations is nonzero, as the function $f$ has nonzero jacobian determinant at $\bar{Z}_i \in B(\bar{a})$ for $i = 1,2,\dotsc$.
	Thus, $y_i = x_i$ for $i = 1,2,\dotsc$. This contradicts $\bar{x} \ne \bar{y}$.
	Hence, the function $f$ is injective.
\end{proof}

\begin{theorem}
	Let $A$ be an open subset of $\mathbb{R}^n$ and assume that $f : A \to \mathbb{R}^n$ has continuous partial derivatives $D_jf_i$ on $A$.
	If $J_f(\bar{x}) \ne 0$ for all $\bar{x} \in A$, then $f$ is an open mapping.
\end{theorem}
\begin{proof}
	Let $S$ be an open subset of $A$.
	Let $\bar{x} \in S$.
	Clearly, $f$ has continuous partial derivatives on $S$ and $J_f(\bar{x}) \ne 0$ for all $\bar{x} \in S$.
	Thus, there is an $n$-ball $B(\bar{x})$ in which $f$ is injective.
	Therefore, $f(B(\bar{x})$ is open in $\mathbb{R}^n$.
	Since $\bar{x} \in S$ is arbitrary, $S = \cup_{\bar{x} \in S} B(\bar{x})$.
	And $f(S) = \cup_{\bar{x} \in S} f(B(\bar{x}))$.
	Therefore, $f(S)$ is open.
	Since open set $S$ is arbitrary, $f$ is an open mapping.
\end{proof}

\begin{commentary}
\begin{remark}[Properties]
	Functions with non-zero Jacobian determinant has following properties :
\begin{enumerate}
	\item If $J_f \ne 0$ in $n$-ball $B$ about $\bar{a}$ which has different values at its boundaries, then $f(B)$ has an $n$-ball about $f(\bar{a})$.
	\item If $J_f \ne 0$, $f$ has continuous partial derivatives in $S$, and $f$ is injective in an open set $A$, then $f(A)$ is open.
	\item Let $S$ be an open set in $\mathbb{R}^n$, $f$ has continuous partial derivatives in $S$, and $J_f(\bar{a}) \ne 0$ for some $\bar{a} \in S$, then $f$ is injective on an $n$-ball $B(\bar{a})$ in $S$.
	\item Let $A$ be an open set in $\mathbb{R}^n$, $f$ has continuous partial derivatives in $A$, and $J_f \ne 0$ in $A$, then $f$ is an open mapping.
\end{enumerate}
\end{remark}
\end{commentary}

\subsubsection{Inverse function Theorem}
\begin{theorem}[Inverse function]
Let $S$ be an open subset of $\mathbb{R}^n$ and $f$ be a continuously differentiable function\footnote{$f \in C'(S)$ : $f$ is continuously differentiable on $S$} $f : S \to \mathbb{R}^n$.
If $J_f(\bar{a}) \ne 0$ for some $\bar{a} \in S$, then there are two open sets $X \subset S$, and $Y \subset f(S)$ such that
\begin{enumerate}
	\item $\bar{a} \in X \text{ and } f(\bar{a}) \in Y$
	\item $Y = f(X)$
	\item $f$ is injective
	\item there exists another function $g : Y \to X$ such that $g(f(\bar{x})) = \bar{x},\ \forall \bar{x} \in X$
	\item $g$ is continuously differentiable on $Y$
\end{enumerate}
\begin{commentary}
	In other words, if $f \in C'$ and there exists $\bar{a} \in S$ such that $J_f(\bar{a}) \ne 0$, then $f$ has an inverse $f^{-1}$ in a neighbourhood of $f(\bar{a})$ and $f^{-1} \in C'$.
\end{commentary}
\end{theorem}
\begin{proof}
\begin{commentary}Step 1 : Construction of open sets $X$ and $Y$.\end{commentary}

Given that, $J_f(\bar{a}) \ne 0$ and $f \in C'$.
Thus all partial derivatives of $f$ are continuous on $S$.
Then $J_f$ is continuous on $S$, 
By the continuity of $J_f$ at $\bar{a}$, there exists a neighbourhood of $\bar{a}$, say $B_1(\bar{a})$ in which $J_f \ne 0$.
That is, $\forall \bar{x} \in B_1(\bar{a}),\ J_f(\bar{x}) \ne 0$.
Therefore,(by theorem) there exists an $n$-ball $B(\bar{a})$ on which $f$ is injective.
Let $B$ be an $n$-ball with center $\bar{a}$ contained in $B(\bar{a})$.
Then $f$ is injective on $B$.
Therefore,(by theorem) $f(B)$ contains an $n$-ball with center $f(\bar{a})$.
Let $Y$ be the $n$-ball contained in $f(B)$.
And  $X=f^{-1}(Y) \cap B$.
That is, the inverse image of $Y$ on $B$.
Since $f$ is continuous, $f^{-1}(Y)$ is open.
Thus, $X$ is an intersection of open sets.
And therefore, $X$ is open.\\
\begin{commentary}Step 2 : The inverse of $f$, say $g$.\end{commentary}

Clearly $\bar{a} \in X$ and $f(\bar{a}) \in Y$.
Also $Y = f(X)$ and $f$ is injective on $X$( since, $X \subset B$).

The closure of $B$, $\bar{B}$ is compact and $f$ is injective and continuous on $\bar{B}$.
Then\footnote{Existence of inverse of a continuous function on a compact set in metric spaces.} there exists a continuous function $g$ defined on $f(\bar{B})$ such that $g \circ f$ is the identity function on $\bar{B}$.
That is, $\forall x \in \bar{B},\ g(f(\bar{x})) = x$.
Thus, $g(X) = Y$ and $g$ is unique.\\
\begin{commentary}Step 3 : $g$ has continuous partial derivatives.\end{commentary}

Define a real-valued function $h : S^n \to \mathbb{R}$ by $h(\bar{Z}) = \det[D_jf_i(\bar{Z}_i)]$ where $\bar{Z}_1,\bar{Z}_2,\dots,\bar{Z}_n \in S$ and $\bar{Z} = (\bar{Z}_1,\bar{Z}_2,\dots,\bar{Z}_n)$.
Now, let $\bar{Z} = (\bar{a},\bar{a},\dots,\bar{a})$.
Then $h(\bar{Z}) \ne 0$ and $h$ is continuous on $S^n$.
Therefore, $\bar{Z}$ has a neighbourhood on which $h$ does not vanish (that is, nonzero).
Let $B_2(\bar{a})$ be the corresponding $n$-ball with center $\bar{a}$ such that $\bar{Z}_i \in B_2(\bar{a}) \implies h(\bar{Z}) \ne 0$.

Let $B$ be an $n$-ball with center $\bar{a}$ contained in $B_2(\bar{a})$.
Now $\bar{B} \subset B_2(\bar{a})$.
And $h(\bar{Z}) \ne 0,\ \forall \bar{Z}_i \in \bar{B}$.

We have, $g = (g_1, g_2, \dots,g_n)$.
It is enough to prove that $g_k \in C'$ for $k=1,2,\dots,n$.
Again, it is enough to prove that $D_rg_k$ exists and is continuous for $1 \le r \le n$.
(Fix some $r$ and prove that $D_rg_k$ is continuous.)

Let $\bar{y} \in Y$.
Define $\bar{x} = g(\bar{y})$ and $\bar{x}' = g(\bar{y}+t\bar{u}_r)$ where $t$ is sufficiently small such that $\bar{y}+t\bar{u}_r \in Y$.
Then $\bar{x},\bar{x}' \in X$.
And $f(\bar{x}')-f(\bar{x}) = t\bar{u}_r$.
Therefore $f_i(\bar{x})-f_i(\bar{x}') = 0$ when $i \ne r$.
And $f_i(\bar{x}')-f_i(\bar{x}) = t$ when $i = r$.
By mean-value theorem,
	\[ \frac{f_i(\bar{x}') - f(\bar{x})}{t} = \nabla f_i(\bar{Z}_i) \cdot \frac{\bar{x}'-\bar{x}}{t} \]
where $\bar{Z}_i \in L(\bar{x},\bar{x}')$, the line segment joining $\bar{x}$ and $\bar{x}'$.
Since $\det[D_jf_i(\bar{Z}_i)] = h(\bar{Z}) \ne 0$, this system of linear equations in $n$ unknowns, $\frac{x_j'-x_j}{t}$ has a unique solution.
As $t \to 0$, $\bar{x}' \to \bar{x}$.
And $\bar{Z}_i \to \bar{x}$.
Since $J_f(\bar{x}) \ne 0$, the limit
	\[ \lim_{t \to 0} \frac{g_k(\bar{y}+t\bar{u}_r)-g_k(\bar{y})}{t} \]
exists.
Thus, $D_rg_k(\bar{y})$ exists $\forall y \in Y$ and every $r$.	
By Cramer's rule, this limit is a quotient of two determinants of partial derivatives of $f$, which are all continuous since $f \in C'$.
Therefore, $D_rg_k$ are all continuous and $g \in C'$.
\end{proof}

\subsubsection{Implicit function Theorem}
\begin{theorem}
	Let $S$ be an open set in $\mathbb{R}^{n+k}$ and
	$f$ be a function $f : S \to \mathbb{R}^n$.
	Suppose $f$ is continuously differentiable on $S$.
	Let $(\bar{x}_0,\bar{t}_0) \in S$
	such that $\bar{x}_0 \in \mathbb{R}^n$, $\bar{t}_0 \in \mathbb{R}^k$, $f(\bar{x}_0,\bar{t}_0) = \bar{0}$ and $J_f(\bar{x}_0,\bar{t}_0) \ne 0$.
	Then there exists an open set $T_0$ containing $\bar{t}_0$ in $\mathbb{R}^k$ and
	a unique function $g : T_0 \to \mathbb{R}^n$ such that
\begin{enumerate}
	\item $g$ is continuously differentiable on $T_0$
	\item $g(\bar{t}_0) = \bar{x}_0$
	\item $f(g(\bar{t}),\bar{t}) = \bar{0},\ \forall \bar{t} \in T_0$
\end{enumerate}
\end{theorem}
\begin{proof}
	Given $f : S \to \mathbb{R}^n$ where $S \subset \mathbb{R}^{n+k}$.
	We have $f = (f_1,f_2,\dots,f_m)$.
	Also given that $f \in C'(S)$, $f(\bar{x}_0;\bar{t}_0) = 0$, and $J_f(\bar{x}_0;\bar{t}_0) \ne 0$.
	Define a function $F : S \to \mathbb{R}^{n+k}$ defined by $F = (F_1,F_2,\dots,F_{n+k})$.
	\[ F_m(\bar{x};\bar{t}) = \begin{cases}
		f_m(\bar{x};\bar{t}) & 1 \le m \le n\\
		t_{m-n} & n < m \le n+k
	\end{cases} \]
	\begin{commentary}
		For example, let $n = 3$, $k = 2$.
		Let $\bar{x} = (1,2,3)$ and $\bar{t} = (4,5)$.
		Suppose $f_k(1,2,3,4,5) = a_k$.
		Then $F(\bar{x};\bar{t}) = (a_1,a_2,a_3,4,5)$.
	\end{commentary}

	Then the Jacobian determinant of $F'(\bar{x};\bar{t})$ is given by
	\[ J_F(\bar{x};\bar{t}) = \begin{vmatrix}
		\begin{matrix}
			D_1f_1 & D_2f_1 & \dots & D_mf_1 \\
			D_1f_2 & D_2f_2 & \dots & D_mf_2 \\
			\vdots & \vdots & \ddots & \vdots \\
			D_1f_m & D_2f_m & \dots & D_mf_m
		\end{matrix}
		& 0 & \dots & 0 \\
		0 & 1 & \dots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \dots & 1 
	\end{vmatrix} \]
	Thus $J_F(\bar{x}_0;\bar{t}_0) = J_f(\bar{x}_0;\bar{t}_0) \ne 0$.
	Also, $F(\bar{x}_0;\bar{t}_0) = (\bar{0};\bar{t}_0)$.
	Therefore, by inverse function theorem, there exists open sets $X,Y$ containing $(\bar{x}_0;\bar{t}_0)$ and $(\bar{0};\bar{t}_0)$ such that $F$ is injective on $X$, $X = F^{-1}(Y)$ and there exists a unique local inverse function $G$ such that $G(F(\bar{x};\bar{t})) = (\bar{x};\bar{t})$ and $G \in C'(Y)$.

	Let $G = (v;w)$.
	That is, $v_i = G_i$ for $1 \le i \le n$.
	And $w_j = G_{n+j}$ for $1 \le j \le k$.
	We have, $G(F(\bar{x};\bar{t})) = (\bar{x};\bar{t})$.
	Therefore, $v(F(\bar{x};\bar{t})) = \bar{x}$ and $w(F(\bar{x};\bar{t})) = \bar{t}$.
	Since $X \subset F^{-1}(Y)$ and $F$ is one-to-one on $X$, for every $(\bar{x};\bar{t}) \in Y$, there exists $(\bar{x}';\bar{t}') \in X$ such that $F(\bar{x}';\bar{t}') = (\bar{x};\bar{t})$.
	By the definition of $F$, $\bar{t}' = \bar{t}$.
	Therefore, $v(\bar{x};\bar{t}) = v(F(\bar{x}';\bar{t}) = \bar{x}'$ and $w(\bar{x};\bar{t}) = w(F(\bar{x}';\bar{t}) = \bar{t}$.
	Now, we have $G : Y \to X$ defined by $G(\bar{x};\bar{t}) = (\bar{x}';\bar{t})$.

	Let $T_0$ be a subset of $\mathbb{R}^k$ defined by $T_0 = \{ \bar{t} \in \mathbb{R}^k : (\bar{0};\bar{t}) \in Y \}$.
	For each $\bar{t} \in T_0$ let $g : T_0 \to \mathbb{R}^n$ is defined by $g(\bar{t}) = v(\bar{0};\bar{t})$.
	The set $T_0$ is open.
	And $g \in C'(T_0)$ since $g$ is constructed from the components of $G$ which has continuous partial derivatives on $Y$.
	(ie, $G \in C'(Y)$.)

	Clearly, $g(\bar{t}_0) = v(\bar{0};\bar{t}_0) = \bar{x}_0$.
	And $(\bar{0};\bar{t}) = F(\bar{x}_0;\bar{t}_0)$.
	Therefore, we have $f(v(\bar{x};\bar{t});\bar{t}) = \bar{x}$.
	Let $\bar{x} = \bar{0}$, then $f(g(\bar{t});\bar{t}) = \bar{0}$.
	It is enough to prove that the function $g$ is unique.
	Suppose $f(g(\bar{t});\bar{t}) = f(h(\bar{t});\bar{t})$.
	Since $f$ is one-to-one on $X$, $(g(\bar{t});\bar{t}) = (h(\bar{t});\bar{t})$ for every $\bar{t} \in T_0$.
	And $g(\bar{t}) = h(\bar{t}),\ \forall \bar{t} \in T_0$.
\end{proof}

\subsubsection{Extrema of function of one variable}
\begin{commentary}
	The function $f : \mathbb{R} \to \mathbb{R}$ defined by $f(x) = x^3$ has derivative $f'(x) = 3x^2$.
	Thus $f'(0) = 0$.
	However, $0$ is not a local extrema for the function $f$.
	Thus, derivative of the function vanishing at point is not sufficient for a local extrema at that point.
\end{commentary}
\begin{theorem}[sufficient condition for local extrema]
	Let $n \ge 1$ and function $f$ has $n$th partial derivative in open interval $(a,b)$.
	Suppose for some $c \in (a,b)$,
	\[ f'(c) = f''(c) = \dots = f^{(n-1)}(c) = 0 \text{ and } f^{(n)}(c) \ne 0 \]
	If $n$ is even,
\begin{enumerate}
	\item $f$ has a local minimum at $c$ if $f^{(n)}(c)>0$
	\item $f$ has a local maximum at $c$ if $f^{(n)}(c) < 0$
\end{enumerate}
	and If $n$ is odd, there is neither a local minimum nor a local maximum at $c$.
\end{theorem}
\begin{proof}
	We have $f^{(n)}(c) \ne 0$.
	Thus, there exists an open interval $B(c)$ such that for each $x \in B(c)$, $f^{(n)}(x)$ has the same sign as $f^{(n)}(c)$.
	By Taylor's theorem, we have
	\begin{equation}
		f(x) = f(c) + \sum_{k = 1}^{n-1} \frac{f^{(k)}(c)}{k!}(x-c)^k + \frac{f^{(n)}(x_1)}{n!} (x-c)^n
	\end{equation}
	where $x_1 \in L(x,c)$, the line connecting $x$ and $c$.
	We have, $f^{(k)}(c) = 0$ for $k = 1,2,\dots,n-1$.
	Thus,
	\[ f(x)-f(c) = \frac{f^{(n)}(x_1)}{n!} (x-c)^n \]
	Case 1 : $n$ is even.

	If $n$ is even, then $(x-c)^n > 0$.
	Therefore, 
	$f(x)-f(c)$ has the same sign as $f^{(n)}(x_1)$.
	If $f^{(n)}(c) > 0$, then $f^{(n)}(x_1)$ has a positive value and $f(x)-f(c) > 0$ for every $x \in B(c)$.
	Therefore, $f$ has a local minimum at $c$.
	Similarly, if $f^{(n)}(c) < 0$, then $f(x)-f(c) < 0$ for every $x \in B(c)$.
	Therefore, $f$ has a local maximum at $c$.\\
	Case 2 : $n$ is odd.

	If $n$ is odd, then $(x-c)^n$ takes both positive and negative values.
	Therefore, $f(x) - f(c)$ has both positive and negative values in $B(c)$.
	Thus $f$ has neither local minimum nor local maximum at $c$.
\end{proof}

\subsubsection{Extrema of function of several variables}
\begin{definition}
	If function $f$ is differentiable at $\bar{a}$ and $\nabla f(\bar{a}) = \bar{0}$, the point $\bar{a}$ is a \textbf{stationary point} of $f$.
\end{definition}

\begin{definition}
	A stationary point is a \textbf{saddle point} if every $n$-ball $B(\bar{a})$ contains points $\bar{x}$ such that $f(\bar{x}) > f(\bar{a})$ and other points such that $f(\bar{x}) < f(\bar{a})$.
\end{definition}

\begin{definition}
	A function $Q : \mathbb{R}^n \to \mathbb{R}$ defined by $Q(\bar{x}) = \sum\limits_{i=1}^n \sum\limits_{j=1}^n a_{ij}x_ix_j$
	where $a_{ij} \in \mathbb{R}$ is a function of the \textbf{quadratic form} or simply a quadratic form.
\begin{description}
	\item[symmetric quadratic form] $a_{ij} = a_{ji},\ \forall i,j$. 
	\item[positive definite quadratic form] $Q(\bar{x}) > 0, \forall \bar{x} \ne \bar{0}$. 
	\item[negative definite quadratic form] $Q(\bar{x}) < 0, \forall \bar{x} \ne \bar{0}$. 
\end{description}
\end{definition}

\begin{commentary}
	Suppose $f : \mathbb{R}^n \to \mathbb{R}^m$.
	And second derivative of $f$ exists if the derivative of $f$, $f'$ is differentiable.
	Thus, $f'(\bar{a}+h\bar{t}) = f'(\bar{a}) + h f''(\bar{a})(\bar{t}) + |h| \|t\| E_{\bar{a}}(h\bar{t})$ where $E_{\bar{a}} \to \bar{0}$ as $h \to 0$.
	Writing the Taylor's first order formula using matrices, we can see that $f''(\bar{a})(\bar{t})$ is of the quadratic form.
\end{commentary}

\begin{theorem}
	Let $f$ be a function $f : \mathbb{R}^n \to \mathbb{R}^m$.
	Suppose that the second order partial derivatives $D_{i,j}f$ exists in an $n$-ball $B(\bar{a})$ and are continuous at $\bar{a}$ where $\bar{a}$ is a stationary point of $f$.
	\[ \text{Let } Q(\bar{t}) = \frac{1}{2} f''(\bar{a},\bar{t}) = \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n D_{i,j} f(\bar{a})t_it_j \]
\begin{enumerate}
	\item If $Q(\bar{t}) > 0$ for all $\bar{t} \ne \bar{0}$, $f$ has a relative minimum at $\bar{a}$.
	\item If $Q(\bar{t}) < 0$ for all $\bar{t} \ne \bar{0}$, $f$ has a relative maximum at $\bar{a}$.
	\item If $Q(\bar{t})$ takes both positive and negative values, then $f$ has a saddle point at $\bar{a}$.
\end{enumerate}
\end{theorem}
\begin{proof}
	Define $Q : \mathbb{R}^n \to \mathbb{R}$ by $Q(\bar{t}) = \frac{1}{2}f''(\bar{a};\bar{t})$.
	Then $Q$ is continuous for every $\bar{t} \in \mathbb{R}^n$.
	Let $S$ be the boundary of the $n$-ball $B(\bar{0},1)$.
	That is, $S = \{ \bar{t} \in \mathbb{R}^n : \|t\|=1\}$.\\
	Case 1 : Suppose that $Q(\bar{t}) > 0,\ \forall \bar{t} \ne \bar{0}$.

	We have, $Q$ is a continuous real-valued function on a compact interval, $S$.
	Therefore, $Q$ attains its extrema.
	Thus $Q$ has a minimum value at point in $S$, say $m$.
	Clearly $Q(\bar{t}) > 0 \implies m > 0$.
	\begin{align*}
		\text{By Taylor's formula, }& f(\bar{a}+\bar{t}) - f(\bar{a}) = \nabla f(\bar{a}) \cdot \bar{t} + \frac{1}{2} f''(\bar{z};\bar{t}) \text{ where } \bar{z} \in L(\bar{a}+\bar{t},\bar{a})
		\intertext{For every $\bar{a} \in S,\ \nabla f(\bar{a}) = \bar{0}$. And $f(\bar{a}+\bar{t}) - f(\bar{a}) \to \frac{1}{2}f''(\bar{a};\bar{t})$ as $\bar{t} \to \bar{0}$.}
		\text{Thus, }f(\bar{a}+\bar{t}) - f(\bar{a}) & = \frac{1}{2}f''(\bar{a};\bar{t}) + \|t\|^2 E(\bar{t})\\
		& = Q(\bar{t}) + \|t\|^2 E(\bar{t}) \text{ where } E(\bar{t}) \to \bar{0} \text{ as } \bar{t} \to \bar{0}
	\end{align*}
	Let $c = \frac{1}{\|\bar{t}\|}$.
	Then $c\bar{t} \in S$ and $Q(c\bar{t}) = c^2 Q(\bar{t}) \ge m$.
	Thus, $Q(\bar{t}) \ge m\|\bar{t}\|^2$.\\
	Therefore, $f(\bar{a}+\bar{t}) - f(\bar{a}) \ge m\|\bar{t}\|^2 + \|\bar{t}\|^2 E(\bar{t})$\\

	Choose $n$-ball $B(\bar{0},r)$ such that $|E(\bar{t})| < \frac{m}{2}$ for every $\bar{t} \in B(\bar{0},r)$.
	Thus,
	\[ -\frac{m}{2}\|\bar{t}\|^2 \le -\|\bar{t}\|^2 |E(\bar{t})| \le 0 \]
	Therefore, $f(\bar{a}+\bar{t}) - f(\bar{a}) \ge m\|\bar{t}\|^2 - \frac{m}{2}\|\bar{t}\|^2 = \frac{m}{2}\|t\|^2$ for every $\bar{t} \in B(\bar{0},r)$.
	Clearly, $f$ has a local minimum at $\bar{a}$.\\
	Case 2 : Suppose $Q(\bar{t}) < 0,\ \forall \bar{t} \ne \bar{0}$, then consider $-f$.

	Clearly, function $-f$ has a local minimum at $\bar{t}$.
	Thus $f$ has a local maximum at $\bar{t}$.\\
	Case 3 : Suppose $Q(\bar{t})$ takes both positive and negative values.
	\begin{align*}
		\text{We have, } f(\bar{a}+\lambda \bar{t}) - f(\bar{a}) & = Q(\lambda \bar{t}) + \lambda^2 \|t\|^2 E(\lambda \bar{t}).\\
		& = \lambda^2 [ Q(\bar{t}) + \|\bar{t}\|^2 E(\lambda\bar{t})]
	\end{align*}
	Choose $n$-ball $B(\bar{0},r)$ such that $\|\bar{t}\|^2 E(\lambda\bar{t}) < \frac{1}{2} |Q(\bar{t})|,\ \forall \bar{t} \in B(\bar{0},r)$.
	We have $\bar{t} \in B(\bar{0},r) \implies \lambda < r$.
	Then, error function $E(\bar{t})$ on the RHS is small enough, not to affect the sign of the RHS.
	Thus $f(\bar{a}+\lambda\bar{t}) - f(\bar{a})$ has the same sign as $Q(\bar{t})$.
	Therefore, $\bar{a}$ is a saddle point.
\end{proof}

\begin{theorem}
Let $f$ be a real-valued function $f : \mathbb{R}^2 \to \mathbb{R}$ with continuous second order partial derivatives at a stationary point $\bar{a} \in \mathbb{R}^2$.
Let $A = D_{1,1}f(\bar{a})$, $B = D_{1,2}f(\bar{a}) = D_{2,1}f(\bar{a})$, and $C = D_{2,2}f(\bar{a})$.
And let $\Delta = \begin{vmatrix}A & B \\ B & C \end{vmatrix} = AC - B^2$.
Then we have,
\begin{enumerate}
	\item If $\Delta > 0$ and $A > 0$, then $f$ has a relative minimum at $\bar{a}$.
	\item If $\Delta > 0$ and $A < 0$, then $f$ has a relative maximum at $\bar{a}$.
	\item if $\Delta < 0$, then $f$ has a saddle point at $\bar{a}$.
\end{enumerate}
\end{theorem}
\begin{proof}
	We have, function $f : \mathbb{R}^2 \to \mathbb{R}$ with $\nabla f(\bar{a}) = (0,0)$.
	Consider the quadratic form $Q(x,y) = \frac{1}{2}[Ax^2+Bxy+Cy^2]$ where $A = D_{1,1}f(\bar{a})$, $B = D_{1,2}f(\bar{a})$, and $C = D_{2,2}f(\bar{a})$.
	Therefore, $Q(x,y) = \frac{1}{2}f''(\bar{a};\bar{t})$.\\
	Case 1 : Suppose $A \ne 0$.
	\begin{align*}
		Q(x,y) & = \frac{1}{2A}[A^2x^2+ABxy+ACy^2] \\
		& = \frac{1}{2A}[(Ax+By)^2 - B^2y^2 + ACy^2] \\
		& = \frac{1}{2A}[(Ax+By)^2 + \Delta y^2]
	\end{align*}
	If $\Delta > 0$, then $Q(x,y)$ has the same sign as $A$.
	Therefore, $f$ has a local minimum/maximum at $\bar{a}$ depending on the sign of $A$.\\
	Case 2 : Suppose $A = 0$.
	Then $Q(x,y)  = \frac{1}{2}[Bxy+Cy^2] = \frac{1}{2}(Bx+Cy)y$.

	Now we have two lines in $\mathbb{R}^2$, $y = 0$ and $Bx+Cy = 0$.
	These two lines divides $\mathbb{R}^2$ into four regions.
	The value of $Q(x,y)$ is positive in two of those regions and negative in the other two regions.
	Therefore, $f(\bar{a}+\bar{t})-f(\bar{a})$ assumes both positive and negative values in any neighbourhood $B(\bar{a},r)$.
	Therefore, $\bar{a}$ is a saddle point.
\end{proof}


%\chapter{Multiple Riemann Integrals*}
%\chapter{Multiple Lebesgue Integrals*}
%\chapter{Cauchy's Theorem and the Residue Calculus*}
%\cite{rudin}
%\chapter{The Real and Complex Number Systems}
%\chapter{Basic Topology}
%\chapter{Numerical Sequences and Series}
%\chapter{Continuity}
%\chapter{Differentiation}
%\chapter{The Riemann- Stieltjes Integral}
%\chapter{Sequences and Series of Functions}
%\chapter{Some Special Functions}
%\chapter{Functions of Several Variables}
%\chapter{Integration on Differential Forms}
\section{Integration on Differential Forms}
\begin{definition}
	A \textbf{$k$-cell} in $R^k$ is given by, 
	\[ I^k = \{ \bar{x} \in \mathbb{R}^k : a_i \le x_i \le b_i,\ \forall i \} \]
	where $\bar{a},\bar{b} \in \mathbb{R}^k$.
	Let $f$ be a continuous, real-valued function on $I^k$.
	Then, the \textbf{integral} of $f$ over $I^k$ is given by,
	\[ \int_{I^k}f(\bar{x}) d\bar{x} = f_0 \text{ where } f_k = f \text{ and } \]
	\[ f_{r-1} = \int_{a_r}^{b_r} f_r(x_0,x_1,\dots,x_r) dx_r,\ r = 1,2,\dots,k \]
\begin{commentary}
	In other words,
	\[ \int_{I^k} f(\bar{x})\ d\bar{x} = \idotsint_{a_k}^{b_k} \left[f(x_0,x_1,\dots,x_k)\ dx_k\right]\ dx_{k-1} \dotsm dx_1 \]
\end{commentary}
\end{definition}

\begin{theorem}
	For every $f \in \mathscr{C}(I^k)$, $L(f) = L'(f)$.

\begin{commentary}
	In other words, integral of a function over a $k$-cell is independent of the order in which those $k$ integrations are carried out.
\end{commentary}
\end{theorem}
\begin{proof}
\begin{commentary}Step 1 : ``Separable'' Functions ie, $h(\bar{x}) = \prod h_i(x_i)$.\end{commentary}

\begin{commentary}
	(``separable'' is not standard.
	It is only for the purpose of understanding.)
\end{commentary}

Let $h(\bar{x}) = h_1(x_1)h_2(x_2)\dots h_k(x_k)$ where $h_j \in [a_j,b_j]$.
	\[ L(h) = \int_{I^k} \left(\prod_{i = 1}^k h_i(x_i)\right) d\bar{x} = \prod_{i = 1}^k \int_{a_k}^{b_k} h_i(x_i)\ dx_i = L'(h) \]
\begin{commentary}Step 2 : Algebra of ``separable'' functions, $\mathscr{A}$.\end{commentary}

Let $\mathscr{A}$ be all finite sums of functions such as $h$.
Let $g \in \mathscr{A}$.
\begin{align*}
	L(g) = & \int_{I^k} \left(\sum_j \prod_i h_{i,j}(x_i) \right) d\bar{x} \\
	& = \sum_j \int_{I^k} \prod_i h_{(j)}(\bar{x})\ d\bar{x}\\
	& = \sum_j \prod_i \int_{a_k}^{b_k} h_{i,j}(x_i)\ d(x_i)\\
	& = L'(g)
\end{align*}
\begin{commentary}Step 3 : All functions continuous on $I^k$.\end{commentary}

Let $f \in \mathscr{C}(I^k)$.
ie, a function which is continuous in $I^k$.

\begin{commentary}
	\textbf{Stone-Weierstrass theorem} - Let $\mathscr{A}$ be an algebra of real, continuous functions on a compact set $K$.
	If $\mathscr{A}$ separates points on $K$ and if $\mathscr{A}$ vanishes at no point of $K$, then the uniform closure $\mathscr{B}$ of $\mathscr{A}$ consists of all real, continuous functions on $K$.

	Clearly, the algebra of functions $\mathscr{A}$ separates points on $I^k$ and vanishes nowhere on $I^k$.
	Suppose $\bar{x} \ne \bar{y}$, then there exists $m$ such that $x_m \ne y_m$.
	Thus $h(\bar{x}) = x_m$ separates $\bar{x}$ and $\bar{y}$.
	Again $h(\bar{x}) = 1$ vanishes nowhere on $I^k$.
	Thus every function which is continuous on $I^k$ is the limit of some uniformly convergent sequence of functions in $\mathscr{A}$.
\end{commentary}
	
Let $V = \prod_{j = 1}^k (b_j - a_j)$.
Then by Stone-Weierstrass theorem,
for any $\epsilon > 0$,	there exists a function $g \in \mathscr{A}$
such that $\|f-g\| < \frac{\epsilon}{V}$
where the norm of a function $f$ is defined by
$\|f\| = \max\{f(\bar{x}) : \bar{x} \in I^k\}$.

Therefore, it is sufficient to prove that $\|L(f)-L'(f)\| < \epsilon$.
Since $\|f-g\| < \frac{\epsilon}{V}$, $|L(f-g)| < \epsilon$ and $|L'(f-g)| < \epsilon$. Thus,
\begin{align*}
	L(f)-L'(f) & = L(f) - L(g) + L'(g) - L'(f) \\
	& = L(f-g) + L'(g-f)\\
	|L(f)-L'(f)| & < 2\epsilon
\end{align*}
Therefore, $L(f) = L'(f)$.
\end{proof}

\begin{definition}
	Let $f : \mathbb{R}^k \to \mathbb{C}$.
	The \textbf{support} of $f$ is the closure of the set of all points $\bar{x} \in \mathbb{R}^k$ such that $f(\bar{x}) \ne 0$.
\end{definition}

\begin{remark}
	Let $f$ be a continuous function with compact support.
	And $I^k$ be any $k$-cell containing the support of $f$.
	Then, $\int_{\mathbb{R}^k} f\ d\bar{x} = \int_{I^k} f\ d\bar{x}$.
\end{remark}

\begin{commentary}
	$L(f) = L'(f)$ where $f$ is the limit function of a sequence of functions with compact support.
	\cite[\S10.4 Example]{apostol}
%	``The definition of support permits closure of non-vanishing points.
%	And this might be of some use(I don't know yet) when analysing limit functions which belong to the closure of continuous functions on $I^k$.''
\end{commentary}

\begin{definition}
	Let $E$ be an open subset in $\mathbb{R}^n$.
	Then function $G : E \to \mathbb{R}^n$ is \textbf{primive}
	if it satisfies
	\begin{equation}
		G(\bar{x}) = \sum_{i \ne m} x_i \bar{e}_i + g(\bar{x})\bar{e}_m
	\end{equation}
	for some integer $m$ and
	some function $g : E \to \mathbb{R}$
	(where $\bar{e}_i$ are the unit co-ordinate vectors).
	\begin{equation}
		G(\bar{x}) = \bar{x} + [g(\bar{x})-x_m]\bar{e}_m
	\end{equation}
	If $g$ is differentiable at $\bar{a}$, then $G$ is also differentiable at $\bar{a}$.
	The matrix $[\alpha_{i,j}]$ of $G'(\bar{a})$ is given by 
	\[ \alpha_{i,j} = \begin{cases} 
		D_jg(\bar{a}) & i = m\\
		1 & i \ne m, j = i \\
		0 & i \ne m, j \ne i
	\end{cases} \]
	The Jacobian of $G$ at $\bar{a}$ is given by,
	$J_G(\bar{a}) = \det[G'(\bar{a})] = D_mg(\bar{a})$.

	Total derivative $G'(\bar{a})$ is invertible if and only if $D_mg(\bar{a}) \ne 0$.
\end{definition}

\begin{definition}
	A linear operator $B$ on $\mathbb{R}^n$ that interchanges some pair of members of the standard basis and leaves the others fixed is a \textbf{flip}.
\end{definition}

\begin{theorem}
	Suppose $F$ is a $\mathscr{C}'$-mapping of an open set $E \subset \mathbb{R}^n$ into $\mathbb{R}^n$, $\bar{0} \in E$, $F(\bar{0}) = \bar{0}$, and $F'(\bar{0})$ is invertible.
	Then there is a neighbourhood of $\bar{0}$ in $\mathbb{R}^n$ in which the representation $F(\bar{x}) = B_1B_2 \dotsm B_{n-1}G_n\circ G_{n-1}\circ \dotsm \circ G_1(\bar{x})$ is valid where $G_i$ are primitive $\mathscr{C}'$-mapping in some neighbourhood of $\bar{0}$, $G_i(\bar{0}) = \bar{0}$, $G'(\bar{0})$ is invertible and each $B_i$ is either a flip or identity operator.

\begin{commentary}
	In other words, locally $F$ is a composition of primitive mappings and flips.
\end{commentary}
\end{theorem}
\begin{proof}
Proof by mathematical induction on $m$.
\begin{commentary}
\begin{align}
	\text{There exists } & \text{a neighbourhood of } \bar{0},\ V_m  \text{ such that } F_m \in \mathscr{C}'(V_m)\\
	F_m(\bar{0}) & = \bar{0}\\
	F_m'(\bar{0}) & \text{ is invertible and }\\
	P_{m-1}F_m(\bar{x}) & = P_{m-1}\bar{x},\ \bar{x} \in V_m
\end{align}
	where the $k$th projection $P_k : \mathbb{R}^n \to \mathbb{R}^n$ is defined by $P_k(\bar{x}) = x_1\bar{e}_1 + x_2\bar{e}_2 + \dotsb + x_k\bar{e}_k + 0\bar{e}_{k+1} + \dotsb + 0\bar{e}_n$.
Clearly, $P_0(\bar{x}) = \bar{0}$.

	The essence of this proof lies in the fourth statement which is designed to construct a seqeunce of functions $F_1,F_2,\dots,F_n$ such that the other three statements remains true for every function in this sequence.\\
\end{commentary}

\begin{commentary}Step 1 : Initial Case, prove that all the four statements are true for $m = 1$.\end{commentary}

Define $F_1 = F$ and $V_1 = E$.
Thus, $F \in \mathscr{C}'(E) \implies F_1 \in \mathscr{C}'(V_1)$.
Also we have, $F_1(\bar{0}) = F(\bar{0}) = 0$, and $F_1'(\bar{0}) = F'(\bar{0})$ is invertible.
Obviously the trivial projection, $P_0(F_1(\bar{x})) = P_0(F(\bar{x})) = \bar{0} = P_0(\bar{x})$ for every $\bar{x} \in V_1$.\\
\begin{commentary}Step 2 : Induction Hypothesis, suppose all the four statements are true for $m = 1,2,\dots,n-1$.\end{commentary}

Suppose that for each $m = 1,2,\dots,n-1$, there exists a neighbourhood of $\bar{0}$, say $V_m$ such that $F_m \in \mathscr{C}'(V_m),\ F_m(\bar{0}) = \bar{0},\ F_m'(\bar{0})$ is invertible and $P_{m-1}F_m(\bar{x}) = P_{m-1}(\bar{x})$ for every $\bar{x} \in V_m$.\\
\begin{commentary}Step 3 : Induction Step, prove that all the four statements are true for $m = n$.\end{commentary}
\begin{align*}
	P_{m-1}F_m(\bar{x}) & = P_{m-1}\bar{x},\ \bar{x} \in V_m \\
	& = x_1\bar{e}_1 + x_2\bar{e}_2 + \dotsb + x_{m-1}\bar{e}_{m-1} 
	\intertext{We may write remaining components of $F_m(\bar{x})$ using real-valued functions. That is, $k$th component of the function $F_m$, say $F_{m_k} = \alpha_k$.}
	F_m(\bar{x}) & = P_{m-1}(\bar{x}) + \sum_{i = m}^n \alpha_i(\bar{x})\bar{e}_i
\end{align*}
Since $F_m \in \mathscr{C}'(V_m)$, the functions $\alpha_i \in \mathscr{C}'(V_m)$.
Taking $m$th partial derivative on either sides, we get
	\[ D_mF_m(\bar{0}) = F_m'(\bar{0})\bar{e}_m  = \sum_{i = m}^n D_m \alpha_i (\bar{0})\bar{e}_i \]
We have, $F_m'(\bar{0})$ is invertible.
Thus $F_m'(\bar{0})\bar{e}_m \ne 0$.
Thus there exists some integer $k$ such that $m \le k \le n$ and $D_m\alpha_k(\bar{0})\bar{e}_k \ne 0$.
Let $B_m$ be the flip that interchanges $m$ and $k$.
If $m = k$, then $B_m$ is identity map.
\begin{equation}
	\text{Define } G_m(\bar{x}) = \bar{x} + [\alpha_k(\bar{x}) - x_m]\bar{e}_m
\end{equation}
Then $G_m \in \mathscr{C}'(V_m)$, $G_m$ is a primitive mapping, and $G_m'(\bar{0})$ is invertible since $D_m\alpha_k(\bar{0}) \ne 0$.

By inverse function theorem, there exists an open set $U_m$, $0 \in U_m$ and $U_m \subset V_m$ such that $G_m$ is a bijection from $U_m$ onto a neighbourhood of $\bar{0}$, say $V_{m+1}$ in which $G_m^{-1}$ is continuously differentiable.
\begin{equation}
	\text{Define } F_{m+1}(\bar{y}) = B_mF_m \circ G_m^{-1}(\bar{y}),\ \bar{y} \in V_{m+1}
\end{equation}
Then $F_m \in \mathscr{C}'(V_{m+1})$, $F_{m+1}(\bar{0}) = 0$ and $F_{m+1}'(\bar{0})$ is invertible by the chain rule.
Also, for $\bar{x} \in U_m$, we have
\begin{align*}
	P_mF_{m+1}(G_m(\bar{x})) & = P_m B_m F_m (\bar{x}) \\
	& = P_m (P_{m-1}\bar{x} + \alpha_k(\bar{x}\bar{e}_m + \dotsb) \\
	& = P_{m-1}\bar{x} + \alpha_k(\bar{x})\bar{e}_m \\
	& = P_m G_m(\bar{x})\\
	\implies P_mF_{m+1}(\bar{y}) & = P_m(\bar{y})
\end{align*}
Thus, by mathematical induction, we have characterised a finite sequence of function $F_1,F_2,\dots,F_n$ such that all the four statements are true.\\
\begin{commentary}Step 4 : Using the finite sequence of functions $F_1, F_2, \dots, F_n$ constructed in the proof, we can represent $F$ in terms of flips and primitive mappings.\end{commentary}

Let $\bar{x} \in U_m$ and $\bar{y} = G_m(\bar{x})$. We have,
\begin{align*}
	B_mF_m \circ G_m^{-1}(\bar{y}) & = F_{m+1}(\bar{y}) \\
	B_m B_m F_m(\bar{x}) & = B_m F_{m+1}(G_m(\bar{x})) \\
	IF_m(\bar{x}) & = B_m F_{m+1}(G_m(\bar{x})) \\
	F_m(\bar{x}) & = B_m(F_{m+1}(G_m(\bar{x}))\\
\end{align*}
\begin{equation}
	\text{Thus, } F_m = B_m F_{m+1} \circ G_m,\ m = 1,2,\dots, n
\end{equation}
Therefore, we have
\begin{align*}
	F & = F_1 \\
	& = B_1F_2\circ G_1 \\
	& = B_1 (B_2F_3 \circ G_2) \circ G_1 = B_1B_2F_3 \circ G_2 \circ G_1 \\
	& \vdots \\
	F & = B_1 B_2 \dotsm B_n F_n \circ G_{n-1} \circ G_{n-2} \circ \dotsm \circ G_1
	\intertext{Since $P_n F_n(\bar{x}) = P_{n-1}(\bar{x})$, $F_n$ is a primitive mapping, say $G_n$.}
	F & = B_1 B_2 \dotsm B_n G_n \circ G_{n-1} \circ G_{n-2} \circ \dotsm \circ G_1
\end{align*}
\end{proof}

\begin{remark}
	Let $K$ be a compact subset of $\mathbb{R}^n$.
	Then a family of function $\psi_1,\psi_2,\dots,\psi_s$ where $\psi_j : K \to \mathbb{R}$ is a partition of unity if it satisfies
	\begin{enumerate}
		\item $0 \le \psi_j(\bar{x}) \le 1$ for every $\bar{x} \in K,\ j = 1,2,\dots, s$.
		\item $\sum\limits_{j=1}^s \psi_j(\bar{x}) = 1$ for every $\bar{x} \in K$.
	\end{enumerate}
\end{remark}
\begin{theorem}[partitions of unity]
	Suppose $K$ is a compact subset of $\mathbb{R}^n$, and $\{V_\alpha\}$ is an open cover of $K$.
	Then there exists functions $\psi_1,\psi_2,\dots,\psi_s \in \mathscr{C}(\mathbb{R}^n)$ such that
	\begin{enumerate}
		\item $0 \le \psi_i \le 1$ for $1 \le i \le s$
		\item each $\psi_i$ has its support in some $V_\alpha$ and
		\item $\psi_1(\bar{x}) + \psi_2(\bar{x}) + \dotsb + \psi_s(\bar{x}) = 1$ for every $\bar{x} \in K$.
	\end{enumerate}
	\begin{commentary}
		In other words, every open cover of a compact subset of $\mathbb{R}^n$ has a partition of unity with compact support in a finite subcover.
	\end{commentary}
\end{theorem}
\begin{proof}
	\begin{commentary}
		Step 1 : Construction of $\phi_j$.
	\end{commentary}

	We have, $\{ V_\alpha \}$ is a cover of $K$.
	Therefore, every $\bar{x} \in K$ belongs to some $V_{\alpha(\bar{x})}$.
	And there exists open balls $B(\bar{x})$ and $W(\bar{x})$ such that
	\begin{equation}
		\bar{x} \in B(\bar{x}) \subset \bar{B}(\bar{x}) \subset W(\bar{x}) \subset \bar{W}(\bar{x}) \subset V_{\alpha(\bar{x})}
	\end{equation}
	The family $\{ B(\bar{x}) \}$ is an open cover of $K$.
	Since $K$ is compact, the open cover $\{ B(\bar{x}) \}$ has a finite subcover, say $\{ B(\bar{x}_1),B(\bar{x}_2),\dots,B(\bar{x}_s) \}$.
	That is, there are points $\bar{x}_1,\bar{x}_2,\dots,\bar{x}_s$ in $K$ such that
	\begin{equation}
		K = B(\bar{x}_1) \cup B(\bar{x}_2) \cup \dotsb \cup B(\bar{x}_s)
	\end{equation}
	Since $\mathbb{R}^n$ is a metric space,\footnote{metric spaces have a simpler version of Urysohn's lemma or apply Urysohn's lemma}
	there exists continuous functions $\phi_j : \mathbb{R}^n \to [0,1]$
	such that $\phi_j(B(\bar{x}_j)) = \{1\}$,
	$\phi_j(\mathbb{R}^n-W(\bar{x}_j)) = \{0\}$ and
	$0 \le \phi_j(\bar{x}) \le 1$ for every $\bar{x} \in \mathbb{R}^n$.\\
	\begin{commentary}
		Step 2 : Construction of $\psi_j$.
	\end{commentary}
	\begin{align*}
		\text{Define, } \psi_1 & = \phi_1 \\
		\psi_2 & = (1-\phi_1)\phi_2\\
		& \vdots\\
		\psi_s & = (1-\phi_1)(1-\phi_2) \dotsm (1-\phi_{s-1})\phi_s
	\end{align*}
	Clearly, $0 \le \psi_j \le 1$ for $j = 1,2,\dots,s$.
	And $\psi_j$ has a compact support in $W(\bar{x}_j) \subset V_{\alpha(\bar{x}_j)}$.\\
	\begin{commentary}
		Step 3 : $\{\psi_j\}$ is a partition of unity.
	\end{commentary}

	We claim that,
	\begin{equation}
		\psi_1+\psi_2 + \dotsb+\psi_j = 1 - (1-\phi_1)(1-\phi_2) \dotsm (1-\phi_j),\ j = 1,2,\dots,s
	\end{equation}
	It is proved using mathematical induction on $j$.
	Clearly, it is true for $j = 1$.
	$\psi_1 = \phi_1 = 1 - 1 + \phi = 1 - (1-\phi)$.
	Suppose the claim is true for $j = k$ for some $1 \le k < s$.
	Then we have, $\psi_1 + \psi_2 + \dotsb + \psi_k = 1-(1-\phi_1)(1-\phi_2) \dotsm (1-\phi_k)$.
	\begin{align*}
		\text{Thus, }\psi_1 + \psi_2 + \dotsb + \psi_k + \psi_{k+1} = & 1-(1-\phi_1)(1-\phi_2) \dotsm (1-\phi_k) \\
		& + (1-\phi_1)(1-\phi_2) \dotsm (1-\phi_k)\phi_{k+1}\\
		= & 1-(1-\phi_1)(1-\phi_2) \dotsm (1-\phi_{k+1})
	\end{align*}
	Thus, the claim is true for $j = 1,2,\dots,s$.
	Let $\bar{x} \in K$.
	Then $\bar{x} \in B(\bar{x}_j)$ for some $j$.
	By definition of $\phi_j$, $\phi_j(\bar{x}) = 1$.
	That is, $(1-\phi_j(\bar{x})) = 0$.
	\begin{align*}
		\implies (1-\phi_1(\bar{x}))(1-\phi_2(\bar{x})) \dotsm (1-\phi_s(\bar{x})) & = 0 \\
		\implies \psi_1(\bar{x}) + \psi_2(\bar{x}) + \dotsb + \psi_s(\bar{x}) & = 1
	\end{align*}
	Therefore, $\psi_1(\bar{x}) + \psi_2(\bar{x}) + \dotsb + \psi_s(\bar{x}) = 1$ for every $\bar{x} \in K$.
\end{proof}

\begin{definition}
	By theorem, every open cover $\{V_\alpha\}$ of a compact subset $K$ has a partition of unity $\{ \psi_j \}$ with compact support in a finite subcover $\{V_{\alpha_j}\}$.
	Then $\{ \psi_j \}$ is \textbf{subordinate} to the cover $\{V_\alpha\}$.
\end{definition}

\begin{corollary}
	If $f \in \mathscr{C}(\mathbb{R}^n)$ and the support of $f$ lies in $K$,
	then\\ $f = \sum\limits_{i=1}^s \psi_i f$. Each $\psi_i$ has its support in some $V_\alpha$.

	\begin{commentary}
		Any continuous function with support in a compact set can be represented as sum of continuous functions $\psi_j f$ with small supports.
	\end{commentary}
\end{corollary}
\begin{proof}
	Let $K$ be compact subset of $\mathbb{R}^n$.
	And support of a function $f$ lies in $K$.
	Then,
	\begin{align*}
		f(\bar{x}) & = I(\bar{x}) f(\bar{x}) = \left( \sum_{j = 1}^s \psi_j(\bar{x}) \right) f(\bar{x}) \\
		& = \sum_{j = 1}^s \psi_j(\bar{x}) f(\bar{x}) = \sum_{j = 1}^s (\psi_j f)(\bar{x})
	\end{align*}
	Let $\{ \psi_j \}$ be a partition of unity.
	Let $K'$ be the support of $f$ and $V_{\alpha_j}$ be support of each $\psi_j$.
	Then $K' \cap V_{\alpha_j}$ is the support of each $\psi_j f$.
\end{proof}

\begin{theorem}[effect of change of variable on multiple integral]
	Suppose $T$ is a one-to-one $\mathscr{C}'$-mapping of an open set $E \subset \mathbb{R}^k$ into $\mathbb{R}^k$ such that $J_T(\bar{x}) \ne 0$ for all $\bar{x} \in E$.
	If $f$ is a continuous function on $\mathbb{R}^k$ whose support is compact and lies in $T(E)$, then
	\begin{equation}
		\int_{\mathbb{R}^k} f(\bar{y})\ d\bar{y} = \int_{\mathbb{R}^k} f(T(\bar{x}))|J_T(\bar{x})|\ d\bar{x}
	\end{equation}
\end{theorem}
\begin{proof}
	\begin{commentary}
		Step 1 : `separable' functions
	\end{commentary}

	Let $E$ be an open subset of $\mathbb{R}^k$.
	We claim that\footnote{This proof is my own work. There may exist simpler proofs.}
	the theorem is true for functions $h : E \to \mathbb{R}$ of the form $h(\bar{y}) = h_1(y_1)h_2(y_2) \dotsm h_k(y_k)$.
	We know that, every continuous function $T$ is locally a composition of primitives and flips.
	Therefore it is enough to prove that the theorem is true for functions $h$ with transformations $T$ primitives, flips and their compositions.\\
	\begin{commentary}
		Step 2 : Trasformation $T$ is a primitive
	\end{commentary}

	Let $G$ be a primitive with change in $m$th coordinate.
	Then
	\begin{equation}
		G(\bar{x}) = x_1\bar{e}_1 + x_2\bar{e}_2 + \dotsm + g(\bar{x})\bar{e}_m + \dotsm + x_k\bar{e}_k
	\end{equation}
	We have,
	\begin{equation}
		J_G(\bar{x}) = \begin{vmatrix}
			1 & 0 & \dots & 0 & \dots & 0 & 0 \\
			0 & 1 & \dots & 0 & \dots & 0 & 0 \\
			\vdots & \vdots & \ddots & \vdots & \iddots & \vdots & 0 \\ 
			D_1g(\bar{x}) & D_2g(\bar{x}) & \dots & D_mg(\bar{x}) & \dots & D_{k-1}g(\bar{x}) & D_kg(\bar{x}) \\
			\vdots & \vdots & \iddots & 0 & \ddots & \vdots & \vdots \\
			0 & 0 & \dots & 0 & \dots & 1 & 0 \\
			0 & 0 & \dots & 0 & \dots & 0 & 1 
		\end{vmatrix}
	\end{equation}
	Therefore, $J_G(\bar{x}) = D_mg(\bar{x})$.
	\begin{align*}
		\int_{\mathbb{R}^k} h(\bar{y})\ d\bar{y} & = \prod_{j = 1}^k \int h_j(y_j)\ dy_j\\
		& = \left( \prod_{j \ne m} \int h_j(y_j)\ dy_j \right) \int h_m(y_m)\ dy_m \\
		G(\bar{x}) = \bar{y} \implies & (x_1,x_2,\dots,g(\bar{x}),\dots,x_k) = (y_1,y_2,\dots,y_m,\dots,y_k)\\
		\implies & x_j = y_j \text{ for } j \ne m \text{ and } g(\bar{x}) = y_m\\
		\int_{\mathbb{R}^k} h(\bar{y})\ d\bar{y} & = \left( \prod_{j \ne m} \int h_j(x_j)\ 1\ dx_j \right) \int h_m(g(\bar{x}))\ D_mg(\bar{x})\ dx_m \\
		\int_{\mathbb{R}^k} h(\bar{y})\ d\bar{y} & = \int_{\mathbb{R}^k} h(G(\bar{x}))\ |J_G(\bar{x})|\ d\bar{x}\\
	\end{align*}
	\begin{commentary}
		Step 3 : Transformation $T$ is a flip.
	\end{commentary}

	Let $B$ be a flip that interchanges $m$ and $n$ coordinates.
	When $m = n$, $B$ is an identity map and the theorem is trivially true.
	\begin{equation}
		B(\bar{x}) = x_1\bar{e}_1 + \dotsb + x_n\bar{e}_m + \dotsb +  x_m\bar{e}_n + \dotsb + x_k\bar{e}_k
	\end{equation}
	The Jacobian matrix is an identity matrix with $m$th and $n$th rows interchanged.
	For example,
	\begin{equation}
		J_B(\bar{x}) = \begin{vmatrix}
			0 & 1 & \dots & 0 \\
			1 & 0 & \vdots & 0 \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \dots &  1
		\end{vmatrix}
	\end{equation}
	Therefore, $J_B(\bar{x}) = \pm 1$.
	\begin{align*}
		\int_{\mathbb{R}^k} h(\bar{y})\ d\bar{y} & = \prod_{j = 1}^k \int h_j(y_j)\ dy_j\\
		& = \left( \prod_{j \ne m,n} \int h_j(y_j)\ dy_j \right) \int h_n(y_n)\ dy_n \int h_m(y_m)\ dy_m \\
		B(\bar{x}) = \bar{y} \implies & (x_1,\dots,x_n,\dots,x_m,\dots,x_k) = (y_1,\dots,y_m,\dots,y_n,\dots,y_k)\\
		\int_{\mathbb{R}^k} h(\bar{y})\ d\bar{y} & = \left( \prod_{j \ne m,n} \int h_j(x_j)\ dx_j \right) \int h_n(x_m)\ dx_n \int h_m(x_n)\ dx_m \\
		& = \int_{\mathbb{R}^k} h(B(\bar{x}))\ 1\ d\bar{x} = \int_{\mathbb{R}^k} h(B(\bar{x}))\ |J_B(\bar{x}|\ d\bar{x}
	\end{align*}

	\begin{commentary}
		Notice that, substituting $y_n = x_m$ gives $\int h_n(x_m)\ dx_n$.
		This is due to the fact that $\bar{y} = B(\bar{x})$ have $x_m\bar{e}_n$ in place of $y_n\bar{e}_n$.
		Therefore, parameter of $h_n$ is on the same axis $\bar{e}_n$.

		Step 4 : If the theorem is true for two transformations, then it is true for their composition.
	\end{commentary}

	Suppose the theorem is true for transformations $P$ and $Q$.
	And $S = P \circ Q$.
	Let $\bar{z} = P(\bar{y})$ and $\bar{y} = Q(\bar{x})$.
	Then $\bar{z} = S(\bar{x})$.
	Since $m(P)m(Q) = m(S)$, we have $J_P(\bar{y}) J_Q(\bar{x}) = J_S(\bar{x})$.
	\begin{align*}
		\int_{\mathbb{R}^k} f(\bar{z})\ d\bar{z} & = \int_{\mathbb{R}^k} f(P(\bar{y}))\ |J_P(\bar{y})|\ d\bar{y} \\
		& = \int_{\mathbb{R}^k} f(P(Q(\bar{x})))\ |J_P(Q(\bar{x}))|\ |J_Q(\bar{x})|\ d\bar{x} \\
		& = \int_{\mathbb{R}^k} f(S(\bar{x}))\ |J_S(\bar{x})|\ d\bar{x}
	\end{align*}
	Therefore, the theorem is true for their composition.

	Now for any function of the form $h(\bar{x}) = \prod\limits_{j = 1}^k h_j(x_j)$, the theorem is true for any continuous transformation $T$.
	Then it is true for the algebra of functions $\mathscr{A}$.
	And by Stone-Weierstrass theorem,\footnote{I haven't checked whether the application of Stone-Weierstrass theorem causes any problem. It is the same proof technique as we have seen in $L(f) = L'(f)$.}
	the theorem is true for every continuous function $f$.
\end{proof}

\subsection{Differential Forms}
\begin{definition}
	Suppose $E$ is an open set in $\mathbb{R}^n$.
	A \textbf{$k$-surface} in $E$ is a $\mathscr{C}'$-mapping $\Phi$ from a compact set $D \subset \mathbb{R}^k$ into $E$.
	$D$ is the \textbf{parameter domain} of $\Phi$.
\end{definition}

\begin{definition}
	Suppose $E$ is an open set in $\mathbb{R}^n$.
	A \textbf{differential form} of order $k \ge 1$ in $E$, a $k$-form in $E$ is a function $\omega$,
	symbolically represented by the sum
	\begin{equation} \omega = \sum a_{i1 \dotsm ik}(\bar{x})\ dx_{i1}\wedge dx_{i2} \wedge \dotsm \wedge dx_{ik}
	\end{equation}
	which assigns to each $k$-surface $\Phi$ in $E$ a number $\omega(\Phi) = \int_{\Phi} \omega $,
	according to the rule
	\begin{equation}
		\int_{\Phi} \omega = \int_D \sum a_{i1 \dotsm ik}(\Phi(\bar{u})) \frac{\partial(x_{i1},x_{i2},\dots,x_{ik})}{\partial(u_1,u_2,\dots,u_k)}d\bar{u}
	\end{equation}
	where $D$ is the paramter domain of $\Phi$ and
	\begin{equation}
	\frac{\partial(x_{i1},x_{i2},\dots,x_{ik})}{\partial(u_1,u_2,\dots,u_k)} = \begin{vmatrix}
		\frac{\partial x_{i1}}{\partial u_1} & \frac{\partial x_{i1}}{\partial u_2} & \dots & \frac{\partial x_{i1}}{\partial u_k} \\
		\frac{\partial x_{i2}}{\partial u_1} & \frac{\partial x_{i2}}{\partial u_2} & \dots & \frac{\partial x_{i2}}{\partial u_k} \\
		\vdots & \vdots & \ddots & \vdots \\
		\frac{\partial x_{ik}}{\partial u_1} & \frac{\partial x_{ik}}{\partial u_2} & \dots & \frac{\partial x_{ik}}{\partial u_k} \\
	\end{vmatrix}
	\end{equation}
\end{definition}
\begin{commentary}
	Let $\omega = 4\ dx_1 \wedge dx_3 + 3x_2^2\ dx_2 \wedge dx_1$.
	Then $\omega$ is a $2$-form with $a_{1,3}(x,y,z) = 4$, $a_{2,1}(x,y,z) = 3y^2$ and all other $a_{i_1 i_2}(x,y,z) = 0$.

	Consider the upper hemi-sphere of unit radius, $S^2_{y \ge 0}$ in $\mathbb{R}^3$ and the closure of the unit disc $\bar{S}^1 = D \subset \mathbb{R}^2$.
	Then $D$ is compact.
	Let $\Phi : D \to S^2_{y \ge 0}$ defined by $\Phi(x,y) = (x,+\sqrt{1-x^2-y^2},y)$.
	Clearly, $\Phi \in \mathscr{C}'(D)$.
	Thus, $\Phi$ is a $2$-surface with parameter domain $D$.

	Clearly, $\Phi_1(x,y) = x$, $\Phi_2(x,y) = \sqrt{1-x^2-y^2}$ and $\Phi_3(x,y) = y$.
	We have, $\omega(\Phi) = \int_\Phi \omega$.
	\begin{equation}
		\omega(\Phi) = \int_\Phi \omega =  \iint_D \omega_\Phi(x,y) \frac{\partial ( \Phi_{i_1}, \Phi_{i_2})}{\partial ( x, y) } dx\ dy
	\end{equation}
	where $\omega_{\Phi}(x,y)$ is obtained by evaluating each function $a_{i_1 i_2 \dotsm i_k}(\Phi(x,y))$.

	\begin{align*}
		\omega(\Phi) & = \iint_D a_{1,3}(\Phi(x,y)) \begin{vmatrix} \frac{\partial \Phi_1}{\partial x} & \frac{\partial \Phi_1}{\partial y} \\
		\frac{\partial \Phi_3}{\partial x} & \frac{\partial \Phi_3}{\partial y} \end{vmatrix} dx dy + \iint_D a_{2,1}(\Phi(x,y)) \begin{vmatrix} \frac{\partial \Phi_2}{\partial x} & \frac{\partial \Phi_2}{\partial y} \\
		\frac{\partial \Phi_1}{\partial x} & \frac{\partial \Phi_1}{\partial y} \end{vmatrix} dx dy \\
		& = \int_0^1 \int_0^{\sqrt{1-y^2}} 4 \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} dx dy \\
		& + \int_0^1 \int_0^{\sqrt{1-y^2}} 3(1-x^2-y^2) \begin{vmatrix} \frac{1}{2}\frac{-2x}{\sqrt{1-x^2-y^2}} & \frac{1}{2} \frac{-2y}{\sqrt{1-x^2-y^2}} \\ 1 & 0 \end{vmatrix} dx dy \\
		& = \int_0^1 \int_0^{\sqrt{1-y^2}} 4 dx dy + \int_0^1 \int_0^{\sqrt{1-y^2}} \frac{3}{\sqrt{1-x^2-y^2}} dx\ dy
	\end{align*}
\end{commentary}

\begin{remark}[example 1]
	Integrals of $1$-forms are line integrals.
	And $\omega(\gamma) = 0$ for every closed curve $\gamma$.\cite[10.12a]{rudin}
\end{remark}

\begin{remark}[example 2]
	Let $\gamma : [ 0,2\pi ] \to \mathbb{R}^2$ defined by $\gamma(t) = (a\cos t , b\sin t)$.
	Then $\gamma$ is a $1$-surface with parameter domain $[0,2\pi]$.\cite[10.12b]{rudin}
	Let $\omega$ be a $1$-form defined by $\omega = x dy$.
	Then 
	\begin{equation*}
		\omega(\gamma) = \int_\gamma \omega = \int_\gamma x\ dy = \int_0^{2\pi} ab\cos^2 t\ dt = \pi ab
	\end{equation*}
	Similarly, $\omega = ydx$ gives
	\begin{equation*}
		\omega(\gamma) = \int_\gamma \omega = \int_\gamma y\ dx = \int_0^{2\pi} -ab\sin^2 t\ dt = -\pi ab
	\end{equation*}
\end{remark}

\begin{remark}[example 3]
	Let $0 \le r \le 1$, $0 \le \theta \le \pi$ and $0 \le \phi \le 2\pi$.
	Then $D \subset \mathbb{R}^n$ defined by $\{(r,\theta,\phi)\}$ is compact.
	Define $\Phi : D \to \mathbb{R}^3$ by $\Phi(r,\theta,\phi) = (r\sin \theta \cos \phi, r\sin \theta \sin \phi, r \cos \theta)$.
	Then $\Phi$ is a $3$-surface in $\mathbb{R}^3$.
	We have
	\begin{equation*}
		J_\Phi(r,\theta,\phi) = \begin{vmatrix}
			\sin \theta \cos \phi & r\cos \theta \cos \phi & -r\sin \theta \sin \phi \\
			\sin \theta \sin \phi & r\cos \theta \sin \phi & r \sin \theta \cos \phi \\
			\cos \theta & -r\sin \theta & 0 
		\end{vmatrix} = r^2\sin \theta
	\end{equation*}
	Let $\omega = dx_1 \wedge dx_2 \wedge dx_3$.
	Then $\omega(\Phi) = \int_\Phi \omega = \int_D J_\Phi = \frac{4\pi}{3}$ is the volume of the unit ball $\Phi(D)$.
\end{remark}

\begin{remark}
	A $k$-form $\omega$ is of class $\mathscr{C}'$ or $\mathscr{C}''$ if the functions $a_{i1 \dotsm ik}$ are all of class $\mathscr{C}'$ or $\mathscr{C}''$.
	A $0$-form in $E$ is defined to be a continuous function in $E$.
	And $0$ is the only $k$-form in any open set $E \subset \mathbb{R}^n$.
\end{remark}

\begin{definition}
	Let $\omega = a(\bar{x})\ {dx_i}_1 \wedge {dx_i}_2 \wedge \dotsm \wedge {dx_i}_k$.
	Then \textbf{$\bar{\omega}$} is the $k$-form obtained by interchanging a pair subscripts of $\omega$.
	ie, $\bar{\omega} = a(\bar{x})\ {dx_i}_2 \wedge {dx_i}_1 \wedge \dotsm \wedge {dx_i}_k$. 
\end{definition}

\subsubsection{Elementary properties of $k$-forms}
\begin{definition}
	Let $E$ be an open set in $\mathbb{R}^n$.
	And $\Phi$ be a $k$-surface in $E$.
	And let $\omega_1,\omega_2$ be $k$-forms in $E$, then
	\begin{enumerate}
		\item $\omega_1 = \omega_2 \iff \omega_1(\Phi) = \omega_2(\Phi) \iff \int_\Phi \omega_1 = \int_\Phi \omega_2,\ \forall \Phi \in E$
		\item $\omega = 0 \iff \omega(\Phi) = 0 \iff \int_\Phi \omega = 0,\ \forall \Phi \in E$
		\item $k$-form Addition, $\omega_1+\omega_2$ \\
			$\omega = \omega_1 + \omega_2 \iff \omega(\Phi) = \omega_1(\Phi) + \omega_2(\Phi) \iff \int_\Phi \omega = \int_\Phi \omega_1 + \int_\Phi \omega_2$.
		\item Scalar multiplication, $c\omega$ \\
			$c\omega(\Phi) = c(\omega(\Phi)) \iff \int_\Phi c\omega = c\int_\Phi \omega$.
		\item Inverse $k$-form, $-\omega$ \\
			$-\omega(\Phi) = -(\omega(\Phi)) \iff \int_\Phi -\omega = -\int_\Phi \omega$
		\item $\bar{\omega} = -\omega$.
	\end{enumerate}
\end{definition}

\begin{remark}
	Let $\omega = a(\bar{x})\ {dx_i}_1 \wedge {dx_i}_2 \wedge \dotsm \wedge {dx_i}_k$. If $\bar{\omega} = \omega$, then $\omega = 0$. Since $\wedge$ is anticommutative.

	Thus, differential $k$-forms with repeated subscripts are $0$.
	For example, $\omega = dx_i \wedge dx_j \wedge dx_i = 0$.
\end{remark}

\begin{definition}
	Let $\bar{I} = (i_1,i_2,\dots,i_k)$ be an \textbf{increasing $k$-index}.
	That is, $1 \le i_1 \le i_2 \le \dotsb \le i_k \le n$.
	Then $dx_{\bar{I}}$ of the form $dx_{\bar{I}} = dx_{i1} \wedge dx_{i2} \dotsm dx_{ik}$ is \textbf{basic $k$-form} in $\mathbb{R}^n$.
\end{definition}

\begin{remark} List of all basic $k$-forms
	\begin{description}
		\item[$0$-form] $0$
		\item[$1$-forms] $dx_1, dx_2, \dots, dx_n$
		\item[$2$-forms] $dx_i \wedge dx_j$ for every $1 \le i,j \le n$.
		\item[$3$-forms] $dx_i \wedge dx_j \wedge dx_k$ for every $1 \le i,j,k \le n$.
	\end{description}
\end{remark}
	
\begin{remark}
	There are $(^n_k)$ basic $k$-forms in $\mathbb{R}^n$.

	Every $k$-form can be represented in terms of basic $k$-forms.
	For every $k$-tuple $(j_1,j_2,\dots,j_k)$, $dx_{j1} \wedge \dotsm \wedge dx_{jk} = \sigma(j_1,j_2,\dots,j_k)dx_{\bar{J}}$ where $\bar{J}$ is the increasing $k$-index obtained by interchanging pairs.
	And $\sigma$ maps odd permutations to $-1$ and even permutations to $1$.

	Standard representation of a $k$-form
	\begin{equation}
		\omega = \sum_I b_I(\bar{x})dx_I
	\end{equation}
	For example : $x_1\ dx_2 \wedge dx_1 - x_2\ dx_3 \wedge dx_2 + x_3\ dx_2 \wedge dx_3 + dx_1 \wedge dx_2 = (1-x)\ dx_1 \wedge dx_2 + (x_2+x_3)\ dx_2 \wedge dx_3$ is a 2-form in $\mathbb{R}^3$.
\end{remark}

\begin{commentary}
	$dx_1 \wedge dx_2 \wedge dx_3 = dx_2 \wedge dx_3 \wedge dx_1$,
	since $(1\ 2\ 3) \in A_3 \implies \sigma(1\ 2\ 3) = 1$.
	And, $dx_1 \wedge dx_2 \wedge dx_3 = -dx_1 \wedge dx_3 \wedge dx_2$,
	$(2\ 3) \notin A_3 \implies \sigma(2\ 3) = -1$.
	Here, $A_3$ is an alternating group of all even permutation on $\{1,2,3\}$.
\end{commentary}
%\chapter{The Lebesgue Theory}

