%Text Books : \cite{apostol}, \cite{rudin}

%Module 1:
%The Weirstrass theorem, other forms of Fourier series, the Fourier integral theorem, the exponential form of the Fourier integral theorem, integral transforms and convolutions, the convolution theorem for Fourier transforms.
%(Chapter 11 Sections 11.15 to 11.21 of \cite{apostol}) (20 hours.)
%Module 2:
%Multivariable Differential Calculus, The directional derivative, directional derivatives and continuity, the total derivative, the total derivative expressed in terms of partial derivatives, An application of complex- valued functions, the matrix of a linear function, the Jacobian matrix, the matrix form of the chain rule. Implicit functions and extremum problems, the mean value theorem for differentiable functions,
%(Chapter 12 Sections. 12.1 to 12.11 of \cite{apostol}) (22 hours.)
%Module 3: 
%A sufficient condition for differentiability, a sufficient condition for equality of mixed partial derivatives, functions with non-zero Jacobian determinant, the inverse function theorem ,the implicit function theorem, extrema of real- valued functions of one variable, extrema of real-valued functions of several variables.
%Chapter 12 Sections-. 12.12 to 12.13 of \cite{apostol} 
%Chapter 13 Sections-. 13.1 to 13.6 of \cite{apostol} (28 hours.)
%Module 4:
%Integration of Differential Forms Integration, primitive mappings, partitions of unity, change of variables, differential forms.
%(Chapter 10 Sections. 10.1 to 10.14 of \cite{rudin}) (20 hours)

%Module 1 - \cite{apostol} 11
%Module 2 - \cite{apostol} 12
%Module 3 - \cite{apostol} 12, 13
%Module 4 - \cite{rudin} 10
%\cite{apostol}
%\chapter{The Real and Complex Number Systems*}
%\chapter{Some Basic Notations of Set Theory*}
%\chapter{Elements of Point Set Topology*}
%\chapter{Limits and Continuity*}
%\chapter{Derivatives*}
%\chapter{Functions of Bounded Variation and Rectifiable Curves*}
%\chapter{The Riemann-Stieltjes Integral*}
%\chapter{Infinite Series adn Infinite Products*}
%\chapter{Sequences of Functions*}
%\chapter{The Lebesgue Integral*}

%\chapter{Fourier Series and Fourier Integrals}
\section{Integral Transforms}
\subsection{The Weierstrass Approximation Theorem}
\begin{important}
Every continuous, real valued function on a compact interval has a polynomial approximation.\cite[Theorem 11.17]{apostol}
\end{important}
\begin{theorem}[Weierstrass]
Let $f$ be a real-valued, continuous function on a compact interval $[a,b]$.
Then for every \(\epsilon > 0\), there is a polynomial $p$ such that \(|f(x)-p(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{theorem}
\begin{synopsis}
Given a real-valued continuous function on compact interval $[a,b]$, we can construct a real-valued, continous function $g$ on $\mathbb{R}$ which is periodic with period $2\pi$.
We have, if \(f \in L(I)\) and $f$ is bounded almost everywhere in $I$, then \(f \in L^2(I)\).\cite[Theorem 10.52]{apostol}.
By Fejer's theorem (\cite[Theorem 11.15]{apostol}), the fourier series generated by $g$ (\cite[definition 11.3]{apostol}) converges to the Cesaro sum (\cite[Definition 8.47]{apostol}), which is $g$ itself in this case.
Thus for any \(\epsilon > 0\), there is a finite sum of trignometric functions.
The power series expansions of trignometric functions (\cite[definition 9.27]{apostol}) being uniformly convergent, there exists a polynomial $p_m$ which approximates $g$.
And we can construct $p$ (polynomial approximation of $g$) using $p_m$.
\end{synopsis}
\begin{proof}
Define \(g : \mathbb{R} \to \mathbb{R}\), \[ g(t) = \begin{cases} f(a+(b-a)t/\pi),\ t \in [0,\pi) \\ f(a+(2\pi-t)(b-a)/\pi),\ t \in [\pi,2\pi] \\ g(t - 2n\pi),\ t > 2\pi,\ n \in \mathbb{N} \\ g(t+2n\pi),\ t < 0,\ n \in \mathbb{N} \end{cases}\]

Thus $g$ is a continuous, real-valued, periodic function with period $2\pi$ such that
\begin{equation}
	f(x) = g\left(\frac{\pi (x-a)}{b-a}\right),\ x \in [a,b] \label{equ:fx}
\end{equation}

The fourier series generated by $g$ is given by, \[ g(t) \sim \frac{a_0}{2} + \sum_{k=1}^\infty \left( a_k \cos kt + b_k \sin kt \right)\] \[ \text{ where } a_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos kt\ dt,\ b_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin kt\ dt\]

Let \(\sequence{s_n(t)}\) be the sequence of partial sums of the fourier series generated by $g$.
And \( \sequence{\sigma_n(t)}\)  be the sequence of averages of $s_n(t)$ given by, \[\sigma_n(t) = \frac{1}{n} \sum_{k = 1}^n s_k(t),\text{ where } s_k(t) = \frac{a_0}{2} + \sum_{j = 1}^k \left( a_j \cos jt + b_j \sin jt \right)\]

Function \(f \in L(I)\) being real-valued continuous function on a compact interval, it is bounded and hence is Lebesgue square integrable. ie, \(f \in L^2(I)\).
Thus, \(g \in L^2(I)\).

Since $g$ is continous on $\mathbb{R}$, the function \(s : \mathbb{R} \to \mathbb{R}\) defined by, \[ s(t) = \lim_{h \to 0^+} \frac{g(t+h)-g(t-h)}{2} \] is well-defined on $\mathbb{R}$ and \(s(t) = g(t),\ \forall t \in \mathbb{R}\).

Then by Fejer's Theorem, the sequence \(\sequence{\sigma_n(t)}\) converges uniformly to $g(t)$ for every \(t \in \mathbb{R}\).
Thus, given \(\epsilon > 0\), there exists \(N \in \mathbb{N}\) such that \(\forall t \in \mathbb{R}\), \(|g(t)-\sigma_N(t)| < \frac{\epsilon}{2}\).

We have,
\begin{equation}
	\sigma_N(t) = \sum_{k=0}^N \left(A_k \cos kt + B_k \sin kt \right),\text{ where } A_k, B_k \in \mathbb{R}
	\label{equ:sigmaN}
\end{equation}
By the power series expansion of the trignometric functions about origin,
\begin{equation}
	\cos kt = \sum_{j = 1}^\infty \left(\frac{\cos^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty A_j' t^j \text{ where } A_j' \in \mathbb{R}
	\label{equ:coskt}
\end{equation}
\begin{equation}
	\sin kt = \sum_{j = 1}^\infty \left(\frac{\sin^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty B_j' t^j \text{ where } B_j' \in \mathbb{R}
	\label{equ:sinkt}
\end{equation}

Since the above power series expansions of trignometric functions are uniformly convergent, their finite linear combination \(\sequence{\sigma_N(t)}\) is also uniformly convergent.
ie, Given \(\epsilon > 0\) there exists \(m \in \mathbb{N}\) such that for every \(t \in \mathbb{R}\)
\[\left|\sum_{k = 0}^m C_k t^k - \sigma_N(t)\right| < \frac{\epsilon}{2} \text{ where } C_k \in \mathbb{R}\]

Therefore, \(| p_m(t) - g(t)| \le | p_m(t) - \sigma_N(t) | + |\sigma_N(t) - g(t)| < \epsilon\) where \(p_m(t) = \sum_{k = 0}^m C_k t^k\).
Define \(p : [a,b] \to \mathbb{R}\) by,
\begin{equation}
	p(x) = p_m\left( \frac{\pi(x-a)}{b-a} \right)
	\label{equ:px}
\end{equation}

By equations \ref{equ:fx} and \ref{equ:px}, \(|p(x)-f(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{proof}

\subsection{Other Forms of Fourier Series}

Let \(f \in L([0,2\pi])\), then the fourier series generated by $f$ is given by,
\[ f(x) \sim \frac{a_0}{2}+\sum_{n=1}^\infty \left( a_n \cos nx + b_n \sin nx \right) \]
\[ \text{ where } a_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos nt\ dt,\qquad b_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin nt\ dt \]

By Euler's forumula \(e^{inx} = \cos nx + i\sin nx\).
We have, \(\cos nx = \frac{(e^{inx}+e^{-inx})}{2}\) and \(\sin nx = \frac{(e^{inx}-e^{-inx})}{2i}\)

\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( \alpha_n e^{inx} + \beta_n e^{-inx} \right) \]

\[ \text{ where } \alpha_n = \frac{(a_n - ib_n)}{2} \qquad \beta_n = \frac{(a_n+ib_n)}{2} \]

Therefore, by assigning \(\alpha_0 = a_0/2\), \(\alpha_{-n} = \beta_n\), we get the following exponential form of fourier series generated by $f$,

\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^{inx} \text{ where } \alpha_n = \frac{1}{2\pi} \int_0^{2\pi} f(t)\ e^{-int}\ dt \]

Note : If $f$ is periodic with period $2\pi$, then the interval of integration $[0,2\pi]$ can be replaced with any interval of length $2\pi$.
eg. $[-\pi,\pi]$

\subsubsection{Periodic with period $p$}
Let \(f \in L([0,p])\) and $f$ is periodic with period $p$.
Then
\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( a_n \cos \frac{2\pi nx}{p} + b_n \sin \frac{2\pi nx}{p} \right) \]
\[ \text{ where } a_n = \frac{2}{p} \int_0^p f(t) \cos \frac{2\pi nt}{p}\ dt \qquad b_n = \frac{2}{p} \int_0^p f(t) \sin \frac{2\pi nt}{p}\ dt \]
Therefore, we have the exponential form of the above fourier series given by,
\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^\frac{2\pi inx}{p},\text{ where } \alpha_n = \frac{1}{p} \int_0^p f(t)\ e^\frac{-2\pi int}{p}\ dt \]
	
\subsection{Fourier Integral Theorem}
\begin{theorem}[Fourier Integral Theorem]
Let \(f \in L(-\infty,\infty)\).
Suppose \(x \in \mathbb{R}\) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
\begin{enumerate}
	\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
	\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals \[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \] exists.
\end{enumerate}
Then, 
\[ \frac{f(x+)+f(x-)}{2} = \frac{1}{\pi} \int_0^\infty \int_{-\infty}^\infty f(u)\cos v(u-x)\ du\ dv, \] the integral $\int_0^\infty$ being an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\[ f(x+t)\frac{\sin \alpha t}{\pi t} dt \to f(u)\frac{\sin \alpha(u-x)}{\pi(u-x)} \to \frac{f(u)}{\pi} \int_0^\alpha \cos v(u-x) dv \]
By Riemann-Lebesgue lemma\cite[Theorem 11.6]{apostol},
\[ f \in L(I) \implies \lim_{\alpha \to +\infty} \int_I f(x) \sin \alpha t\ dt = 0 \]
By Jordan's Theorem\cite[Theorem 10.8]{apostol}, if $g$ is of bounded variation on $[0,\delta]$, then
\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
By Dini's Theorem\cite[Theorem 10.9]{apostol}, if the limit $g(x+)$ exists and Lebesgue integral \( \int_0^\delta \frac{g(t)+g(0+)}{t} dt \) exists for some \( \delta > 0 \), then
\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
The order of Lebesgue integrals can be interchanged.\cite[Theorem 10.40]{apostol}

Suppose \(f \in L(X)\) and \(g \in L(Y)\).
Then \[ \int_X f(x) \left(\int_Y g(y) k(x,y) dy \right) dx = \int_Y g(y) \left( \int_X f(x) k(x,y) dx \right) dy \]
\end{synopsis}
\begin{proof}
	Consider \( \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \).
	We prove that this integral is equal to the either sides.
	\[ \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt = \int_{-\infty}^{-\delta} + \int_{-\delta}^0 + \int_0^{-\delta}  + \int_{\delta}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \] 
	We have, function \( \frac{f(x+t)}{\pi t} \) is bounded on \( (-\infty,-\delta)\cup(\delta,\infty) \), hence \( \frac{f(x+t)}{\pi t} \) is Lebesgue integrable on \( (-\infty,-\delta) \cup (\delta,\infty) \).
	
	By Riemann Lebesgue lemma, 
	\[ \frac{f(x+t)}{\pi t} \in L(-\infty,-\delta) \implies \int_{-\infty}^{-\delta} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0, \]
	\[ \frac{f(x+t)}{\pi t} \in L(\delta,\infty) \implies \int_{\delta}^{\infty} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0 \]

	\paragraph{Case 1}
	Suppose $f$ is of bounded variation on $[x-\delta,x+\delta]$, put \( g(t) = f(x+t) \) then $g$ is of bounded variation on $[-\delta,\delta]$.
	Thus $g$ is of bounded variation on $[0,\delta]$.
	Then by Jordan's Theorem
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	\paragraph{Case 2}
	Suppose both the limits $f(x+)$ and $f(x-)$ exists and both Lebesgue integrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and } \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \]
	exists.

	Thus, we have $f(x+)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \) exists.
	Put \( g(t) = f(x+t) \), then \( g(0+) = f(x+) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	Similarly, $f(x-)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \) exists.
	Put \( g(t) = f(x-t) \), then \( g(0+) = f(x-) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\begin{align*}
		\lim_{\alpha \to +\infty} \frac{2}{\pi}\int_{-\delta}^0 f(x+t)\frac{\sin \alpha t}{t} dt 
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta f(x-\tau) \frac{\sin \alpha \tau}{\tau} d\tau\\
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(\tau) \frac{\sin \alpha \tau}{\tau} d\tau = g(0+) = f(x-)
	\end{align*}

	Then by either cases,
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt  
		& = \lim_{\alpha \to +\infty} \int_{-\delta}^0 + \int_0^\delta f(x+t) \frac{\sin \alpha t}{\pi t} dt \\
		& = \frac{f(x+)+f(x-)}{2}
	\end{align*}

	We have, \( \int_0^\alpha \cos v(u-x) dv = \frac{\sin v(u-x)}{u-x} \).
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt 
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \frac{ \sin \alpha (u-x)}{u-x} du,\ (\text{put }u = x+t)\\
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \left( \int_0^\alpha \cos v(u-x) dv \right) du\\
		& = \lim_{\alpha \to +\infty} \int_0^\alpha \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv,\\
		& \text{since, the order of Lebesgue integrals can be reversed.}\\
		& = \int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv\\
		\text{where, } \int_0^\infty \text{ is not }& \text{a Lebesgue integral, but an improper Riemann integral }
	\end{align*}
	Therefore,
	\begin{align*}
		\int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt \\
		& =  \frac{f(x+)+f(x-)}{2}
	\end{align*}
\end{proof}

\begin{remark}
	If a function $f$ on $(-\infty,\infty)$ is non-periodic, then it may not have a fourier series represenation.
	In such cases, we have fourier intergral representaion.
\end{remark}

\subsection{Exponential form of Fourier Integral Theorem}
Let \( f \in L(-\infty,\infty) \).
Suppose \( x \in \mathbb{R} \) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
\begin{enumerate}
	\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
	\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \] exists.
\end{enumerate}
Then, \[ \frac{f(x+)+f(x-)}{2} = \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)}\ du\right) dv \]
\begin{proof}
Let \( F(v) = \int_{-\infty}^\infty f(u) \cos v(u-x) du \).
Then \( F(v) = F(-v) \) and 
\begin{align*}
	\lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) dv
	& = \lim_{\alpha \to \infty} \frac{1}{\pi} \int_0^\alpha \int_{-\infty}^\infty f(u) \cos v(u-x) du dv\\
	& = \frac{f(x+)+f(x-)}{2}
\end{align*}
Let \( G(v) = \int_{-\infty}^\infty f(u) \sin v(u-x) du \).
Then \( G(v) = -G(-v) \) and
\[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha G(v) dv = 0 \]
Thus \[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) + iG(v) dv = \frac{f(x+)+f(x-)}{2} \]
\end{proof}

\subsection{Integral Transforms}
\begin{definition}
	Integral transform $g(y)$ of $f(x)$ is a Lebesgue integral or Improper Riemann integral of the form
	\[ g(y) = \int_{-\infty}^\infty K(x,y) f(x)\ dx \], where $K$ is the kernal of the transform.
	We write \( g = \mathscr{K}(f) \).
\end{definition}

\begin{remark}
	Integral transforms(operators) are linear operators.
	 ie, \( \mathscr{K}(af_1 + bf_2) = a\mathscr{K}f_1 + b\mathscr{K}f_2 \)
\end{remark}

\begin{remark} A few commonly used integral transforms,
\begin{enumerate}
	\item Exponential Fourier Transform $\mathscr{F}$,
		\[ \mathscr{F}f = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	\item Fourier Cosine Transform $\mathscr{C}$,
		\[ \mathscr{C}f = \int_0^\infty \cos xy f(x)\ dx \]
	\item Fourier Sine Transform $\mathscr{S}$,
		\[ \mathscr{S}f = \int_0^\infty \sin xy f(x)\ dx \]
	\item Laplace Transform $\mathscr{L}$,
		\[ \mathscr{L}f = \int_0^\infty e^{-xy} f(x)\ dx \]
	\item Mellin Transform $\mathscr{M}$,
		\[ \mathscr{M}f = \int_0^\infty x^{y-1}f(x)\ dx \]
\end{enumerate}
\end{remark}

\begin{remark} Suppose \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_{-\infty}^\infty e^{-ixy}f(x)\ dx = \int_0^\infty e^{-ixy}f(x)\ dx = \int_0^\infty \cos xy \ f(x)\ dx + i \int_0^\infty \sin xy \ f(x)\ dx \]
	\[ \mathscr{F}f = \mathscr{C}f + i\mathscr{S}f \]

	Therefore Fourier Cosine $\mathscr{C}$ and Sine $\mathscr{S}$ transforms are special cases of fourier integral transform, $\mathscr{F}$ provided $f$ vanishes on negative real axis.
\end{remark}

\begin{remark} Let \( y = u+iv \), \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_0^\infty e^{-xy}f(x) = \int_0^\infty e^{-xu}e^{-ixv}f(x)\ dx = \int_0^\infty e^{-ixv} \phi_u(x) dx \]
	where \( \phi_u(x) = e^{-xu}f(x) \).
	\[ \mathscr{L}f = \mathscr{F}\phi_u \]
	Therefore Laplace transform, $\mathscr{L}$ is a special case of Fourier integral transform, $\mathscr{F}$.
\end{remark}

\begin{remark} Let \( g(y) = \mathscr{F}f(x) \).
	\[ g(y) = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	Suppose $f$ is continuous at $x$, then by fourier integral theorem,
	\begin{align*}
		f(x)	& = \frac{1}{2\pi} \int_{-\infty}^\infty \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)} du \right) dv\\
			& = \int_{-\infty}^\infty e^{-ivx} \left( \frac{1}{2\pi} \int_{-\infty}^\infty e^{ivu} f(u)\ du \right) dv\\
			& = \int_{-\infty}^\infty g(v) e^{-ivx} dv = \mathscr{F}g \text{ where } g(v) = \frac{1}{2\pi}\int_{-\infty}^\infty f(u) e^{ivu} du 
	\end{align*}
	The above function $g(v)$ gives the \textbf{inverse fourier transformation} of $f$.

	Let $g$ be fourier transform of $f$, then $f$ is uniquely determined by its fourier transform $g$ by,
	\[ f(x) = \mathscr{F}^{-1}g(y) = \frac{1}{2\pi} \lim_{\alpha \to \infty} \int_{-\alpha}^\alpha g(y) e^{ixy} dy \]
\end{remark}

\begin{enumerate}
	\setcounter{enumi}{5}
	\item Inverse Fourier Transform $\mathscr{F}^{-1}$,
		\[ \mathscr{F}^{-1}f = \int_{-\infty}^\infty \frac{e^{ixy}}{2\pi}f(x)\ dx \]
\end{enumerate}

\subsection{Convolutions}
\begin{definition}
	Let \( f,g \in L(-\infty,\infty) \).
	Let $S$ be the set of all points $x$ for which the Lebesgue integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists.
	Then the function \( h : S \to \mathbb{R} \) is a convolution of $f$ and $g$.
	And \( h = f \ast g \).
\end{definition}

\begin{remark}
	Convolution operator is commutative.
	ie, \( h = f \ast g = g \ast f \)
	\begin{commentary}
		(hint : take $u = x-t$)
	\end{commentary}
	% $u = x-t \implies du = -dt$
	% sign is reversed as the order of limits are switched.
	% ie, $t = \infty \to u = -\infty$
\end{remark}

\begin{remark}
	Suppose $f,g$ vanishes on negative real axis, then
	\[ h(x) = \int_{-\infty}^\infty f(t)\ g(x-t)\ dt = \int_{-\infty}^0 + \int_0^x  + \int_x^\infty f(t)\ g(x-t)\ dt = \int_0^x f(t)\ g(x-t)\ dt \] 
\end{remark}

\begin{remark}
	Singularity of convolution is a point at which the convolution integral fails to exists.
\end{remark}

\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and either $f$ or $g$ is bounded in $\mathbb{R}$.
	Then the convoluton integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists for every \( x \in \mathbb{R} \) and the function $h$ so defined is bouned in $\mathbb{R}$.
	In addition, if the bounded function is continuous on $\mathbb{R}$, then $h$ is continuous and \( h \in L(\mathbb{R}) \).
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}
	If $f,g$ are both unbounded, the convolution integral may not exist.
	\[ \text{ eg: } f(t) = \frac{1}{\sqrt{t}},\ g(t) = \frac{1}{\sqrt{1-t}} \]
\end{remark}

\begin{theorem}
	Let \( f,g \in L^2(\mathbb{R}) \).
	Then the convolution integral $f \ast g$ exists for each \( x \in \mathbb{R} \) and the function \( h : \mathbb{R} \to \mathbb{R} \) defined by \( h(x) = f \ast g (x) \) is bounded in $\mathbb{R}$.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\subsection{The Convolution Theorem for Fourier Tranforms}
\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and at least one of $f$ or $g$ is continuous and bounded on $\mathbb{R}$.
	Let \( h = f \ast g \).
	Then for every real $u$,
	\[ \int_{-\infty}^\infty h(x) e^{-ixu} dx = \left( \int_{-\infty}^\infty f(t) e^{-itu} dt \right) \left( \int_{-\infty}^\infty g(y) e^{-iyu} dy \right) \]
	The integral on the left exists both as a Lebesgue integral and an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}[Application of Convolution Theorem]
	\[ B(p,q) = \frac{\Gamma{p} \Gamma{q}}{\Gamma{p+q}},\text{ where } B(p,q) = \int_0^1 x^{p-1} (1-x)^{q-1} dx,\ \Gamma{p} = \int_0^\infty t^{p-1} e^{-t} dt \]
\end{remark}

\section{Multivariate Differential Calculus}

In this chapter, we deal with real functions of several variables.
Instead of $\mathbf{c}$, we write \( \overline{c} \in \mathbb{R}^n \), then \( \overline{c} = (c_1, c_2, \dotsc, c_n) \) where \( c_j \in \mathbb{R} \) for every \(j = 1,2, \dotsc, n\).
Again, suppose \(f : \mathbb{R}^n \to \mathbb{R}^m\) and \(f(\overline{x}) = \overline{y}\), then \(\overline{y} = (y_1, y_2, \dotsc, y_m)\) where each $y_k$ is real.
The unit co-ordinate vector, $\overline{u_k}$ is given by \( {u_k}_j = \delta_{j,k} \)

\subsection{Directional Derivative}
\textsl{Motivation : The existence of all partial derivatives of a multivariate real function $f$ at a point $\overline{c}$ doesn't imply the continuity of $f$ at $\overline{c}$.
	Thus, we need a suitable generalisation for the partial derivative which could characterise continuity.
	And directional derivative is such an attempt.}

\begin{definition}[Directional Derivative]
	Let \(S \subset \mathbb{R}^n\) and \(f : S \to \mathbb{R}^m\).
	Let $\overline{c}$ be an interior points of $S$ and \( \overline{u} \in \mathbb{R}^n \), then there exists an open ball $B(\overline{c},r)$ in $S$.
	Also for some $\delta > 0$ the line segment \( \alpha : [0,\delta] \to S \) given by \( \alpha(t) = \overline{c}+t\overline{u} \) lie in $B(\overline{c},r)$.
	
	Then the Directional derivative of $f$ at an interior point $\overline{c}$ in the direction $\overline{u}$ is given by
	\[ f'(\overline{c},\overline{u}) = \lim_{h \to 0} \frac{f(\overline{c}+h\overline{u}) - f(\overline{c})}{h} \]
\end{definition}

\begin{remark}
	The direction derivative of $f$ at an interior point $\overline{c}$ in the direction $\overline{u}$ exists only if the above limit exists.
\end{remark}

\begin{remark}Example, \cite[Exercise 12.2a]{apostol}

Suppose \(\overline{x},\overline{a},\overline{c},\overline{u} \in \mathbb{R}^n\).
Let \(f : \mathbb{R}^n \to \mathbb{R}\) such that \(f(\overline{x}) = \overline{a}\cdot\overline{x}\).
Then \[ f'(\overline{c},\overline{u}) = \lim_{h \to 0} \frac{\overline{a}\cdot(\overline{c}+h\overline{u}) - \overline{a}\cdot\overline{c}}{h} = \overline{a}\cdot\overline{u}\]
\end{remark}

\begin{remark}[Properties] Let \(f : S \to \mathbb{R}^m \), where \( S \subset \mathbb{R}^n\)
\begin{enumerate}
	\item \( f'(\overline{c},\overline{0}) = \overline{0} \)\\
	\textsl{Note : The zero vectors belongs to $\mathbb{R}^n, \mathbb{R}^m$ respectively.}
	\item \( f'(\overline{c},\overline{u_k}) = \frac{\partial f}{\partial u_k}(\overline{c}) = D_k f(\overline{c}) \), the $k^{th}$ partial derivative of $f$.
	\item Let \( f = (f_1, f_2, \cdots, f_m), \text{ such that } f(\overline{c}) = \left(f_1(\overline{c}),f_2(\overline{c}),\dotsc,f_m(\overline{c})\right) \).
	Then, 
	\[ \exists f'(\overline{c},\overline{u}) \iff \forall k, \exists f_k'(\overline{c},\overline{u}) \text{ and } f'(\overline{c},\overline{u}) = \left(f_1'(\overline{c},\overline{u}),f_2'(\overline{c},\overline{u}),\dotsc,f_m'(\overline{c},\overline{u})\right) \]
	ie, Directional derivative of $f$ exists iff directional derivative of each component function $f_k$ exists.
	And the components of the directional derivatives of $f$ are the directional derivaties of the components of $f$.

	Thus \( D_k f(\overline{c}) = \left(D_k f_1(\overline{c}),D_k f_2(\overline{c}),\dotsc,D_k f_m(\overline{c}) \right) \) holds.
	\item Let \( F(t) = f(\overline{c}+t\overline{u}) \), then \( F'(0) = f'(\overline{c},\overline{u}) \) and \( F'(t) = f'(\overline{c}+t\overline{u},\overline{u}) \)
	\item Let \( f(\overline{c}) = \overline{c}\cdot\overline{c} = \|\overline{c}\|^2 \), and \(F(t) = f(\overline{c}+t\overline{u}) \), then \( F'(t) = 2\overline{c}\cdot\overline{u}+2t\|\overline{u}\|^2 \) and \( F'(0) = f'(\overline{c},\overline{u}) = 2\overline{c}\cdot\overline{u} \)
	\item Let \(f\) be linear, then \( f'(\overline{c},\overline{u}) = f(\overline{u}) \)
	\item Existence of all partial derivatives doesn't imply existence of all directional derivatives.
	\[ f(x,y) = \begin{cases} x+y \qquad \text{ if } x = 0 \text{ or } y = 0 \\ 1 \qquad \qquad \text{otherwise} \end{cases} \]
	For above \(f\), directional derivatives exists only along the co\nobreakdash-ordinates (ie, partial derivatives).
	\item Existence of all directional derivatives doesn't imply continuity.
	\[ f(x,y) = \begin{cases} xy^2(x^2+y^4) \qquad x \ne 0 \\ 0 \hspace{2.5cm} x = 0 \end{cases} \]
	Above \(f\) is discontinuous at \((0,0)\), however all directional derivatives exists and has finite value.
	\end{enumerate}
\end{remark}

\subsection{Total Derivative}
We may define a total derivative \( T_c(h) = hf'(c) \) in the case of real-functions of single variable as follows :-

\[ \text{Let }E_c(h) = \begin{cases} \frac{f(c+h)-f(c)}{h} - f'(c),\qquad h \ne 0 \\ 0,\hspace{3.4cm} h = 0 \end{cases} \]
Then, \( f(c+h) = f(c) + hf'(c) + hE_c(h) \) and as \( h \to 0 \), \( E_c(h) \to 0\).
Also \( T_c(h) = f'(c)h \) is a linear function of $h$.
ie, \( T_c(ah_1+bh_2) = aT_c(h_1)+bT_c(h_2) \).
Now, we will define a total derivative of multivariate function that has these two properties.

\begin{definition}[Total Derivative]
The function \( f: \mathbb{R}^n \to \mathbb{R}^m \) is differentiable at $\overline{c}$ if there exists a \textbf{linear} function \( T_{\overline{c}} : \mathbb{R}^n \to \mathbb{R}^m \) such that \( f(\overline{c}+\overline{v}) = f(\overline{c}) + T_{\overline{c}}(\overline{v}) + \|\overline{v}\| E_{\overline{c}}(\overline{v}) \) where \( E_{\overline{c}}(\overline{v}) \to \overline{0} \) as \( \overline{v} \to \overline{0} \).
\end{definition}

The linear function $T_{\overline{c}}$ is the total derivative of $f$ at $\overline{c}$, \( T_{\overline{c}}(\overline{0}) = \overline{0} \) and the condition above gives the First Order Taylor's Formula for \( f(\overline{c}+\overline{v})-f(\overline{c}) \).

\begin{remark}[Properties] Let \( f : \mathbb{R}^n \to \mathbb{R}^m \) and \( f'(\overline{c})(\overline{v}) = T_{\overline{c}}(\overline{v}) \) be the total derivative of $f$ at $\overline{c}$ evaluated at $\overline{v}$.
Then, 
\begin{enumerate}
	\item \( f'(\overline{c})(\overline{v}) = f'(\overline{c},\overline{u}) \)
	\item If $f$ is differentiable at $\overline{c}$, then $f$ is continuous at $\overline{c}$.
	\item \( f'(\overline{c})(\overline{v}) = v_1 D_1 f(\overline{c}) + v_2 D_2 f(\overline{c}) + \dots + v_n D_n f(\overline{c}) \)
	\end{enumerate}
\end{remark}

\begin{note}
The above $f'$ is a function from $\mathbb{R}^n$ to the set of all linear functions \( \mathscr{L} = \{ h : \mathbb{R}^n \to \mathbb{R}^m\} \).
$f'(\overline{c})$ is a linear function (in fact, total derivative $T_{\overline{c}}$) which maps $\overline{v}$ into the directional derivatives of $f$ at $\overline{c}$ in the direction $\overline{v}$.
This notation generalises $f'$ for univariate $f$ as well.(put $n=m=1$)

In this subject, we use the following notations,
\begin{description}
	\item[$D_kf(\overline{c})$] partial derivative
	\item[$f'(\overline{c},\overline{v})$] directional derivative
	\item[$f'(\overline{c})(\overline{v})$] total derivative
	\item[$\nabla{}f(\overline{c})$] gradient vector
\end{description}
\end{note}

\begin{theorem}
If $f$ is differentiable at $\overline{c}$ with total derivative $T_{\overline{c}}$, then for every $\overline{u} \in \mathbb{R}^n$, $T_{\overline{c}}(\overline{u}) = f'(\overline{c},\overline{u})$.
( ie, \( f'(\overline{c})(\overline{v}) = f'(\overline{c},\overline{v}) \) )
\end{theorem}
\begin{proof}
For \( \overline{v} = \overline{0}$, we have $T_{\overline{c}}(\overline{0}) = 0 = f'(\overline{c},\overline{0}) \).

Suppose \( \overline{v} \ne \overline{0} \), then put \( \overline{v} = h\overline{u} \).
Since $f$ is differentiable at $\overline{c}$, $f$ has total derivative at $\overline{c}$.
That is, there exists a linear function $T_{\overline{c}}$ such that \( f(\overline{c}+h\overline{u}) = f(\overline{c}) + T_{\overline{c}}(h\overline{u}) + \|h\overline{u}\|E_{\overline{c}} (h\overline{u}) \) where $E_{\overline{c}}(h\overline{u}) \to \overline{0}$ as $h\overline{u} \to \overline{0}$.
\begin{align*}
	\implies  & f(\overline{c}+h\overline{u}) = f(\overline{c}) + hT_{\overline{c}}(\overline{u}) + |h|\|\overline{u}\|E_{\overline{c}} (h\overline{u}),\ E_{\overline{c}}(h\overline{u}) \to \overline{0} \text{ as } h\overline{u} \to \overline{0} \\
	\implies  & \frac{f(\overline{c}+h\overline{u}) - f(\overline{c})}{h} = T_{\overline{c}}(\overline{u}) + \frac{|h|\|\overline{u}\|E_{\overline{c}}(h\overline{u})}{h},\  E_{\overline{c}}(h\overline{u}) \to \overline{0} \text{ as } h \to 0 \\
	\implies  & \lim_{h \to 0} \frac{f(\overline{c}+h\overline{u}) - f(\overline{c})}{h} = T_{\overline{c}}(\overline{u}) + \lim_{h \to 0} \frac{|h|\|\overline{u}\|E_{\overline{c}}(h\overline{u})}{h} \\
	\implies & f'(\overline{c},\overline{u}) = T_{\overline{c}}(\overline{u})
\end{align*}
\end{proof}

\begin{note}
$T_{\overline{c}}$ is linear, however $E_{\overline{c}}$ is not linear.
Thus $E_{\overline{c}}(h\overline{u}) \ne h E_{\overline{c}} (\overline{u})$.

As $h \to 0$, $h\overline{u} \to \overline{0}$ and \( E_{\overline{c}}(h\overline{u}) \to \overline{0}$.
Since the order of the function \( E_{\overline{c}}(h\overline{u}) \) is much smaller than that of $h$, the limit on the right converges to 0.
\end{note}

\begin{theorem}
If $f$ is differentiable at \( \overline{c} \), then $f$ is continuous at \( \overline{c} \).
\end{theorem}
\begin{proof}
Let $\overline{v} \ne 0$, then
\begin{align*}
	\overline{v} = v_1 \overline{u_1} & + v_2 \overline{u_2} + \dots + v_n \overline{u_n},\\
	\overline{v} \to \overline{0} \implies & \forall j,\ v_j \to 0 \\
	T \text{ is linear }\implies & T_{\overline{c}} (\overline{v}) = v_1 T_{\overline{c}}(\overline{u_1}) + v_2 T_{\overline{c}}(\overline{u_2}) + \dots + v_n T_{\overline{c}}(\overline{u_n})\\
	\text{Thus, } & T_{\overline{c}}(\overline{v}) \to \overline{0} \text{ as } \overline{v} \to 0
\end{align*}
Since $f$ differentiable at \( \overline{c} \), there exists linear function $T_{\overline{c}}$ such that
\begin{align*}
	f(\overline{c}+\overline{v}) & =  f(\overline{c}) + T_{\overline{c}}(\overline{v}) + \|v\|E_{\overline{c}}(\overline{v}) \\
	\implies & \lim_{\overline{v} \to \overline{0}} f(\overline{c}+\overline{v}) = f(\overline{c}) + \lim_{\overline{v} \to \overline{0}} T_{\overline{c}}(\overline{v}) + \lim_{\overline{v} \to \overline{0}} \|v\|E_{\overline{c}}(\overline{v})\\
	\implies & \lim_{\overline{v} \to \overline{0}} f(\overline{c}+\overline{v}) = f(\overline{c})
\end{align*}
\end{proof}

\begin{theorem}
Let $S \subset \mathbb{R}^n$ and $f : S \to \mathbb{R}^m$ be differentiable at an interior point $\overline{c}$ of $S$, where $S \subseteq \mathbb{R}^n$.
If $\overline{v} = v_1\overline{u_1}+v_2\overline{u_2} + \cdots + v_n\overline{u_n}$, then
\[ f'(\overline{c})(\overline{v}) = \sum_{k=1}^n v_k D_k f(\overline{c}) \]
In particular, if $f$ is real-valued $(m = 1)$ we have, $f'(\overline{c})(\overline{v}) = \nabla{}f(\overline{c}).\overline{v}$
\end{theorem}
\begin{proof}
Suppose $f : S \to \mathbb{R}^m$ is differentiable at $\overline{c}$, then there exists a linear function $f'(\overline{c}) : S \to \mathbb{R}^m$ such that $f(\overline{c}+\overline{v}) = f(\overline{c}) + f'(\overline{c})(\overline{v}) + \|\overline{v}\| E_{\overline{c}}(\overline{c})$ where $E_{\overline{c}} \to \overline{0}$ as $\overline{v} \to \overline{0}$.
\begin{align*}
	f'(\overline{c})(\overline{v}) & = f'(\overline{c})\left(\sum_{k=1}^n v_k \overline{u_k}\right)\\
	& = \sum_{k=1}^n v_k f'(\overline{c})(\overline{u_k}), \text{ since $f'(\overline{c})$ is linear}\\
	& = \sum_{k=1}^n v_k D_k f(\overline{c}), \text{ since $f'(\overline{c})(\overline{u_k}) = f'(\overline{c},\overline{u_k}) = D_k f(\overline{c})$}\\
	\intertext{Let $m = 1$, then $f : S \to \mathbb{R}$ }
	f'(\overline{c})(\overline{v}) & = \sum_{k=1}^n v_k D_k f(\overline{c}) = \nabla{}f(\overline{c}).\overline{v}\\
	& \text{ since } \nabla{}f(\overline{c}) = \left( D_1f(\overline{c}),\ D_2f(\overline{c}),\ \cdots ,\ D_nf(\overline{c}) \right)
\end{align*}
\end{proof}

\begin{remark}
Let $f : S \to \mathbb{R}$, then $f(\overline{c} +\overline{v}) = f (\overline{c}) + \nabla{}f(\overline{c}).\overline{v} + o(\|\overline{v}\|)$ as $\overline{v} \to \overline{0}$.
\end{remark}

\begin{remark}[Complex-valued Functions]
\end{remark}

\subsection{Matrix of Linear Function}
Let $T : \mathbb{R}^n \to \mathbb{R}^m$ be a linear function.
Let $\{\overline{u_1},\ \overline{u_2},\ \cdots,\ \overline{u_n}\}$ be standard basis for $\mathbb{R}^n$ and  $\{\overline{e_1},\ \overline{e_2},\ \cdots,\ \overline{e_m}\}$ be standard basis for $\mathbb{R}^m$.
Let $\overline{v} \in \mathbb{R}^n$, then $\overline{v} = \sum_{k=1}^n v_k\overline{u_k}$ and $T(\overline{v}) = \sum_{k=1}^n v_k T(\overline{u_k})$ and
\begin{commentary}
\begin{align*}
	T(\overline{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} T(\overline{u_1}) \\ T(\overline{u_2}) \\ \cdots \\ T(\overline{u_n}) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}\overline{e_1}+t_{21}\overline{e_2}+\cdots+t_{m1}\overline{e_m} \\ t_{12}\overline{e_1}+t_{22}\overline{e_2}+\cdots+t_{m2}\overline{e_m} \\ \cdots \\ t_{1n}\overline{e_1}+t_{2n}\overline{e_2}+\cdots+t_{mn}\overline{e_m} \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} \overline{e_1} \\ \overline{e_2} \\ \cdots \\ \overline{e_m} \end{bmatrix} \\
	\intertext{ We may take the transpose,}
	T(\overline{v}) & = \begin{bmatrix} \overline{e_1} & \overline{e_2} & \cdots & \overline{e_m} \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \cdots & t_{1n} \\ t_{21} & t_{22} & \cdots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n  \end{bmatrix} 
\end{align*}
\end{commentary}
\[ T(\overline{v}) = T \left( \sum_{k=1}^n v_k \overline{u_k} \right) = \sum_{k=1}^n v_k T(\overline{u_k}) = \sum_{k=1}^n v_n \sum_{j=1}^m t_{kj}\overline{e_j} \]
Thus matrix of $T$ is given by, $m(T) = (t_{ik})$ where $T(\overline{u_k}) = \sum_{k=1}^n t_{ik}\overline{e_i}$.
\begin{commentary}
\begin{remark}[Example]
Let $T : \mathbb{R}^3 \to \mathbb{R}^2$ defined by $T(x,y,z)=(2x+y,y-z)$.
\begin{align*}
	T(1,2,3) = & T((1,0,0) + 2(0,1,0) + 3(0,0,1)) \\
	= & T(\overline{u_1}+2\overline{u_2}+3\overline{u_3}) \\
	= & T(\overline{u_1}) + 2T(\overline{u_2}) + 3T(\overline{u_3}) \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} T(\overline{u_1}) \\ T(\overline{u_2}) \\ T(\overline{u_3}) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} (2,0) \\ (1,1) \\ (0,-1) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2(1,0) \\ (1,0)+(0,1) \\ -1(0,1) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2\overline{e_1} \\ \overline{e_1}+\overline{e_2} \\ -\overline{e_2} \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 1 & 1 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} \overline{e_1} \\ \overline{e_2} \end{bmatrix} \\
	= & 4\overline{e_1}-\overline{e_2} = 4(1,0) - 1(0,1) = (4,-1)
\end{align*}
\[ \text{ In the above case, }m(T) = \begin{bmatrix} 2 & 0 \\ 1 & 1\\ 0 & -1 \end{bmatrix}\]
Using the matrix of linear function $m(T)$, we can compute the image of any point in $R^3$ by matrix multiplication.
\end{remark}
\end{commentary}

\subsubsection{Matrix of the composition of two linear functions}
Let $T : \mathbb{R}^n \to \mathbb{R}^m$ and $S : \mathbb{R}^m \to \mathbb{R}^p$ be two linear functions with domain of $S$ containing the range of $T$ (so that $S \circ T$ is well defined).
Then $S \circ T : \mathbb{R}^n \to \mathbb{R}^p$ is defined by
\[ S \circ T(\overline{x}) = S(T(\overline{x})),\ \forall \overline{x} \in \mathbb{R}^n\]
Since $S,T$ are linear, $S \circ T$ is also linear.
\begin{align*}
	S \circ T(a \overline{x} + b \overline{y}) = & S(T(a \overline{x} + b\overline{y})) = S(a T(\overline{x}) + b T(\overline{y})) = a S(T(\overline{x}))) + b S(T(\overline{y})) \\
	= & a S \circ T(\overline{x}) + b S \circ T(\overline{y}),\ \forall a,b \in \mathbb{R},\ \forall \overline{x},\overline{y} \in \mathbb{R}^n
\end{align*}
Let $\{\overline{u_1},\overline{u_2},\cdots,\overline{u_n}\}$ be the standards basis for $\mathbb{R}^n$, $\{\overline{e_1},\overline{e_2},\cdots,\overline{e_m}\}$ be the standards basis for $\mathbb{R}^m$ and $\{\overline{w_1},\overline{w_2},\cdots,\overline{w_p}\}$ be the standards basis for $\mathbb{R}^p$.
Let $\overline{v} \in \mathbb{R}^n$, then $\overline(v) = \sum_{i=1}^n v_i\overline{u_i}$, and $S \circ T(\overline{v})=\sum_{i=1}^n v_n S \circ T(\overline{u_i})$
\begin{commentary}
\begin{align*}
	S \circ T(\overline{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} S \circ T(\overline{u_1}) \\ S \circ T(\overline{u_2}) \\ \cdots \\ S \circ T(\overline{u_n}) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} S(t_{11}\overline{e_1} + \cdots + t_{m1}\overline{e_m}) \\ S(t_{12}\overline{e_1} + \cdots + t_{m2}\overline{e_m}) \\ \cdots \\ S(t_{1n}\overline{e_1} + \cdots + t_{mn}\overline{e_m}) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}S(\overline{e_1}) + \cdots + t_{m1}S(\overline{e_m}) \\ t_{12}S(\overline{e_1}) + \cdots + t_{m2}S(\overline{e_m}) \\ \cdots \\ t_{n1}S(\overline{e_1}) + \cdots + t_{mn}S(\overline{e_m}) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} S(\overline{e_1}) \\ S(\overline{e_2}) \\ \cdots \\ S(\overline{e_m}) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11}\overline{w_1}+s_{12}\overline{w_2}+\cdots+s_{1p}\overline{w_p} \\ s_{12}\overline{w_1}+s_{22}\overline{w_2}+\cdots+s_{p2}\overline{w_p} \\ \cdots \\ s_{1m}\overline{w_1}+ s_{2m}\overline{w_2}+\cdots+s_{pm}\overline{w_p} \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11} & s_{21} & \cdots & s_{p1} \\ s_{12} & s_{22} & \cdots & s_{p2} \\ \vdots & \vdots & \ddots & \vdots \\ s_{1m} & s_{2m} & \cdots & s_{pm} \end{bmatrix} \begin{bmatrix} \overline{w_1} \\ \overline{w_2} \\ \vdots \\ \overline{w_p} \end{bmatrix}\\
	\intertext{We may take transpose,}
	S \circ T(\overline{v}) & = \begin{bmatrix} \overline{w_1} & \overline{w_2} & \vdots & \overline{w_p} \end{bmatrix} \begin{bmatrix} s_{11} & s_{12} & \cdots & s_{1m} \\ s_{21} & s_{22} & \cdots & s_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ s_{p1} & s_{p2} & \cdots & s_{pm} \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \cdots & t_{1n} \\ t_{21} & t_{22} & \cdots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n  \end{bmatrix} 
\end{align*}
Remember : Given $T : \mathbb{R}^n \to \mathbb{R}^m$, then we may take $m(T)$ either as $m \times n$ matrix or $n \times m$ matrix.
Since, we chose $m \times n$, $m(S \circ T) = m(S)m(T)$.
Otherwise, $m(S \circ T) = m(T)m(S)$.
This may change for different authors.
\end{commentary}

Suppose $m(S) = (s_{ij})$ and $m(T) = (t_{ij})$ respectively.
Then
\[ S(e_k) = \sum_{i=1}^p s_{ik} \overline{w_i},\ k=1,2,\cdots,m \text{ and }\]
\[ T(u_j) = \sum_{k=1}^m t_{kj} \overline{e_k},\ j=1,2,\cdots,n \]
\begin{align*}
	(S \circ T)(\overline{u_j}) = & S(T(\overline{u_j})) = S\left(\sum_{k=1}^m t_{kj}\overline{e_k}\right) = \sum_{k=1}^m t_{kj}S(\overline{e_k}) \\ 
	= & \sum_{k=1}^m t_{kj}\left( \sum_{i=1}^p s_{ik} \overline{w_i}\right) = \sum_{i=1}^p \left(\sum_{k=1}^m s_{ik}t_{kj}\right)\overline{w_i}
\end{align*}
Therefore, $m(S \circ T) = \sum_{k=1}^m s_{ik}t_{kj} = (s_{ik})(t_{kj}) =  m(S)m(T)$.

\subsection{The Jacobian Matrix}
Let $\overline{u_1}, \overline{u_2}, \cdots, \overline{u_n}$ be the unit co-ordinate vectors in $\mathbb{R}^n$ and $\overline{e_1}, \overline{e_2}, \cdots, \overline{e_m}$ be the unit co-ordinate vectors in $\mathbb{R}^m$.
Let function $f : \mathbb{R}^n \to \mathbb{R}^m$ be differentiable at $\overline{c} \in \mathbb{R}^n$.
Then there exists a linear function $T = f'(\overline{c}) : \mathbb{R}^n \to \mathbb{R}^m$ such that $f(\overline{c}+\overline{v}) = f(\overline{c})+f'(\overline{c})(\overline{v}) + \|\overline{v}\|+E_{\overline{c}}(\overline{v})$.
We have, $T(\overline{u_k}) = f'(\overline{c})(\overline{u_k}) = f'(\overline{c},\overline{u_k}) = D_kf(\overline{c}) = D_k \sum_{i=1}^m f_i(\overline{c})\overline{e_i}$.

Clearly, the matrix of total derivative $T$, $m(T) = (t_{ik}) = (D_k f_i(\overline{c}))$.
This matrix is called Jacobian matrix of $f$ at $\overline{c}$ and is denoted by $Df(\overline{c})$.
\[ Df(\overline{c}) = \begin{bmatrix} D_1 f_1(\overline{c}) & D_2 f_1(\overline{c}) & \cdots & D_n f_1(\overline{c}) \\ D_1 f_2(\overline{c}) & D_2 f_2(\overline{c}) & \cdots & D_n f_2(\overline{c}) \\ \vdots & \vdots & \ddots & \vdots \\ D_1 f_m(\overline{c}) & D_2 f_m(\overline{c}) & \cdots & D_n f_m(\overline{c}) \end{bmatrix} \]

\subsubsection{Properties of Jacobian matrix}
\begin{enumerate}
	\item $k$th row of $Df(\overline{c})$ is gradient vector of $f_k$
		\[\nabla f_k(\overline{c}) = (D_1f_k(\overline{c}),D_2f_k(\overline{c}),\cdots,D_nf_k(\overline{c}))\]
	\item When $m = 1$, $Df(\overline{c}) = \nabla f(\overline{c})$.
	\item $f'(\overline{c})(\overline{v}) = \sum_{k=1}^m \left(\nabla f_k(\overline{c}) \cdot \overline{v}\right) \overline{e_k}$
	\item $\|f'(\overline{c})(\overline{v})\| \le M\|v\|$ where $M = \sum_{k=1}^m \|\nabla f_k(\overline{c})\| $, by property (3)
	\item $f'(\overline{c})(\overline{v}) \to \overline{0}$ as $\overline{v} \to \overline{0}$, by property (4)
\end{enumerate}

\subsubsection{Chain Rule}
\begin{commentary}
Chain Rule for real function : $\frac{d F \circ G}{dx}(x) = \frac{d}{dy}F(y) \frac{d}{dx}G(x) = F'(y)\ G'(x)$

For example : $\frac{d}{dx} (ax+3)^3 = \frac{d}{dy}y^3 \frac{d}{dx} \left(ax+3\right) = 3ay^2 = 3a(ax+3)^2$
\end{commentary}
\begin{theorem}
Let $g$ be differentiable at $\overline{a}$, with total derivative $g'(\overline{a})$ and $\overline{b} = g(\overline{a})$.
Let $f$ is differentiable at $\overline{b}$, with total derivative $f'(\overline{b})$.
Then $h = f \circ g$ is differentiable at $\overline{a}$ with total derivative $h'(\overline{a}) = f'(\overline{b}) \circ g'(\overline{a})$.
\begin{commentary}
Try to read $h'(\overline{a}) = H$, $f'(\overline{b}) = F$, $g'(\overline{a}) = G$, then $H = F \circ G \implies H(x) = F(G(x))$
In other words, $h'(\overline{a})(\overline{v}) = f'(\overline{b}) \circ g'(\overline{a})\ (\overline{v}) = f'(\overline{b})(g'(\overline{a})(\overline{v}))$.
\end{commentary}
\end{theorem}
\begin{proof}
Given $\epsilon > 0$, let $y \in \mathbb{R}^p$ such that $\|y\| < \epsilon$.
Let $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^p \to \mathbb{R}^n$, then $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.

We have, $h(\overline{a}+\overline{y})-h(\overline{a}) = f(g(\overline{a}+\overline{y})) - f(g(\overline{a})) = f(\overline{b}+\overline{v}) - f(\overline{b})$ where $\overline{b} = g(\overline{a})$, and  $\overline{v} = g(\overline{a}+\overline{y})-g(\overline{a})$.

Since $g$ is differentiable at $\overline{a}$, $g$ satisfies first-order Taylor's formula.
\begin{align*}
	g(\overline{a}+\overline{y}) & =  g(\overline{a}) + g'(\overline{a})(\overline{y}) + \|\overline{y}\| E_{\overline{a}}(\overline{y}) \text{ where } E_{\overline{a}} \to \overline{0} \text{ as } \overline{y} \to \overline{0} \\
	\implies & \overline{v} = g(\overline{a}+\overline{y})-g(\overline{a}) = g'(\overline{a})(\overline{y}) + \|\overline{y}\| E_{\overline{a}}(\overline{y})
\end{align*}
Clearly, as $\overline{y} \to \overline{0} \implies \overline{v} \to g'(\overline{a})(\overline{0}) = \overline{0}$.
Again,  we have $f$ is differentiable at $\overline{b}$, thus $f$ satisfies first-order Taylor's formula.
\[ f(\overline{b}+\overline{v}) = f(\overline{b}) + f'(\overline{b})(\overline{v}) + \|\overline{v}\| E_{\overline{b}}(\overline{v}) \text{ where } E_{\overline{b}} \to \overline{0} \text{ as } \overline{v} \to \overline{0} \]
\begin{align*}
	\implies f(\overline{b}+\overline{v}) - f(\overline{b}) & = f'(\overline{b})(\overline{v}) + \|\overline{v}\| E_{\overline{b}}(\overline{v}) \\
	& = f'(\overline{b})\left( g'(\overline{a})(\overline{y}) + \|\overline{y}\|E_{\overline{a}}(\overline{y}) \right) + \|\overline{v}\| E_{\overline{b}}(\overline{v}) \\
	& = f'(\overline{b})(g'(\overline{a})(\overline{y})) + \|\overline{y}\| E(\overline{y}) \\
	& \text{ where } E(\overline{y}) = f'(\overline{b})(E_{\overline{a}}(\overline{y})) + \frac{\|\overline{v}\|}{\|\overline{y}\|} E_{\overline{b}}(\overline{v}),\ \overline{y} \ne \overline{0}
\end{align*}
\[ \implies h(\overline{a}+\overline{y})-h(\overline{a}) = f(\overline{b}+\overline{v}) - f(\overline{b}) = f'(\overline{b})(g'(\overline{a})(\overline{y})) + \|\overline{y}\|E(\overline{y}) \]

Since $f'(\overline{b})$ and $g'(\overline{a})$ are linear, their composition is also linear.
Therefore, $h$ is differentiable at $\overline{a}$ with a linear, total derivative $h'(\overline{a}) = f'(\overline{b}) \circ g'(\overline{a})$ as it satisfies first-order Taylor's formula if $E_{\overline{y}} \to \overline{0}$ as $\overline{y} \to \overline{0}$.

We have, $\|\overline{v}\| \le \|g'(\overline{a})(\overline{y})\| + \|\overline{y}\|\ \|E_{\overline{a}}(\overline{y})\| \le M\|y\| + \|E_{\overline{a}}(\overline{y})\|\ \|\overline{y}\|$.
\[ \implies \frac{\|\overline{v}\|}{\|\overline{y}\|} \le M + \|E_{\overline{a}}(\overline{y})\| \]

Thus, $\overline{v} \to \overline{0}$ as $\overline{y} \to \overline{0}$.
Then $f'(\overline{b})(\overline{v}) \to f'(\overline{b})(\overline{0}) = \overline{0}$.
And $E_{\overline{a}}(\overline{y}) \to \overline{0}$.
Therefore, $E(\overline{y}) \to \overline{0} + M\overline{0} = \overline{0}$ as $\overline{y} \to \overline{0}$.
\end{proof}

\subsubsection{Matrix form of the chain rule}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$, $g : \mathbb{R}^p \to \mathbb{R}^n$.
And $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.
Suppose $g$ is differentiable at $\overline{a} \in \mathbb{R}^p$ and $f$ is differentiable at $g(\overline{a}) = \overline{b} \in \mathbb{R}^n$.
Then $h$ is differentiable at $\overline{a}$ and the Jacobian matrix of $h$ is given by the chain rule,
\[ Dh(\overline{a}) = Df(\overline{b})Dg(\overline{a}) \text{ where } h = f \circ g,\ \overline{b} = g(\overline{a})\]
In other words,
\[ D_jh_i(\overline{a}) = \sum_{k=1}^n D_k f_i(\overline{b}) D_j g_k(\overline{a}),\ i=1,2,\cdots,m,\ j=1,2,\cdots,p \]

For $m=1$, $D_j h(\overline{a}) = \sum_{k=1}^n D_kf(\overline{b}) D_jg_k(\overline{a})$

For $m=1$ and $p=1$, $h'(\overline{a}) = \sum_{k=1}^n Df(\overline{b}) g_k'(\overline{a}) = \nabla f(\overline{b}) \cdot Dg(\overline{a})$

\begin{theorem}
Let $f$ and $D_2f$ be continuous functions on a rectangle $[a,b] \times [c,d]$.
Let $p$ and $q$ be differentiable on $[c,d]$, where $p(y) \in [a,b]$ and $q(y) \in [c,d]$ for each $y \in [c,d]$.
Define $F$ by the equation,
\[ F(y) = \int_{p(y)}^{q(y)} f(x,y) dx,\ y \in [c,d] \]
Then $F'(y)$ exists for each $y \in (c,d)$ and is given by,
\[ F'(y) = \int_{p(y)}^{q(y)} D_2 f(x,y) dx + f((q,y),y)q'(y) - f(p(y),y)p'(y) \]
\end{theorem}
\begin{commentary}
The following two theorems are required for proving the theorem on differentiating an integral.
\end{commentary}
\begin{theorem}
Let $\alpha$ be of bounded variation on $[a,b]$ and assume that $f \in \mathcal{R}(\alpha)$ on $[a,b]$.
\[ \text{ Define } F(x) = \int_a^x f\ d\alpha,\ x \in [a,b] \]
Then $F$ is of bounded variation on $[a,b]$ and $F$ is continuous at $x$ if $\alpha$ is continuous at $x$.
If $\alpha$ is increasing on $[a,b]$, then the derivative $F'(x)$ exists at each $x \in (a,b)$ where $\alpha'(x)$ exists and where $f$ is continuous.
And 
\[ F'(x) = f(x)\alpha'(x) \]
\label{thm:paralimit}
\end{theorem}
\begin{theorem}
Let $Q = \{ (x,y) : a \le x \le b,\ c \le y \le d \}$.
Assume that $\alpha$ is of bounded variation on $[a,b]$ and for each $y \in [c,d]$, assume that the integral
\[ F(y) = \int_a^b f(x,y)\ d\alpha(x) \]
exists.
If the partial derivative $D_2f$ is continuous on $Q$, the derivative $F'(y)$ exists for each $y \in (c,d)$ and is given by
\[ F'(y) = \int_a^b D_2f(x,y)\ d\alpha(x) \]
\label{thm:fixedlimit}
\end{theorem}
\begin{proof}
Let $G(x_1,x_2,x_3) = \int_{x_1}^{x_2} f(t,x_3)\ dt$.
Then we may write $F(y)$ in terms of $G$.
That is, $F(y) = G(p(y),q(y),y)$.

By 1-dimensional chain rule, we have 
\begin{align*}
	F'(y) & = \frac{dF}{dy} = \frac{\partial G}{\partial p} \frac{dp}{dy} + \frac{\partial G}{\partial q} \frac{dq}{dy} + \frac{\partial G}{\partial y} \\
	& = D_1 G\ p'(y) + D_2 G\ q'(y) + D_3 G
\end{align*}

\begin{itemize}
	\item $D_1 G$\\ 
	Since the variable of differentition is present in the limit of the integral, we use theorem \ref{thm:paralimit} to compute the derivative of the integral.
	We may write, $G(p(y),q(y),y)  = -\int_{q(y)}^{p(y)} f(t,y)\ dt$.
	We are differentiating (partially) with respect to $p(y)$.
	Thus $q(y)$, $y$ are constants for this differentiation.
	Suppose $G(x,a,y) = -H(x) = -\int_a^x f(t,y)\ dt \implies D_1 G = -H'(x)= -f(x,y)$.
	Thus, $D_1 G = -f(p(y),y)$.
	\item $D_2 G$\\
	Again, variable of differentiation is persent in the limit of the integral.
	Thus, we write, $G(p(y),q(y),y) = \int_{p(y)}^{q(y)} f(t,y)\ dt$.
	Now we are differentiating (partially) with respect to the the second component of $G$ which is $q(y)$.
	Clearly, $p(y)$ and $y$ are treated as constants.
	$G(a,x,y) = H(x) = \int_a^x f(t,y)\ dt \implies D_2 G = H'(x) = f(q(y),y)$.
	\item $D_3 G$\\
	Now the variable of integration is not affecting the limits of the integral.
	Also it is given that $D_2 f$ is continuous on $[a,b] \times [c,d]$.
	We write $G(a,b,x) = H(x) = \int_a^b f(t,x)\ dt \implies D_3 G = H'(x) = \int_a^b D_2 f(t,x)\ dt$.
	Thus $D_3 G = \int_{p(y)}^{q(y)} f(t,y)\ dt$.
\end{itemize}
\end{proof}

\subsubsection{The mean-value theorem for differentiable functions}
\begin{theorem}[Mean-Value]
Let $S$ be an open subset of $\mathbb{R}^n$.
Assume $f : S \to \mathbb{R}^m$ is differentiable at each point of $S$.
Let $\overline{x}$, $\overline{y}$ be two points in $S$ such that $L(\overline{x},\overline{y}) = \{ t\overline{x}+(1-t)\overline{y} : t \in [0,1] \}$ is subset of $S$.
Then for every $\overline{a} \in \mathbb{R}^m$, there exists a point $\overline{z} \in L(\overline{x},\overline{y})$ such that
\[ \overline{a}.\left( f(\overline{y})-f(\overline{x}) \right) = \overline{a}.f'(\overline{z})(\overline{y}-\overline{x}) \]
\end{theorem}
\begin{proof}
Let $\overline{u} = \overline{y}-\overline{x}$.
We have $S$ is open subset and $L(\overline{x},\overline{y}) \subset S$, thus there exists $\delta > 0$ such that $\overline{x}+t\overline{u} \in S, \forall t \in (-\delta,1+\delta)$.
\begin{commentary} In other words, the `Line segment $L(\overline{x},\overline{y})$' is properly contained in $S$, in such a way that extending the Line from $\overline{x}$ to $\overline{y}$ a little bit extra one either sides is still contained in $S$.\end{commentary}

Let $\overline{a} \in \mathbb{R}^m$ and $F : (-\delta,1+\delta) \to \mathbb{R}$ defined by $F(t) = \overline{a}.f(\overline{x}+t\overline{u})$.
Then $F$ is differentiable at each $t \in (-\delta,1+\delta)$ and the derivative $F'(t) = \overline{a}.f'(\overline{x}+t\overline{u},\overline{u})$, the directional derivative of $f(\overline{x}+t\overline{u})$ with respect to $\overline{u}$.

\[ f'(\overline{x}+t\overline{u},\overline{u}) = f'(\overline{x}+t\overline{u})(\overline{u}) \implies F'(t) = \overline{a}.f'(\overline{x}+t\overline{u})(\overline{u}) \]
By 1-dimensional mean-value theorem, we have
\[ \exists \theta \in (0,1) \text{ such that } F(1) - F(0) = F'(\theta) \]
By definition of $F$, $F(1) = \overline{a}.f(\overline{x}+\overline{u}) = \overline{a}.f(\overline{y})$.
And $F(0) =\overline{a}.f(\overline{x})$.
Therefore,
\[ F'(\theta) = F(1) - F(0) = \overline{a}.f(\overline{y}) - \overline{a}.f(\overline{x}) = \overline{a}.(f(\overline{y})-f(\overline{x})) \]
We also have,
\[ F'(\theta) = \overline{a}.f'(\overline{x}+\theta \overline{u})(\overline{u}) = \overline{a}.f'(\overline{z})(\overline{y}-\overline{x}), \text{ where } \overline{z} = \overline{x}+\theta \overline{u} \in L(\overline{x},\overline{y}) \]
\end{proof}

\begin{remark}
Suppose $S$ is convex in $\mathbb{R}^m$.
Then for every pair of points $\overline{x},\overline{y} \in S$, $L(\overline{x},\overline{y}) \subset S$.
Thus Mean-value theorem holds for all $\overline{x},\overline{y} \in S$.
\end{remark}

\section{Multivariate Calculus}
\subsection{A sufficient condition for differentiability}
\begin{theorem}
Suppose one of the partial derivatives $D_1f,D_2f,\cdots,D_nf$ exists at $\overline{c}$.
And the remaining $n-1$ partial derivatives exists in some $n$-ball $B(\overline{c})$ and are continuous at $\overline{c}$.
Then $f$ is differentiable at $\overline{c}$.
\end{theorem}
\begin{proof}
Step 1 : Real-valued function

We claim that the function $f : \mathbb{R}^n \to \mathbb{R}^m$ is differentiable at $\overline{c}$ iff each component $f_k$ is differentiable at $\overline{c}$.

Suppose $f$ is differentiable at $\overline{c}$, then there exists a linear, total derivative function $f'(\overline{c})$ satisfying first-order Taylor's formula at $\overline{c}$.

ie, $f(\overline{c}+\overline{v}) = f(\overline{c}) + f'(\overline{c})(\overline{v}) + \|\overline{v}\| E_{\overline{c}}(\overline{v})$ where $E_{\overline{c}}(\overline{v}) \to \overline{0}$ as $\overline{v} \to \overline{0}$.
\begin{align*}
	f(\overline{c}+\overline{v}) & = \left( f_1(\overline{c}+\overline{v}), f_2(\overline{c}+\overline{v}), \cdots, f_m(\overline{c}+\overline{v}) \right)\\
	f(\overline{c}) & = \left( f_1(\overline{c}), f_2(\overline{c}), \cdots, f_m(\overline{c}) \right) \\
	f'(\overline{c})(\overline{v}) & = \left( f'_1(\overline{v}), f'_2(\overline{v}), \cdots, f'_m(\overline{v}) \right) \\
	E_{\overline{c}}(\overline{v}) & = \left( E_1(\overline{v}), E_2(\overline{v}), \cdots, E_m(\overline{v}) \right)
\end{align*}
where each component of the error function $E_k(\overline{v}) \to 0$ as $\overline{v} \to \overline{0}$.
Also since $f'(\overline{c})$ is linear, each of its components $f'_k : \mathbb{R}^n \to \mathbb{R}$ are linear.
\begin{align*}
	f(\overline{c}+\overline{v}) = & \left( f_1(\overline{c}+\overline{v}), f_2(\overline{c}+\overline{v}), \cdots, f_m(\overline{c}+\overline{v}) \right) \\
	= & \left( f_1(\overline{c}), f_2(\overline{c}), \cdots, f_m(\overline{c}) \right) + \left( f'_1(\overline{v}), f'_2(\overline{v}), \cdots, f'_m(\overline{v}) \right) \\
	& + \|\overline{v}\|\left(E_1(\overline{v}), E_2(\overline{v}), \cdots, E_m(\overline{v}) \right) \text{ where } E_k(\overline{v}) \to 0 \text{ as } \overline{v} \to \overline{0}\\
	= & \left( f_1(\overline{c}), f_2(\overline{c}), \cdots, f_m(\overline{c}) \right) + \left( f'_1(\overline{v}), f'_2(\overline{v}), \cdots, f'_m(\overline{v}) \right) \\
	& + \left(\|\overline{v}\|E_1(\overline{v}), \|\overline{v}\|E_2(\overline{v}), \cdots, \|\overline{v}\|E_m(\overline{v}) \right) \text{ where } E_k(\overline{v}) \to 0 \text{ as } \overline{v} \to \overline{0}\\
	= & \left( f_1(\overline{c}) + f'_1(\overline{v}) + \|\overline{v}\|E_1(\overline{v}), \cdots, f_m(\overline{c}) + f'_m(\overline{v}) + \|\overline{v}\|E_m(\overline{v}) \right) 
\end{align*}

Thus first-order Taylor's forumula for $f$ at $\overline{c}$ gives first-order Taylor's forumula for each of its components $f_k$.
ie, $f_k(\overline{c}+\overline{v}) = f_k(\overline{c}) + f'_k(\overline{v} + \|\overline{v}\|E_k(\overline{v})$ where $E_k(\overline{v}) \to 0$ as $\overline{v} \to \overline{0}$.
Therefore, $f_k$ are differentiable at $\overline{c}$ for $k = 1,2,\cdots, m$.

Suppose each component $f_k$ of $f$ are differentiable at $\overline{c}$.
Then there exists linear, total derivative functions $f'_k$ satisfying first-order Taylor's formula at $\overline{c}$.
ie, $f_k(\overline{c}+\overline{v}) = f_k(\overline{v}) + f'_k(\overline{v}) + \|\overline{v}\|E_k(\overline{v})$ where $E_k(\overline{v}) \to 0$ as $\overline{v} \to \overline{0}$.

Define $E_{\overline{c}}(\overline{v}) = \left( E_1(\overline{v}), E_2(\overline{v}), \cdots, E_k(\overline{v}) \right)$.
Then $E_{\overline{c}}(\overline{v}) \to \overline{0}$ as $\overline{v} \to \overline{0}$.
Therefore, there exists a linear, total derivative function $f'(\overline{c}) = \left( f'_1, f'_2, \cdots, f'_m \right)$ satisfying first-order Taylor's formula at $\overline{c}$.

Thus, if each (real-valued) component function $f_k$ are differentiable, then $f$ is also differentiable.
Therefore, it is sufficient to prove the theorem for a real-valued function.

Step 2: Telescopic Sum

Assume (without loss of generality) that $D_1f$ exists at $\overline{c}$ and $D_2f,D_3f,\cdots,D_nf$ exist and continuous in some $n$-ball $B(\overline{c})$.
\begin{commentary} Suppose $D_rf$ exists at $\overline{c}$ and all partial derivatives execept $D_rf$ are continuous.
Then $v_0 = \overline{0}$, $v_1 = y_r\overline{u_r}$, $v_2 = y_r\overline{u_r} + y_1\overline{u_1}$, \dots.
Then the following proof can be applied without any loss of generality.
\end{commentary}

Let $\overline{v} = \lambda\overline{y}$ where $\overline{y} = \frac{\overline{v}}{\|\overline{v}\|}$.
Clearly, $\|\overline{y}\| = 1$ and $\lambda = \| \overline{v} \|$.
Choose $\lambda > 0$ such that $\overline{c}+\overline{v} \in B(\overline{c})$ and all the partial derivatives $D_2f, D_3f, \cdots, D_nf$ exists and are continuous in $B(\overline{c})$.

We have, $\overline{y} = (y_1, y_2, \cdots, y_n) = y_1 \overline{u_1} + y_2 \overline{u_2} + \cdots + y_n \overline{u_n}$.

Define $\overline{v_0} = \overline{0},\ \overline{v_1} = y_1\overline{u_1}, \cdots, \ \overline{v_n} = y_1 \overline{u_1} + y_2 \overline{u_2} + \cdots + y_n \overline{u_n}$.
\begin{align*}
	f(\overline{c}+\overline{v}) - f(\overline{c}) = & ( f(\overline{c}+\lambda{} \overline{v_n}) - f(\overline{c}+\lambda{} \overline{v_{n-1}}) ) \\
	& + ( f(\overline{c}+\lambda{} \overline{v_{n-1}}) - f(\overline{c}+\lambda{} \overline{v_{n-2}}) ) \\
	& + \cdots + ( f(\overline{c}+\lambda{} \overline{v_1}) - f(\overline{c}+\lambda{} \overline{v_0}) ) \\
	& = \sum_{k = 1}^n f(\overline{c} + \lambda{} \overline{v_k}) - f(\overline{c} + \lambda{} \overline{v_{k-1}}) \\
	& = \sum_{k = 1}^n f(\overline{c} + \lambda{} \overline{v_{k-1}} + \lambda{} y_k \overline{u_k}) - f(\overline{c} + \lambda{} \overline{v_{k-1}})
\end{align*}
Step 3 : Mean-value theorem

Define $\overline{b_k} = \overline{c}+\lambda{}\overline{v_{k-1}}$.
Then we have
\begin{equation}
f(\overline{c}+\overline{v}) - f(\overline{c}) = \sum_{k = 1}^n f(\overline{b_k} + \lambda{}y_k\overline{u_k})-f(\overline{b_k})
\end{equation}
We know that all partial derivatives exists in $B(\overline{c})$.
%Thus $f$ is continuous in $B(\overline{c})$.
Therefore by 1-dimensional mean-value theorem we have,
\[ f(\overline{b_k}+\lambda{}y_k\overline{u_k}) - f(\overline{b_k}) = \lambda{}y_kD_kf(\overline{a_k}) \text{ where } \overline{a_k} \in L(\overline{b_k},\overline{b_k}+\lambda{}y_k\overline{u_k}) \]
\begin{equation}
	f(\overline{c}+\overline{v}) - f(\overline{c}) = \lambda{} \sum_{k = 1}^n y_kD_kf(\overline{a_k}) \text{ where } \overline{a_k} \in L(\overline{b_k},\overline{b_k}+\lambda{}y_k\overline{u_k})
\end{equation}

Step 4 : Continuity of partial derivatives in $B(\overline{c})$

As $\lambda{} \to 0,\ \overline{v} \to \overline{0}$.
And both $\overline{b_k},\ \overline{b_k}+\lambda{}y_k\overline{u_k} \to \overline{c}$.
Clearly, $\overline{a_k}$ in the line between $\overline{b_k}$ and $\overline{b_k}+\lambda{} y_k \overline{u_k}$ also converges to $\overline{c}$.

For $k \ge 2$, $D_kf$ are continuous in the $n$-ball $B(\overline{c})$.
Thus $D_kf(\overline{a_k}) \to D_kf(\overline{c})$.
We may write, $D_kf(\overline{a_k}) = D_kf(\overline{c}) + E_k(\lambda)$ where $E_k(\lambda) \to \overline{0}$ as $\lambda{} \to 0$.
Also, since $D_1 f$ exists, $D_1f(\overline{c}+\lambda{} y_1\overline{u_1}) \to D_1f(\overline{c})$ as $\lambda{} \to 0$.
\begin{commentary}
\[ \text{ Remember : } D_1 f(\overline{c}) = \lim_{h \to 0} \frac{f(\overline{c}+h\overline{u_1}) - f(\overline{c})}{h} \]
\end{commentary}
\begin{align*}
	f(\overline{c}+\overline{v}) - f(\overline{c}) = & \lambda{} \sum_{k = 1}^n y_kD_kf(\overline{c}) + \lambda{} \sum_{k = 1}^n y_kE_kf(\lambda{})\\
	= & \nabla f(\overline{c}) \cdot \overline{v} + \|\overline{v}\|E(\lambda) \\
	& \text{ where } E(\lambda{}) = \sum_{k = 1}^n y_k E_k(\lambda{}) \to \overline{0} \text{ as } \overline{v} \to \overline{0}
\end{align*}
That is, we have a linear function which satisfies first-order Taylor's formula at $\overline{c}$.
Therefore, $f$ is differentiable.
\end{proof}

\subsection{Sufficient conditions for the equality of mixed partial derivatives}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$.
Then $D_r f$ and $D_k f$ are two partial derivatives of $f$.
And $D_{r,k} f = D_r(D_k f)$ and $D_{k,r} f = D_k (D_r f)$.
\[ D_{r,k} f = \frac{\partial^2 f}{\partial x_r \partial x_k} = \frac{\partial}{\partial x_r} \frac{\partial f}{\partial x_k} \text{ and } D_{k,r} f = \frac{\partial^2 f}{\partial x_k \partial x_r} = \frac{\partial}{\partial x_k} \frac{\partial f}{\partial x_r}  \]
\begin{commentary}
	There are two sufficient conditions for the equality of these mixed partial derivatives in our scope.
\begin{enumerate*}
	\item differentiability of $D_k f$ or
	\item continuity of $D_{r,k} f$ and $D_{k,r}$
\end{enumerate*}
at $\overline{c}$ where the mixed partial derivatives are to be equal.
\end{commentary}

\subsubsection{Differentiability}
\begin{theorem}
Suppose $D_r f$ and $D_k f$ exists in an $n$-ball about $\overline{c}$ and are both differentiable at $\overline{c}$.
Then $D_{r,k} f = D_{k,r} f$.
\end{theorem}
\begin{proof}
Step 1 : Real-valued function

It is sufficient to prove the theorem for real-valued functions.
Let $f : \mathbb{R}^n \to \mathbb{R}^m$, then $f(\overline{c}) = \left( f_1(\overline{c}),f_2(\overline{c}),\cdots,f_m(\overline{c}) \right)$.
And $$D_k f(\overline{c}) = \left( D_k f_1(\overline{c}), D_k f_2(\overline{c}), \cdots, D_k f_m(\overline{c}) \right)$$
Thus it is sufficient to prove that $D_{r,k}f_j(\overline{c}) = D_{k,r}f_j(\overline{c}),\ j = 1,2,\cdots,m$.
That is, it is sufficient to prove equality of mixed partial derivatives of a real-valued function $f_j : \mathbb{R}^n \to \mathbb{R}$.
Also, we will prove it for $n = 2$ and $\overline{c} = (0,0)$.
\begin{commentary}
From Step 2 onwards, we will write $f$ instead of $f_j$ for ease of notation.
\end{commentary}

Step 2 : $\nabla(h)$

Let $f : \mathbb{R}^n \to \mathbb{R}$.
Suppose that the partial derivatives $D_k f$, $D_r f$ exist in the $n$-ball $B(n)$.
And let $h > 0$ such that the rectangle with vertices $(0,0), (0,h), (h,0), (h,h)$ lies in $B(n)$.
\begin{commentary} Suppose $n=3$, $c = (x,y,z)$, and we want to prove equality of $D_{2,3} f$ and $D_{3,2} f$.
Then we will consider the rectangle with vertices $(x,y,z), (x,y,z+h), (x,y+h,z), (x,y+h,z+h)$.
Again, we are taking $n=2$ and $\overline{c} = (0,0)$, only for the ease of notation as the same proof is applicable for any finite natural number, $n$ and any vector $\overline{c} \in \mathbb{R}^n$.\end{commentary}

Define $\nabla(h) = f(h,h)-f(h,0)-f(0,h)+f(0,0)$.

Step 3 : $D_{1,2} f = \frac{\nabla(h)}{h^2} = D_{2,1} f$

Define $G(x) = f(x,h)-f(x,0)$.
Then  we have, $\nabla(h) = G(h)-G(0)$ and $G'(x) = D_1f(x,h) - D_1f(x,0)$.
By 1-dimensional mean value theorem,
\begin{align*}
	G(h)-G(0) = & hG'(x_1)  \text{ where } x_1 \in (0,h) \\
	= & h\left( D_1f(x_1,h)-D_1f(x_1,0)\right)
\end{align*}
We have $D_1 f$ is differentiable at $(0,0)$.
There exists linear, total derivative function $(D_1f)'(0,0)$ where $(D_1f)'(0,0)(x,y) = \nabla D_1 f(0,0) \cdot{} (x,y)$ satisfiying first-order Taylor's formula at $(0,0)$. 
\begin{commentary}
\[ \text{ Remember :} f'(\overline{c})(\overline{v}) = \sum_{k = 1}^n v_k D_k f(\overline{c}) = \nabla f(\overline{c}) \cdot{} \overline{v} \]
\end{commentary}
\begin{align*}
	D_1 f((0,0) + (x_1,h)) = & D_1 f(0,0) + \nabla D_1 f(0,0) \cdot{} (x_1,h) + \|(x_1,h)\| E_1(h) \\
	& \text{ where } E_1(h) \to 0 \text{ as } h \to 0\\
	D_1 f(x_1,h) = & D_1 f(0,0) + x_1 D_{1,1} f(0,0) + h D_{2,1} f(0,0) + \left|\sqrt{x_1^2+h^2}\right| E_1(h)
\end{align*}

Similarly,
\begin{align*}
	D_1 f((0,0) + (x_1,0)) = & D_1 f(0,0) + \nabla D_1 f(0,0) \cdot{} (x_1,0) + \|(x_1,0)\| E_2(h) \\
	& \text{ where } E_2(h) \to 0 \text{ as } h  \to 0 \\
	D_1 f(x_1,0) = & D_1 f(0,0) + x_1 D_{1,1} f(0,0) + |x_1| E_2(h)
\end{align*}
Therefore $\nabla(h) = h (D_1 f(x_1,h) - D_1 f(x_1,0)) = h^2 D_{2,1} f(0,0) + E(h)$ where $E(h) = h|\sqrt{x_1^2+h^2}| E_1(h) - h|x_1|E_2(h)$ and $E(h) \to 0$ as $h \to 0$.

Since $0 < x_1 < h$, we have
$$ 0 \le E(h) \le h^2\left(\sqrt{2}E_1(h)-E_2(h)\right)$$

Therefore,
	$$\lim_{h \to 0} \frac{\nabla(h)}{h^2} \le \lim_{h \to 0} \frac{h^2 D_{2,1} f(0,0)}{h^2} + \lim_{h \to 0} \frac{h^2 (\sqrt{2}E_1(h) - E_2(h))}{h^2} = D_{2,1}f(0,0)$$
\begin{equation}
\lim_{h \to 0} \frac{\nabla(h)}{h^2} = D_{2,1} f(0,0)
\end{equation}

Similarly, define $H(y) = f(h,y)-f(0,y)$.
Then we have, $\nabla(h) = H(h)-H(0)$ and $H'(y) = D_2f(h,y) - D_2f(0,y)$.
By 1-dimensional mean value theorem,
\begin{align*}
	H(h)-H(0) = & hH'(y_1)  \text{ where } y_1 \in (0,h) \\
	= & h \left( D_2f(h,y_1)-D_2f(0,y_1) \right)
\end{align*}
We have $D_2 f$ is differentiable at $(0,0)$.
Thus there exists a linear, total derivative function $(D_2f)'(0,0)$ where $(D_2f)'(0,0)(x,y) = \nabla D_2 f(0,0) \cdot{} (x,y)$ satisfying first-order Taylor's formula at $(0,0)$.
That is,
\begin{align*}
	D_2 f((0,0)+(h,y_1)) = & D_2 f(0,0) + \nabla D_2 f(0,0) \cdot{} (h,y_1) + \|(h,y_1)\| E_3(h) \\
	& \text{ where } E_3(h) \to 0 \text{ as } h \to 0 \\
	D_2 f(h,y_1) = & D_2 f(0,0) + h D_{1,2} f(0,0) + y_1 D_{2,2} f(0,0) + \left|\sqrt{h^2+y_1^2}\right| E_3(h)
\end{align*}

Again,
\begin{align*}
	D_2 f((0,0)+(0,y_1)) = & D_2 f(0,0) + \nabla D_2 f(0,0) \cdot{} (0,y_1) + |y_1| E_4(h) \\
	& \text{ where } E_4(h) \to 0 \text{ as } h \to 0 \\
	D_2 f(0,y_1) = & D_2 f(0,0) + y_1 D_{2,2} f(0,0) + |y_1| E_4(h)
\end{align*}
Therefore, $\nabla(h) = h( D_2 f(h,y_1) - D_2 f(0,y_1) ) = h^2 D_{1,2} f(0,0) + E'(h)$ where $E'(h) = \left|\sqrt{h^2+y_1^2}\right| E_3(h) - |y_1|E_4(h)$ and $E'(h) \to 0$ as $h \to 0$. And 
\begin{equation}
	\lim_{h \to 0} \frac{\nabla(h)}{h^2}  = D_{1,2} f(0,0)
\end{equation}
Therefore, $D_{1,2} f(0,0) = D_{2,1} f(0,0)$.
\end{proof}

\subsubsection{Continuity}
\begin{theorem}
Suppose $D_r f$ and $D_k f$ exists in an $n$-ball about $\overline{c}$.
And $D_{r,k} f$ and $D_{k,r} f$ are continuous at $\overline{c}$.
Then $D_{r,k} f = D_{k,r} f$.
\end{theorem}
\begin{proof}
We have $D_r f = (D_r f_1 , D_r f_2, \cdots, D_r f_m)$.
Therefore, it is sufficient to prove the theorem for real-valued functions.
Suppose $n = 2$, $\overline{c} = (0,0)$ and the partial derivatives $D_1 f$ and $D_2 f$ exist and are continuous in some $2$-ball about $(0,0)$.
Suppose $(h,h)$ lies in that $2$-ball, then $D_1 f(h,h) \to D_1 f(0,0)$ as $h \to 0$.
\end{proof}

\begin{remark} A function $f$ such that $D_{1,2} f \ne D_{2,1} f$.
\[ \text{Let, }f(x,y) = \begin{cases} \frac{xy(x^2-y^2)}{x^2+y^2} & (x,y) \ne (0,0) \\ 0 & (x,y) = (0,0) \end{cases} \]
\begin{align*}
	D_1 f(x,y) = & \frac{\partial}{\partial x} \frac{x^3y-xy^3}{x^2+y^2} \\
	= & \frac{(3x^2y-y^3)(x^2+y^2) - 2x(x^3y-xy^3)}{(x^2+y^2)^2} \\
	= & \frac{x^4y+4x^2y^3-y^5}{(x^2+y^2)^2}\\
	D_1 f_{_{(x = 0)}} = & -y \implies D_{2,1} f_{_{(x = 0)}} = \frac{\partial}{\partial y} -y = -1
\end{align*}
\begin{align*}
	D_2 f(x,y) = & \frac{\partial}{\partial y} \frac{x^3y-xy^3}{x^2+y^2} \\
	= & \frac{(x^3-3xy^2)(x^2+y^2)-2y(x^3y-xy^3)}{(x^2+y^2)^2} \\
	= & \frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2} \\
	D_2 f_{_{(y = 0)}} = & x \implies D_{1,2} f_{_{(y = 0)}}  = \frac{\partial}{\partial x} x = 1
\end{align*}
Therefore, $D_{1,2} f \ne D_{2,1} f$ in the neighbourhood of $(0,0)$.
\begin{commentary} This treatment save a lot of time.
After $D_1 f$, we are planning to perform $D_{2,1} f = D_2 (D_1 f)$ in which the value of $x$ is going to be treated as a constant.
Therefore, we can simplify the expression by substituting $x = 0$ at this stage.
If you are not confident enough to substitute that ``early''.
You may take partial derivative with respect to $y$ and then substitute $x = 0$ and $y = 0$.
Why don't we substitute $y = 0$ before $D_2 f$ is something you should know already !\end{commentary}
\end{remark}

%\chapter{Implicit Functions and Extremum Problems}
\subsection{Implicit Functions and Extremum Problems}
\begin{definition}[Implicit function]
Let $f$ be a function.
Consider the equation, $f(\overline{x},\overline{y}) = 0$.
If there exists a function $g$ such that $\overline{x} = g(\overline{y})$, then $g$ is an implicit form of $f$ or $g$ is defined implicitly by $f$.

For example, a linear system of equations $Ax-b = 0$ implicitly defines $x = A^{-1}b$ provided $ A$ has non-zero determinant.
\end{definition}

\begin{definition}[Jacobian Determinant]
Let $f : \mathbb{R}^n \to \mathbb{R}^n$, then determinant of the Jacobian matrix $Df(\overline{x})$ is the Jacobian determinant of $f$, $J_f(\overline{x})$.
\end{definition}

\begin{theorem}
Let $f : \mathbb{C} \to \mathbb{C}$.
Then $J_f(z) = |f'(z)|^2$.
\end{theorem}
\begin{proof}
Suppose $f : \mathbb{C} \to \mathbb{C}$ where $f(z) = u(z) + iv(z)$ where $u : \mathbb{C} \to \mathbb{R}$ and $v : \mathbb{C} \to \mathbb{R}$.
\begin{commentary}These real-valued functions $u,v$ have respective $u^*,v^*$ multivariate real functions such that $u^* : \mathbb{R}^2 \to \mathbb{R}$, where $u(z) = u^*(x,y)$ and $z = x+iy$.
Let $f(z) = z^2+1$.
Then $u^*(x,y) = x^2-y^2+1$ and $v^*(x,y) = -2xy$.
And theoretically we use derivatives of $u^*$ when we mention derivatives of $u$.\end{commentary}

Then $f$ has a derivative at $z$ only if the partial derivatives $D_1u,D_2u,D_1v,D_2v$ exists at $z$ and satisfies Cauchy-Riemann equations.
ie $D_1u(z) = D_2v(z)$ and $D_1v(z) = -D_2u(z)$.\cite[Theorem 5.22]{apostol}.

Thus we have $f'(z) = D_1u + iD_1v$ \cite[Theorem 12.6]{apostol}\footnote{Prove using first-order Taylor's formula}.
\begin{align*}
	f'(z) & = D_1u(z) + iD_1v(z) \\
	|f'(z)|^2 & = (D_1u(z))^2 + (D_1v(z))^2\\
	\intertext{For ease of representation, we write $D_1u$ instead of $D_1u(z)$}
	|f'(z)|^2 & = (D_1u)^2 + (D_1v)^2
\end{align*}
We also have
\[ J_f(z) = |Df(z)| = \begin{vmatrix} D_1u & D_2u \\ D_1v & D_2v \end{vmatrix} = D_1uD_2v - D_1vD_2u = (D_1u)^2 + (D_1v)^2 \]
Therefore, $J_f(z) = |f'(z)|^2$.
\end{proof}

\subsubsection{Functions with non-zero Jacobian determinant}
\begin{commentary}
That is, $f : \mathbb{R}^n \to \mathbb{R}^n$ such that $J_f \ne 0$ in an $n$-ball. In other words, we have an $n$-ball $B(\overline{x})$ such that $J_f(\overline{y}) \ne 0,\ \forall \overline{y} \in B(\overline{x})$.
\end{commentary}

\begin{theorem}
Let $B$ be an $n$-ball about $\overline{a}$ in $\mathbb{R}^n$, $\partial B$ be its boundary and $\bar{B} = B \cup \partial B$ be its closure.\footnote{$\bar{B}$ : The line above $B$ has a different meaning compare to $\overline{a}$ (situations like this are an abuse of language).}
Let $f : \mathbb{R}^n \to \mathbb{R}^n$ be continuous in $\bar{B}$ and all partial derivatives, $D_jf_i(\overline{x})$ exists for every $\overline{x} \in B$.
Let $f(\overline{x}) \ne f(\overline{a})$ for every $\overline{x} \in \partial B$ and $J_f(\overline{x}) \ne 0$ for every $\overline{x} \in B$.
Then $f(B)$ contains an $n$-ball about $f(\overline{a})$.
\begin{commentary}
\[ B = \{ \overline{x} : \| \overline{x} - \overline{a} \| < r \} \]
\[ \partial B = \{ \overline{x} : \| \overline{x} - \overline{a} \| = r \} \]
\[ \bar{B} = \{ \overline{x} : \| \overline{x} - \overline{a} \| \le r \} \]
\end{commentary}
\end{theorem}
\begin{proof}
Define $g : \partial B \to \mathbb{R}$ where $g(\overline{x}) = \| f(\overline{x}) - f(\overline{a}) \| $.
We have, $f(\overline{x}) \ne f(\overline{a})$ for every $\overline{x} \in \partial B$, thus $g(\overline{x}) > 0$ for every $\overline{x} \in \partial B$.
Function $f$ is continuous on $\bar{B}$, thus $g$ is continuous on \begin{commentary}$\bar{B}$ and thus $g$ is continuous on its subset\end{commentary} $\partial B$.
Since $\partial B$ is compact, every continuous function on $\partial B$ attains its extrema\footnote{``Every continuous function on a compact set attains its extrema''} and thus $g$ attains its minimum value $m > 0$ somewhere on $\partial B$.

Consider $n$-ball $T$ about $f(\overline{a})$ with radius $\frac{m}{2}$, 
\[ T = B\left(f(\overline{a}),\frac{m}{2}\right) = \left\{ \overline{y} \in \mathbb{R}^n : \| f(\overline{a}) - \overline{y}\| < \frac{m}{2} \right\} \]
Therefore, it is sufficient to prove that $T \subset f(B)$.

Let $\overline{y} \in T$. Define $h : \bar{B} \to \mathbb{R}$ where $h(\overline{x}) = \| f(\overline{x}) - \overline{y} \|$. Again this continuous function $h$ on compact set $\bar{B}$ attains its extrema somewhere on $\bar{B}$. Since $\overline{y} \in T$, $h(\overline{a}) = \| f(\overline{a}) - \overline{y} \| < \frac{m}{2} $. Thus, the minimum of $h$ on $\bar{B}$ is less than $\frac{m}{2}$, \begin{commentary}since $\overline{a} \in \bar{B}$.\end{commentary}

Let $\overline{x} \in \partial B$, then 
\begin{align*}
	h(\overline{x}) = & \| f(\overline{x}) - \overline{y} \| \\
	= &  \| f(\overline{x}) - f(\overline{a}) + f(\overline{a}) - \overline{y} \| \\
	\ge &  \| f(\overline{x}) - f(\overline{a})\| + \| f(\overline{a}) - \overline{y} \| \\ 
	= &  g(\overline{x}) - h(\overline{a}) \\
	> & \frac{m}{2} \text{ since $g(\overline{x}) \ge m$ and $h(\overline{a}) < \frac{m}{2}$}
\end{align*}
	Thus $h$ doesn't attain its minimum on $\partial B$, but at an interior point $\overline{c} \in B$. Consider
	\[ h^2(\overline{x}) = \| f(\overline{x}) - \overline{y}\|^2 = \sum_{r = 1}^n (f_r(\overline{x}) - y_r)^2 \]
	The function $h^2$ also has minimum at the same point $\overline{c}$. Thus all partial derivatives of $h^2$ at $\overline{c}$ are zero. ie, 
	\[ D_k h^2(\overline{c}) = \sum_{r = 1}^n (f_r(\overline{c})-y_r)D_kf_r(\overline{c}) = 0 \]
	This is a system of linear equations with non-zero determinant since $\overline{c} \in B$ and we have $J_f(\overline{c}) \ne 0$. Therefore, $f_r(\overline{c}) = y_r$. That is, $f(\overline{c}) = \overline{y} \in f(B)$. Since $\overline{y} \in T$ is arbitrary, $T \subset f(B)$.
\end{proof}

\begin{theorem}
	Let $A$ be an open subset of $\mathbb{R}^n$ and $f : A \to \mathbb{R}^n$ is continuous and has continuous partial derivatives $D_jf_i$ on $A$. If $f$ is one-to-one on $A$ and $J_f(\overline{x}) \ne 0,\ \forall \overline{x} \in A$, then $f(A)$ is open.
\end{theorem}
\begin{proof}
	Let $\overline{b} \in f(A)$. Then $\overline{b} = f(\overline{a})$ for some $\overline{a} \in A$.
	We have, $f$ is continuous, $f$ has continuous partial derivatives on $A$ and $J_f(\overline{x}) \ne 0$ for every $\overline{x} \in A$.
	Therefore, there exists an open ball $B \subset A$ containing $\overline{a}$ such that $f(B) \subset f(A)$ contains an $n$-ball about $f(\overline{a})$.
	Since $\overline{b} \in f(A)$ is arbitary, every point in $f(A)$ has an $n$-ball containing it in $f(A)$.
	Therefore, $f(A)$ is open.

\begin{commentary} Two assumption in above theorem are trivial.
\begin{enumerate*}
	\item $f$ is continuous in the closed ball, $\bar{B}$.
Set $B$ so chosen that $\bar{B} \subset A$ and $f$ is continuous in $A$.
Thus, $f$ is continuous in $\bar{B}$.
	\item $f$ has different value at boundary compared to center.
ie, $f(\overline{a}) \ne f(\overline{x}),\ \forall x \in \partial B$.
	We have, $f$ is injective on $A$, and $\bar{B} \subset A$.
	Thus $f$ has different values for any two distinct points in it.
Thus, $\forall \overline{x},\overline{y} \in \bar{B},\  \overline{x} \ne \overline{y} \implies \overline{x}, \overline{y} \in A$, and $\overline{x} \ne \overline{y} \implies f(\overline{x}) \ne f(\overline{y})$ \end{enumerate*}
\end{commentary}
\end{proof}

\begin{theorem}
Let $S$ be an open subset of $\mathbb{R}^n$ and $f : S \to \mathbb{R}^n$.
Let components of $f$ has continuous partial derivatives on $S$, $D_jf_i$ and $J_f(\overline{a}) \ne 0$ for some point $\overline{a} \in S$.
Then there is an $n$-ball $B$ about $\overline{a}$ on which $f$ is injective.
\end{theorem}
\begin{proof}
Let $\overline{z} = (\overline{z_1},\overline{z_2},\cdots,\overline{z_n})$ where $\overline{z_i} \in \mathbb{R}^n$.
ie, $\overline{z} \in \mathbb{R}^{n^2}$.
Define function $h : \mathbb{R}^{n^2} \to \mathbb{R}$ by $h(\overline{z}) = \det{[D_jf_i(\overline{z_i})]}$.
Since $f$ has continuous partial derivatives on $S$, each component of $f$ has continuous partial derivatives in $S$ and thus $h$ is continuous on $S^n$ \begin{commentary} which is a subset of $\mathbb{R}^{n^2}$ since $S$ is an open subset of $\mathbb{R}^n$.
\end{commentary}

Let $\overline{a} \in S$ such that $J_f(\overline{a}) \ne 0$.
\begin{commentary}
Existence of such a point in $S$ is assumed.
\end{commentary}
Consider,$\overline{z_i} = \overline{a},\ \forall i$.
Then $\overline{z} = (\overline{a},\overline{a},\cdots,\overline{a})$.
And $h(\overline{z}) = \det{[D_jf_i(\overline{a})]} = J_f(\overline{a}) \ne 0$.

Since $h$ is continous and $h(\overline{z}) \ne 0$.
There exists an $n$-ball $B$ about $\overline{a}$ in $S$ such that $h(\overline{z}) \ne 0$ for $\overline{z_i} \in B$.
We claim that $f$ is injective on $B$.

Suppose $f$ is not injective.
ie, There exists $\overline{x},\overline{y} \in B(\overline{a})$ such that $\overline{x} \ne \overline{y}$ and $f(\overline{x}) = f(\overline{y)}$.
Open ball $B(\overline{a})$ is a convex set.
And the line segment $L(\overline{x},\overline{y}) \subset B(\overline{a})$.
The function $f$ is differentiable on $S$.
On applying mean-value theorem to each component of $f$, we get
$$0 = f_i(\overline{y})-f_i(\overline{x}) = \nabla f_i(\overline{Z}_i)\cdot(\overline{y}-\overline{x}),\ i=1,2,\cdots$$
where $\overline{Z}_i \in L(\overline{x},\overline{y}) \subset B(\overline{a})$.
Therefore, We have
$$ \sum_{k=1}^n D_kf_i(\overline{Z}_i)(y_k-x_k) = 0$$
The determinant of this system of linear equations is nonzero, as the function $f$ has nonzero jacobian determinant at $\overline{Z}_i \in B(\overline{a})$ for $i = 1,2,\cdots$.
Thus, $y_i = x_i$ for $i = 1,2,\cdots$. This contradicts $\overline{x} \ne \overline{y}$.
Hence, the function $f$ is injective.
\end{proof}

\begin{theorem}
Let $A$ be an open subset of $\mathbb{R}^n$ and assume that $f : A \to \mathbb{R}^n$ has continuous partial derivatives $D_jf_i$ on $A$.
If $J_f(\overline{x}) \ne 0$ for all $\overline{x} \in A$, then $f$ is an open mapping.
\end{theorem}
\begin{proof}
Let $S$ be an open subset of $A$.
Let $\overline{x} \in S$.
Clearly, $f$ has continuous partial derivatives on $S$ and $J_f(\overline{x}) \ne 0$ for all $\overline{x} \in S$.
Thus, there is an $n$-ball $B(\overline{x})$ in which $f$ is injective.
Therefore, $f(B(\overline{x})$ is open in $\mathbb{R}^n$.
Since $\overline{x} \in S$ is arbitrary, $S = \cup_{\overline{x} \in S} B(\overline{x})$.
And $f(S) = \cup_{\overline{x} \in S} f(B(\overline{x}))$.
Therefore, $f(S)$ is open.
Since open set $S$ is arbitrary, $f$ is an open mapping.
\end{proof}

\begin{commentary}
\begin{remark}[Properties]
Functions with non-zero Jacobian determinant has following properties :
\begin{enumerate}
	\item If $J_f \ne 0$ in $n$-ball $B$ about $\overline{a}$ which has different values at its boundaries, then $f(B)$ has an $n$-ball about $f(\overline{a})$.
	\item If $J_f \ne 0$, $f$ has continuous partial derivatives in $S$, and $f$ is injective in an open set $A$, then $f(A)$ is open.
	\item Let $S$ be an open set in $\mathbb{R}^n$, $f$ has continuous partial derivatives in $S$, and $J_f(\overline{a}) \ne 0$ for some $\overline{a} \in S$, then $f$ is injective on an $n$-ball $B(\overline{a})$ in $S$.
	\item Let $A$ be an open set in $\mathbb{R}^n$, $f$ has continuous partial derivatives in $A$, and $J_f \ne 0$ in $A$, then $f$ is an open mapping.
\end{enumerate}
\end{remark}
\end{commentary}

\subsubsection{Inverse function Theorem}
\begin{theorem}[Inverse function]
Let $S$ be an open subset of $\mathbb{R}^n$ and $f$ be a continuously differentiable function\footnote{$f \in C'(S)$ : $f$ is continuously differentiable on $S$} $f : S \to \mathbb{R}^n$.
If $J_f(\overline{a}) \ne 0$ for some $\overline{a} \in S$, then there are two open sets $X \subset S$, and $Y \subset f(S)$ such that
\begin{enumerate}
	\item $\overline{a} \in X \text{ and } f(\overline{a}) \in Y$
	\item $Y = f(X)$
	\item $f$ is injective
	\item there exists another function $g : Y \to X$ such that $g(f(\overline{x})) = \overline{x},\ \forall \overline{x} \in X$
	\item $g$ is continuously differentiable on $Y$
\end{enumerate}
\begin{commentary}
	In other words, if $f \in C'$ and there exists $\overline{a} \in S$ such that $J_f(\overline{a}) \ne 0$, then $f$ has an inverse $f^{-1}$ in a neighbourhood of $f(\overline{a})$ and $f^{-1} \in C'$.
\end{commentary}
\end{theorem}
\begin{proof}
	Step 1 : Construction of open sets $X$ and $Y$.\\
	Given that, $J_f(\overline{a}) \ne 0$ and $f \in C'$.
	Thus all partial derivatives of $f$ are continuous on $S$.
	Then $J_f$ is continuous on $S$, 
	By the continuity of $J_f$ at $\overline{a}$, there exists a neighbourhood of $\overline{a}$, say $B_1(\overline{a})$ in which $J_f \ne 0$.
	That is, $\forall \overline{x} \in B_1(\overline{a}),\ J_f(\overline{x}) \ne 0$.
	Therefore,(by theorem) there exists an $n$-ball $B(\overline{a})$ on which $f$ is injective.
	Let $B$ be an $n$-ball with center $\overline{a}$ contained in $B(\overline{a})$.
	Then $f$ is injective on $B$.
	Therefore,(by theorem) $f(B)$ contains an $n$-ball with center $f(\overline{a})$.
	Let $Y$ be the $n$-ball contained in $f(B)$.
	And  $X=f^{-1}(Y) \cap B$.
	That is, the inverse image of $Y$ on $B$.
	Since $f$ is continuous, $f^{-1}(Y)$ is open.
	Thus, $X$ is an intersection of open sets.
	And therefore, $X$ is open.

	Step 2 : The inverse of $f$, say $g$.\\
	Clearly $\overline{a} \in X$ and $f(\overline{a}) \in Y$.
	Also $Y = f(X)$ and $f$ is injective on $X$( since, $X \subset B$).
	
	The closure of $B$, $\bar{B}$ is compact and $f$ is injective and continuous on $\bar{B}$.
	Then\footnote{Existence of inverse of a continuous function on a compact set in metric spaces.} there exists a continuous function $g$ defined on $f(\bar{B})$ such that $g \circ f$ is the identity function on $\bar{B}$.
	That is, $\forall x \in \bar{B},\ g(f(\overline{x})) = x$.
	Thus, $g(X) = Y$ and $g$ is unique.

	Step 3 : $g$ has continuous partial derivatives.\\
	Define a real-valued function $h : S^n \to \mathbb{R}$ by $h(\overline{Z}) = \det[D_jf_i(\overline{Z}_i)]$ where $\overline{Z}_1,\overline{Z}_2,\cdots,\overline{Z}_n \in S$ and $\overline{Z} = (\overline{Z}_1,\overline{Z}_2,\cdots,\overline{Z}_n)$.
	Now, let $\overline{Z} = (\overline{a},\overline{a},\cdots,\overline{a})$.
	Then $h(\overline{Z}) \ne 0$ and $h$ is continuous on $S^n$.
	Therefore, $\overline{Z}$ has a neighbourhood on which $h$ does not vanish (that is, nonzero).
	Let $B_2(\overline{a})$ be the corresponding $n$-ball with center $\overline{a}$ such that $\overline{Z}_i \in B_2(\overline{a}) \implies h(\overline{Z}) \ne 0$.

	Let $B$ be an $n$-ball with center $\overline{a}$ contained in $B_2(\overline{a})$.
	Now $\bar{B} \subset B_2(\overline{a})$.
	And $h(\overline{Z}) \ne 0,\ \forall \overline{Z}_i \in \bar{B}$.

	We have, $g = (g_1, g_2, \cdots,g_n)$.
	It is enough to prove that $g_k \in C'$ for $k=1,2,\cdots,n$.
	Again, it is enough to prove that $D_rg_k$ exists and is continuous for $1 \le r \le n$.
	(Fix some $r$ and prove that $D_rg_k$ is continuous.)

	Let $\overline{y} \in Y$.
	Define $\overline{x} = g(\overline{y})$ and $\overline{x}' = g(\overline{y}+t\overline{u}_r)$ where $t$ is sufficiently small such that $\overline{y}+t\overline{u}_r \in Y$.
	Then $\overline{x},\overline{x}' \in X$.
	And $f(\overline{x}')-f(\overline{x}) = t\overline{u}_r$.
	Therefore $f_i(\overline{x})-f_i(\overline{x}') = 0$ when $i \ne r$.
	And $f_i(\overline{x}')-f_i(\overline{x}) = t$ when $i = r$.
	By mean-value theorem,
	$$\frac{f_i(\overline{x}') - f(\overline{x})}{t} = \nabla f_i(\overline{Z}_i) \cdot \frac{\overline{x}'-\overline{x}}{t}$$
	where $\overline{Z}_i \in L(\overline{x},\overline{x}')$, the line segment joining $\overline{x}$ and $\overline{x}'$.
	Since $\det[D_jf_i(\overline{Z}_i)] = h(\overline{Z} \ne 0$, this system of linear equations in $n$ unknowns, $\frac{x_j'-x_j}{t}$ has a unique solution.
	As $t \to 0$, $\overline{x}' \to \overline{x}$.
	And $\overline{Z}_i \to \overline{x}$.
	Since $J_f(\overline{x}) \ne 0$, the limit
	$$ \lim_{t \to 0} \frac{g_k(\overline{y}+t\overline{u}_r)-g_k(\overline{y})}{t}$$
	exists.
	Thus, $D_rg_k(\overline{y})$ exists $\forall y \in Y$ and every $r$.	
	This limit is a quotient of two determinants of partial derivatives of $f$, which are all continuous since $f \in C'$.
	Therefore, $D_rg_k$ are all continuous and $g \in C'$.
\end{proof}

\subsubsection{Implicit function Theorem}
\begin{theorem}[Implicit function]
Let $S$ be an open subset of $\mathbb{R}^{n+k}$ and $f$ be a function $f : S \to \mathbb{R}^n$.
Suppose $f$ is continuously differentiable on $S$.
Let $(\overline{x_0},\overline{t_0}) \in \mathbb{R}^n \times \mathbb{R}^k$ such that $f(\overline{x_0},\overline{t_0}) = \overline{0}$ and $J_f(\overline{x_0}) \ne 0$.
Then there exists an open set $T_0$ containing $\overline{t_0}$ in $\mathbb{R}^k$ and a unique function $g : \mathbb{R}^k \to \mathbb{R}^n$ such that
\begin{enumerate}
	\item $g$ is continuously differentiable on $T_0$
	\item $g(\overline{t_0}) = \overline{x_0}$
	\item $f(g(\overline{t}),\overline{t}) = \overline{0},\ \forall \overline{t} \in T_0$
\end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}

\subsubsection{Extrema of function of one variable}
\begin{theorem}[Extrema of functions of one variable]
Let $n \ge 1$ and function $f$ has $n$th partial derivative in open interval $(a,b)$.
Suppose for some $c \in (a,b)$,
\[ f'(c) = f''(c) = \cdots = f^{(n-1)}(c) = 0 \text{ and } f^{(n)}(c) \ne 0 \]
If $n$ is even,
\begin{enumerate}
	\item $f$ has a local minimum at $c$ if $f^{(n)}(c)>0$
	\item $f$ has a local maximum at $c$ if $f^{(n)}(c) < 0$
\end{enumerate}
and If $n$ is odd, there is neither a local minimum nor a local maximum at $c$.
\end{theorem}
\begin{proof}
\end{proof}

\subsubsection{Extrema of function of several variables}
\begin{definition}[stationary point]
If function $f$ is differentiable at $\overline{a}$ and $\nabla f(\overline{a}) = \overline{0}$, the point $\overline{a}$ is a stationary point of $f$.
\end{definition}

\begin{definition}[saddle point]
A stationary point is a saddle point if every $n$-ball $B(\overline{a})$ contains points $\overline{x}$ such that $f(\overline{x}) > f(\overline{a})$ and other points such that $f(\overline{x}) < f(\overline{a})$.
\end{definition}

\begin{definition}[quadratic form]
A function $Q : \mathbb{R}^n \to \mathbb{R}$ defined $Q(\overline{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j$ where $a_{ij} \in \mathbb{R}$ is a function of the quadratic form or simply a quadratic form.
\begin{description}
	\item[symmetric quadratic form] $a_{ij} = a_{ji},\ \forall i,j$. 
	\item[positive definite quadratic form] $Q(\overline{x}) > 0, \forall \overline{x} \ne \overline{0}$. 
	\item[negative definite quadratic form] $Q(\overline{x}) < 0, \forall \overline{x} \ne \overline{0}$. 
\end{description}
\end{definition}

\begin{theorem}[Second derivative test for extrema]
Let $f$ be a function $f : \mathbb{R}^n \to \mathbb{R}^m$.
Suppose that the second order partial derivatives $D_{i,j}f$ exists in an $n$-ball $B(\overline{a})$ and are continuous at $\overline{a}$ where $\overline{a}$ is a stationary point of $f$.
\[ \text{Let } Q(\overline{t}) = \frac{1}{2} f''(\overline{a},\overline{t}) = \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n D_{i,j} f(\overline{a})t_it_j \]
\begin{enumerate}
	\item If $Q(\overline{t}) > 0$ for all $\overline{t} \ne \overline{0}$, $f$ has a relative minimum at $\overline{a}$.
	\item If $Q(\overline{t}) < 0$ for all $\overline{t} \ne \overline{0}$, $f$ has a relative maximum at $\overline{a}$.
	\item If $Q(\overline{t})$ takes both positive and negative values, then $f$ has a saddle point at $\overline{a}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}

\begin{theorem}[Second derivative test : simplified]
Let $f$ be a real-valued function $f : \mathbb{R}^2 \to \mathbb{R}$ with continuous second order partial derivatives at a stationary point $\overline{a} \in \mathbb{R}^2$.
Let $A = D_{1,1}f(\overline{a})$, $B = D_{1,2}f(\overline{a}) = D_{2,1}f(\overline{a})$, and $C = D_{2,2}f(\overline{a})$.
And let $\Delta = \begin{vmatrix}A & B \\ B & C \end{vmatrix} = AC - B^2$.
Then we have,
\begin{enumerate}
	\item If $\Delta > 0$ and $A > 0$, then $f$ has a relative minimum at $\overline{a}$.
	\item If $\Delta > 0$ and $A < 0$, then $f$ has a relative maximum at $\overline{a}$.
	\item if $\Delta < 0$, then $f$ has a saddle point at $\overline{a}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}


%\chapter{Multiple Riemann Integrals*}
%\chapter{Multiple Lebesgue Integrals*}
%\chapter{Cauchy's Theorem and the Residue Calculus*}
%\cite{rudin}
%\chapter{The Real and Complex Number Systems}
%\chapter{Basic Topology}
%\chapter{Numerical Sequences and Series}
%\chapter{Continuity}
%\chapter{Differentiation}
%\chapter{The Riemann- Stieltjes Integral}
%\chapter{Sequences and Series of Functions}
%\chapter{Some Special Functions}
%\chapter{Functions of Several Variables}
%\chapter{Integration on Differential Forms}
\section{Integration on Differential Forms}
\begin{definition}
	A $k$-cell in $R^k$ is given by, 
	$$I^k = \{ \overline{x} \in \mathbb{R}^k : a_i \le x_i \le b_i,\ \forall i \}$$
	where $\overline{a},\overline{b} \in \mathbb{R}^k$.
	Let $f$ be a continuous, real-valued function on $I^k$.
	Then, the integral of $f$ over $I^k$ is given by,
	$$\int_{I^k}f(\overline{x}) d\overline{x} = f_0 \text{ where } f_k = f \text{ and }$$
	$$f_{r-1} = \int_{a_r}^{b_r} f_r(x_0,x_1,\cdots,x_r) dx_r,\ r = 1,2,\cdots,k$$
\begin{commentary}
	In other words,
	$$\int_{I^k} f(\overline{x})\ d\overline{x} = \idotsint_{a_k}^{b_k} \left[f(x_0,x_1,\cdots,x_k)\ dx_k\right]\ dx_{k-1}\cdots dx_1$$
\end{commentary}
\end{definition}

\begin{theorem}
	For every $f \in \mathscr{C}(I^k)$, $L(f) = L'(f)$.

\begin{commentary}
	In other words, integral of a function over a $k$-cell is independent of the order in which those $k$ integrations are carried out.
\end{commentary}
\end{theorem}
\begin{proof}
	Step 1 : ``Separable'' Functions ie, $h(\overline{x}) = \prod h_i(x_i)$.\\
	\begin{commentary}
		(``separable'' is not standard.
		It is only for the purpose of understanding.)
	\end{commentary}

	Let $h(\overline{x}) = h_1(x_1)h_2(x_2)\cdots h_k(x_k)$ where $h_j \in [a_j,b_j]$.
	$$ L(h) = \int_{I^k} \left(\prod_{i = 1}^k h_i(x_i)\right) d\overline{x} = \prod_{i = 1}^k \int_{a_k}^{b_k} h_i(x_i)\ dx_i = L'(h)$$
	Step 2 : Algebra of ``separable'' functions, $\mathscr{A}$.\\
	Let $\mathscr{A}$ be all finite sums of functions such as $h$.
	Let $g \in \mathscr{A}$.
	\begin{align*}
		L(g) = & \int_{I^k} \left(\sum_j \prod_i h_{i,j}(x_i) \right) d\overline{x} \\
		& = \sum_j \int_{I^k} \prod_i h_{(j)}(\overline{x})\ d\overline{x}\\
		& = \sum_j \prod_i \int_{a_k}^{b_k} h_{i,j}(x_i)\ d(x_i)\\
		& = L'(g)
	\end{align*}
	Step 3 : All functions continuous on $I^k$.\\
	Let $V = \prod_{j = 1}^k (b_j - a_j)$.
	Also let $f \in \mathscr{C}(I^k)$.
	ie, a function which is continuous in $I^k$.
	Then by Stone-Weierstrass theorem,
	for any $\epsilon > 0$,	there exists a function $g \in \mathscr{A}$
	such that $\|f-g\| < \frac{\epsilon}{V}$
	where the norm of a function $f$ is defined by
	$\|f\| = \max\{f(\overline{x}) : \overline{x} \in I^k\}$.

	Therefore, it is sufficient to prove that $\|L(f)-L'(f)\| < \epsilon$.
	Since $\|f-g\| < \frac{\epsilon}{V}$, $|L(f-g)| < \epsilon$ and $|L'(f-g)| < \epsilon$. Thus,
	\begin{align*}
		L(f)-L'(f) & = L(f) - L(g) + L'(g) - L'(f) \\
		& = L(f-g) + L'(g-f)\\
		|L(f)-L'(f)| & < 2\epsilon
	\end{align*}
	Therefore, $L(f) = L'(f)$.
\end{proof}

\begin{definition}
	Let $f : \mathbb{R}^k \to \mathbb{C}$.
	The support of $f$ is the closure of the set of all points $\overline{x} \in \mathbb{R}^k$ such that $f(\overline{x}) \ne 0$.

	Let $f$ be a continuous function with compact support.
	And $I^k$ be any $k$-cell containing the support of $f$.
	Then, $\int_{\mathbb{R}^k} f\ d\overline{x} = \int_{I^k} f\ d\overline{x}$.
\end{definition}

\begin{commentary}
	$L(f) = L'(f)$ where $f$ is the limit function of a sequence of functions with compact support.
	\cite[\S10.4 Example]{apostol}

	``The definition of support permits closure of non-vanishing points.
	And this might be of some use(I don't know yet) when analysing limit functions which belong to the closure of continuous functions on $I^k$.''
\end{commentary}

\begin{definition}
	Let $E$ be an open subset in $\mathbb{R}^n$.
	Then function $G : E \to \mathbb{R}^n$ is primive if it satisfies
	$$G(\overline{x}) = \sum_{i \ne m} x_i \overline{e}_i + g(\overline{x})\overline{e}_m$$
	for some integer $m$ and some function $g : E \to \mathbb{R}$ (where $\overline{e}_i$ are the unit co-ordinate vectors).
\end{definition}

%\chapter{The Lebesgue Theory}

