%Text Books : \cite{apostol}, \cite{rudin}

%Module 1:
%The Weirstrass theorem, other forms of Fourier series, the Fourier integral theorem, the exponential form of the Fourier integral theorem, integral transforms and convolutions, the convolution theorem for Fourier transforms.
%(Chapter 11 Sections 11.15 to 11.21 of \cite{apostol}) (20 hours.)
%Module 2:
%Multivariable Differential Calculus, The directional derivative, directional derivatives and continuity, the total derivative, the total derivative expressed in terms of partial derivatives, An application of complex- valued functions, the matrix of a linear function, the Jacobian matrix, the matrix form of the chain rule. Implicit functions and extremum problems, the mean value theorem for differentiable functions,
%(Chapter 12 Sections. 12.1 to 12.11 of \cite{apostol}) (22 hours.)
%Module 3: 
%A sufficient condition for differentiability, a sufficient condition for equality of mixed partial derivatives, functions with non-zero Jacobian determinant, the inverse function theorem ,the implicit function theorem, extrema of real- valued functions of one variable, extrema of real-valued functions of several variables.
%Chapter 12 Sections-. 12.12 to 12.13 of \cite{apostol} 
%Chapter 13 Sections-. 13.1 to 13.6 of \cite{apostol} (28 hours.)
%Module 4:
%Integration of Differential Forms Integration, primitive mappings, partitions of unity, change of variables, differential forms.
%(Chapter 10 Sections. 10.1 to 10.14 of \cite{rudin}) (20 hours)

\part{ME010303 Multivariate Calculus \& Integral Transforms}
%Module 1 - \cite{apostol} 11
%Module 2 - \cite{apostol} 12
%Module 3 - \cite{apostol} 12, 13
%Module 4 - \cite{rudin} 10
%\cite{apostol}
%\chapter{The Real and Complex Number Systems*}
%\chapter{Some Basic Notations of Set Theory*}
%\chapter{Elements of Point Set Topology*}
%\chapter{Limits and Continuity*}
%\chapter{Derivatives*}
%\chapter{Functions of Bounded Variation and Rectifiable Curves*}
%\chapter{The Riemann-Stieltjes Integral*}
%\chapter{Infinite Series adn Infinite Products*}
%\chapter{Sequences of Functions*}
%\chapter{The Lebesgue Integral*}

%\chapter{Fourier Series and Fourier Integrals}
\chapter{Module 1}
\section{The Weierstrass Approximation Theorem}
	Every continuous, real valued function on a compact interval has a polynomial approximation.\cite[Theorem 11.17]{apostol}

\begin{theorem}[Weierstrass]
	Let $f$ be a real-valued, continuous function on a compact interval $[a,b]$. Then for every \(\epsilon > 0\), there is a polynomial $p$ such that \(|f(x)-p(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{theorem}
\begin{synopsis}
	Given a real-valued continuous function on compact interval $[a,b]$, we can construct a real-valued, continous function $g$ on $\mathbb{R}$ which is periodic with period $2\pi$. We have, if \(f \in L(I)\) and $f$ is bounded almost everywhere in $I$, then \(f \in L^2(I)\).\cite[Theorem 10.52]{apostol}.By Fejer's theorem (\cite[Theorem 11.15]{apostol}), the fourier series generated by $g$ (\cite[definition 11.3]{apostol}) converges to the Cesaro sum (\cite[Definition 8.47]{apostol}), which is $g$ itself in this case. Thus for any \(\epsilon > 0\), there is a finite sum of trignometric functions. The power series expansions of trignometric functions (\cite[definition 9.27]{apostol}) being uniformly convergent, there exists a polynomial $p_m$ which approximates $g$. And we can construct $p$ (polynomial approximation of $g$) using $p_m$.
\end{synopsis}
\begin{proof}
	Define \(g : \mathbb{R} \to \mathbb{R}\),
	\[ g(t) = \begin{cases}
		f(a+(b-a)t/\pi),\ t \in [0,\pi) \\ f(a+(2\pi-t)(b-a)/\pi),\ t \in [\pi,2\pi] \\ g(t - 2n\pi),\ t > 2\pi,\ n \in \mathbb{N} \\ g(t+2n\pi),\ t < 0,\ n \in \mathbb{N} \end{cases}\]

		Thus $g$ is a continuous, real-valued, periodic function with period $2\pi$ such that
		\begin{equation}
			f(x) = g\left(\frac{\pi (x-a)}{b-a}\right),\ x \in [a,b] \label{equ:fx}
		\end{equation}

		The fourier series generated by $g$ is given by,
		\[ g(t) \sim \frac{a_0}{2} + \sum_{k=1}^\infty \left( a_k \cos kt + b_k \sin kt \right)\]
		\[ \text{ where } a_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos kt\ dt,\ b_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin kt\ dt\]

		Let \(\sequence{s_n(t)}\) be the sequence of partial sums of the fourier series generated by $g$. And \( \sequence{\sigma_n(t)}\)  be the sequence of averages of $s_n(t)$ given by,
		\[\sigma_n(t) = \frac{1}{n} \sum_{k = 1}^n s_k(t),\text{ where } s_k(t) = \frac{a_0}{2} + \sum_{j = 1}^k \left( a_j \cos jt + b_j \sin jt \right)\]

		Function \(f \in L(I)\) being real-valued continuous function on a compact interval, it is bounded and hence is Lebesgue square integrable.ie, \(f \in L^2(I)\). Thus, \(g \in L^2(I)\).\\

		Since $g$ is continous on $\mathbb{R}$, the function \(s : \mathbb{R} \to \mathbb{R}\) defined by,
		\[ s(t) = \lim_{h \to 0^+} \frac{g(t+h)-g(t-h)}{2} \]
		is well-defined on $\mathbb{R}$ and \(s(t) = g(t),\ \forall t \in \mathbb{R}\).\\

		Then by Fejer's Theorem, the sequence \(\sequence{\sigma_n(t)}\) converges uniformly to $g(t)$ for every \(t \in \mathbb{R}\). Thus, given \(\epsilon > 0\), there exists \(N \in \mathbb{N}\) such that \(\forall t \in \mathbb{R}\), \(|g(t)-\sigma_N(t)| < \frac{\epsilon}{2}\).\\

		We have,
		\begin{equation}
			\sigma_N(t) = \sum_{k=0}^N \left(A_k \cos kt + B_k \sin kt \right),\text{ where } A_k, B_k \in \mathbb{R}
			\label{equ:sigmaN}
		\end{equation}
		By the power series expansion of the trignometric functions about origin,
		\begin{equation}
			\cos kt = \sum_{j = 1}^\infty \left(\frac{\cos^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty A_j' t^j \text{ where } A_j' \in \mathbb{R}
			\label{equ:coskt}
		\end{equation}
		\begin{equation}
			\sin kt = \sum_{j = 1}^\infty \left(\frac{\sin^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty B_j' t^j \text{ where } B_j' \in \mathbb{R}
			\label{equ:sinkt}
		\end{equation}

		Since the above power series expansions of trignometric functions are uniformly convergent, their finite linear combination \(\sequence{\sigma_N(t)}\) is also uniformly convergent. ie, Given \(\epsilon > 0\) there exists \(m \in \mathbb{N}\) such that for every \(t \in \mathbb{R}\)
		\[\left|\sum_{k = 0}^m C_k t^k - \sigma_N(t)\right| < \frac{\epsilon}{2} \text{ where } C_k \in \mathbb{R}\]

		Therefore, \(| p_m(t) - g(t)| \le | p_m(t) - \sigma_N(t) | + |\sigma_N(t) - g(t)| < \epsilon\) where \(p_m(t) = \sum_{k = 0}^m C_k t^k\). Define \(p : [a,b] \to \mathbb{R}\) by,
		\begin{equation}
			p(x) = p_m\left( \frac{\pi(x-a)}{b-a} \right)
			\label{equ:px}
		\end{equation}

		By equations \ref{equ:fx} and \ref{equ:px}, \(|p(x)-f(x)| < \epsilon\) for every \(x \in [a,b]\).
	\end{proof}
\section{Other Forms of Fourier Series}
	Let \(f \in L([0,2\pi])\), then the fourier series generated by $f$ is given by,
	\[ f(x) \sim \frac{a_0}{2}+\sum_{n=1}^\infty \left( a_n \cos nx + b_n \sin nx \right) \]
	\[ \text{ where } a_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos nt\ dt,\qquad b_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin nt\ dt \]

	By Euler's forumula \(e^{inx} = \cos nx + i\sin nx\). We have, \(\cos nx = \frac{(e^{inx}+e^{-inx})}{2}\) and \(\sin nx = \frac{(e^{inx}-e^{-inx})}{2i}\)

	\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( \alpha_n e^{inx} + \beta_n e^{-inx} \right) \]

	\[ \text{ where } \alpha_n = \frac{(a_n - ib_n)}{2} \qquad \beta_n = \frac{(a_n+ib_n)}{2} \]

	Therefore, by assigning \(\alpha_0 = a_0/2\), \(\alpha_{-n} = \beta_n\), we get the following exponential form of fourier series generated by $f$,

	\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^{inx} \text{ where } \alpha_n = \frac{1}{2\pi} \int_0^{2\pi} f(t)\ e^{-int}\ dt \]

	Note : If $f$ is periodic with period $2\pi$, then the interval of integration $[0,2\pi]$ can be replaced with any interval of length $2\pi$. eg. $[-\pi,\pi]$

\subsection{Periodic with period $p$}
	Let \(f \in L([0,p])\) and $f$ is periodic with period $p$. Then
	\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( a_n \cos \frac{2\pi nx}{p} + b_n \sin \frac{2\pi nx}{p} \right) \]
	\[ \text{ where } a_n = \frac{2}{p} \int_0^p f(t) \cos \frac{2\pi nt}{p}\ dt \qquad b_n = \frac{2}{p} \int_0^p f(t) \sin \frac{2\pi nt}{p}\ dt \]
	Therefore, we have the exponential form of the above fourier series given by,
	\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^\frac{2\pi inx}{p},\text{ where } \alpha_n = \frac{1}{p} \int_0^p f(t)\ e^\frac{-2\pi int}{p}\ dt \]
	
\section{Fourier Integral Theorem}

\begin{theorem}[Fourier Integral Theorem]
	Let \(f \in L(-\infty,\infty)\). Suppose \(x \in \mathbb{R}\) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
	\begin{enumerate}
		\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
		\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals
			\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \]
			exists.
	\end{enumerate}
	Then, 
	\[ \frac{f(x+)+f(x-)}{2} = \frac{1}{\pi} \int_0^\infty \int_{-\infty}^\infty f(u)\cos v(u-x)\ du\ dv, \]
	the integral $\int_0^\infty$ being an improper Riemann integral.
\end{theorem}
\begin{synopsis}
	\[ f(x+t)\frac{\sin \alpha t}{\pi t} dt \to f(u)\frac{\sin \alpha(u-x)}{\pi(u-x)} \to \frac{f(u)}{\pi} \int_0^\alpha \cos v(u-x) dv \]
	By Riemann-Lebesgue lemma\cite[Theorem 11.6]{apostol},
	\[ f \in L(I) \implies \lim_{\alpha \to +\infty} \int_I f(x) \sin \alpha t\ dt = 0 \]
	By Jordan's Theorem\cite[Theorem 10.8]{apostol}, if $g$ is of bounded variation on $[0,\delta]$, then
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
	By Dini's Theorem\cite[Theorem 10.9]{apostol}, if the limit $g(x+)$ exists and Lebesgue integral \( \int_0^\delta \frac{g(t)+g(0+)}{t} dt \) exists for some \( \delta > 0 \), then
	\[ \lim_{\alpha to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
	The order of Lebesgue integrals can be interchanged\cite[Theorem 10.40]{apostol},\\
	Suppose \(f \in L(X)\) and \(g \in L(Y)\).Then
	\[ \int_X f(x) \left(\int_Y g(y) k(x,y) dy \right) dx = \int_Y g(y) \left( \int_X f(x) k(x,y) dx \right) dy \]
\end{synopsis}
\begin{proof}
	Consider \( \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \). We prove that this integral is equal to the either sides.
	\[ \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt = \int_{-\infty}^{-\delta} + \int_{-\delta}^0 + \int_0^{-\delta}  + \int_{\delta}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \] 
	We have, function \( \frac{f(x+t)}{\pi t} \) is bounded on \( (-\infty,-\delta)\cup(\delta,\infty) \), hence \( \frac{f(x+t)}{\pi t} \) is Lebesgue integrable on \( (-\infty,-\delta) \cup (\delta,\infty) \).\\
	
	By Riemann Lebesgue lemma, 
	\[ \frac{f(x+t)}{\pi t} \in L(-\infty,-\delta) \implies \int_{-\infty}^{-\delta} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0, \]
	\[ \frac{f(x+t)}{\pi t} \in L(\delta,\infty) \implies \int_{\delta}^{\infty} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0 \]

	\paragraph{Case 1}
	Suppose $f$ is of bounded variation on $[x-\delta,x+\delta]$, put \( g(t) = f(x+t) \) then $g$ is of bounded variation on $[-\delta,\delta]$. Thus $g$ is of bounded variation on $[0,\delta]$. Then by Jordan's Theorem
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	\paragraph{Case 2}
	Suppose both the limits $f(x+)$ and $f(x-)$ exists and both Lebesgue integrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and } \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \]
	exists.\\

	Thus, we have $f(x+)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \) exists. Put \( g(t) = f(x+t) \), then \( g(0+) = f(x+) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	Similarly, $f(x-)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \) exists. Put \( g(t) = f(x-t) \), then \( g(0+) = f(x-) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\begin{align*}
		\lim_{\alpha \to +\infty} \frac{2}{\pi}\int_{-\delta}^0 f(x+t)\frac{\sin \alpha t}{t} dt
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta f(x-\tau) \frac{\sin \alpha \tau}{\tau} d\tau\\
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(\tau) \frac{\sin \alpha \tau}{\tau} d\tau = g(0+) = f(x-)
	\end{align*}

	Then by either cases,
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt 
		& = \lim_{\alpha \to +\infty} \int_{-\delta}^0 + \int_0^\delta f(x+t) \frac{\sin \alpha t}{\pi t} dt \\
		& = \frac{f(x+)+f(x-)}{2}
	\end{align*}

	We have, \( \int_0^\alpha \cos v(u-x) dv = \frac{\sin v(u-x)}{u-x} \).
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt 
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \frac{ \sin \alpha (u-x)}{u-x} du,\ (\text{put }u = x+t)\\
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \left( \int_0^\alpha \cos v(u-x) dv \right) du\\
		& = \lim_{\alpha \to +\infty} \int_0^\alpha \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv,\\
		& \text{since, the order of Lebesgue integrals can be reversed.}\\
		& = \int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv\\
		\text{where, } \int_0^\infty \text{ is not }& \text{a Lebesgue integral, but an improper Riemann integral }
	\end{align*}
	Therefore,
	\begin{align*}
		\int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt \\
		& =  \frac{f(x+)+f(x-)}{2}
	\end{align*}
\end{proof}

\begin{remark}
	If a function $f$ on $(-\infty,\infty)$ is non-periodic, then it may not have a fourier series represenation. In such cases, we have fourier intergral representaion.
\end{remark}

\section{Exponential form of Fourier Integral Theorem}
	Let \( f \in L(-\infty,\infty) \). Suppose \( x \in \mathbb{R} \) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
	\begin{enumerate}
		\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
		\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals
			\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \]
			exists.
	\end{enumerate}
	Then, 
	\[ \frac{f(x+)+f(x-)}{2} = \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)}\ du\right) dv \]
	\begin{proof}
		Let \( F(v) = \int_{-\infty}^\infty f(u) \cos v(u-x) du \). Then \( F(v) = F(-v) \) and 
		\begin{align*}
			\lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) dv
			& = \lim_{\alpha \to \infty} \frac{1}{\pi} \int_0^\alpha \int_{-\infty}^\infty f(u) \cos v(u-x) du dv\\
			& = \frac{f(x+)+f(x-)}{2}
		\end{align*}
		Let \( G(v) = \int_{-\infty}^\infty f(u) \sin v(u-x) du \). Then \( G(v) = -G(-v) \) and
		\[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha G(v) dv = 0 \]
		Thus 
		\[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) + iG(v) dv = \frac{f(x+)+f(x-)}{2} \]
	\end{proof}

\section{Integral Transforms}
\begin{definition}
	Integral transform $g(y)$ of $f(x)$ is a Lebesgue integral or Improper Riemann integral of the form
	\[ g(y) = \int_{-\infty}^\infty K(x,y) f(x)\ dx \], where $K$ is the kernal of the transform. We write \( g = \mathscr{K}(f) \).
\end{definition}

\begin{remark}
	Integral transforms(operators) are linear operators.\\
	 ie, \( \mathscr{K}(af_1 + bf_2) = a\mathscr{K}f_1 + b\mathscr{K}f_2 \)
\end{remark}

\begin{remark} A few commonly used integral transforms,
\begin{enumerate}
	\item Exponential Fourier Transform $\mathscr{F}$,
		\[ \mathscr{F}f = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	\item Fourier Cosine Transform $\mathscr{C}$,
		\[ \mathscr{C}f = \int_0^\infty \cos xy f(x)\ dx \]
	\item Fourier Sine Transform $\mathscr{S}$,
		\[ \mathscr{S}f = \int_0^\infty \sin xy f(x)\ dx \]
	\item Laplace Transform $\mathscr{L}$,
		\[ \mathscr{L}f = \int_0^\infty e^{-xy} f(x)\ dx \]
	\item Mellin Transform $\mathscr{M}$,
		\[ \mathscr{M}f = \int_0^\infty x^{y-1}f(x)\ dx \]
\end{enumerate}
\end{remark}

\begin{remark} Suppose \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_{-\infty}^\infty e^{-ixy}f(x)\ dx = \int_0^\infty e^{-ixy}f(x)\ dx = \int_0^\infty \cos xy \ f(x)\ dx + i \int_0^\infty \sin xy \ f(x)\ dx \]
	\[ \mathscr{F}f = \mathscr{C}f + i\mathscr{S}f \]\\
	Therefore Fourier Cosine $\mathscr{C}$ and Sine $\mathscr{S}$ transforms are special cases of fourier integral transform, $\mathscr{F}$ provided $f$ vanishes on negative real axis.
\end{remark}

\begin{remark} Let \( y = u+iv \), \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_0^\infty e^{-xy}f(x) = \int_0^\infty e^{-xu}e^{-ixv}f(x)\ dx = \int_0^\infty e^{-ixv} \phi_u(x) dx \]
	where \( \phi_u(x) = e^{-xu}f(x) \).
	\[ \mathscr{L}f = \mathscr{F}\phi_u \]
	Therefore Laplace transform, $\mathscr{L}$ is a special case of Fourier integral transform, $\mathscr{F}$.
\end{remark}

\begin{remark} Let \( g(y) = \mathscr{F}f(x) \).
	\[ g(y) = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	Suppose $f$ is continuous at $x$, then by fourier integral theorem,
	\begin{align*}
		f(x)	& = \frac{1}{2\pi} \int_{-\infty}^\infty \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)} du \right) dv\\
			& = \int_{-\infty}^\infty e^{-ivx} \left( \frac{1}{2\pi} \int_{-\infty}^\infty e^{ivu} f(u)\ du \right) dv\\
			& = \int_{-\infty}^\infty g(v) e^{-ivx} dv = \mathscr{F}g \text{ where } g(v) = \frac{1}{2\pi}\int_{-\infty}^\infty f(u) e^{ivu} du 
	\end{align*}
	The above function $g(v)$ gives the \textbf{inverse fourier transformation} of $f$.\\

	Let $g$ be fourier transform of $f$, then $f$ is uniquely determined by its fourier transform $g$ by,
	\[ f(x) = \mathscr{F}^{-1}g(y) = \frac{1}{2\pi} \lim_{\alpha \to \infty} \int_{-\alpha}^\alpha g(y) e^{ixy} dy \]
\end{remark}

\begin{enumerate}
	\setcounter{enumi}{5}
	\item Inverse Fourier Transform $\mathscr{F}^{-1}$,
		\[ \mathscr{F}^{-1}f = \int_{-\infty}^\infty \frac{e^{ixy}}{2\pi}f(x)\ dx \]
\end{enumerate}

\section{Convolutions}
\begin{definition}
	Let \( f,g \in L(-\infty,\infty) \). Let $S$ be the set of all points $x$ for which the Lebesgue integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists. Then the function \( h : S \to \mathbb{R} \) is a convolution of $f$ and $g$. And \( h = f \ast g \).
\end{definition}

\begin{remark}
	Convolution operator is commutative.\\
	ie, \( h = f \ast g = g \ast f \)
\end{remark}

\begin{remark}
	Suppose $f,g$ vanishes on negative real axis, then
	\[ h(x) = \int_{-\infty}^\infty f(t)\ g(x-t)\ dt = \int_{-\infty}^0 + \int_0^x  + \int_x^\infty f(t)\ g(x-t)\ dt = \int_0^x f(t)\ g(x-t)\ dt \] 
\end{remark}

\begin{remark}
	Singularity is a point at which the convolution integral fails to exists.
\end{remark}

\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and either $f$ or $g$ is bounded in $\mathbb{R}$. Then the convoluton integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists for every \( x \in \mathbb{R} \) and the function $h$ so defined is bouned in $\mathbb{R}$. In addition, if the bounded function is continuous on $\mathbb{R}$, then $h$ is continuous and \( h \in L(\mathbb{R}) \).
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}
	If $f,g$ are both unbounded, the convolution integral may not exist.
	\[ \text{ eg: } f(t) = \frac{1}{\sqrt{t}},\ g(t) = \frac{1}{\sqrt{1-t}} \]
\end{remark}

\begin{theorem}
	Let \( f,g \in L^2(\mathbb{R}) \). Then the convolution integral $f \ast g$ exists for each \( x \in \mathbb{R} \) and the function \( h : \mathbb{R} \to \mathbb{R} \) defined by \( h(x) = f \ast g (x) \) is bounded in $\mathbb{R}$.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\section{The Convolution Theorem for Fourier Tranforms}
\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and either $f$ or $g$ is continuous and bounded on $\mathbb{R}$. Let \( h = f \ast g \). Then for every real $u$,
	\[ \int_{-\infty}^\infty h(x) e^{-ixu} dx = \left( \int_{-\infty}^\infty f(t) e^{-itu} dt \right) \left( \int_{-\infty}^\infty g(y) e^{-iyu} dy \right) \]
	The integral on the left exists both as a Lebesgue integral and an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}[Application of Convolution Theorem]
	\[ B(p,q) = \frac{\Gamma{p} \Gamma{q}}{\Gamma{p+q}},\text{ where } B(p,q) = \int_0^1 x^{p-1} (1-x)^{q-1} dx,\ \Gamma{p} = \int_0^\infty t^{p-1} e^{-t} dt \]
\end{remark}

%\chapter{Multivariate Differential Calculus}
\chapter{Module 2}

	In this chapter, we deal with real functions of several variables. Instead of $\mathbf{c}$, we write \( \overline{c} \in \mathbb{R}^n \), then \( \overline{c} = (c_1, c_2, \dotsc, c_n) \) where \( c_j \in \mathbb{R} \) for every \(j = 1,2, \dotsc, n\). Again, suppose \(f : \mathbb{R}^n \to \mathbb{R}^m\) and \(f(\overline{x}) = \overline{y}\), then \(\overline{y} = (y_1, y_2, \dotsc, y_m)\) where each $y_k$ is real. The unit co-ordinate vector, $\overline{u_k}$ is given by \( {u_k}_j = \delta_{j,k} \)

\section{Directional Derivative}
	\textsl{Motivation : The existence of all partial derivatives of a multivariate real function $f$ at a point $\overline{c}$ doesn't imply the continuity of $f$ at $\overline{c}$. Thus, we need a suitable generalisation for the partial derivative which could characterise continuity. And directional derivative is such an attempt.}

\begin{definition}[Directional Derivative]
	Let \(S \subset \mathbb{R}^n\) and \(f : S \to \mathbb{R}^m\). Let $\overline{c}$ be an interior points of $S$ and \( \overline{u} \in \mathbb{R}^n \), then there exists an open ball $B(\overline{c},r)$ in $S$. Also for some $\delta > 0$ the line segment \( \alpha : [0,\delta] \to S \) given by \( \alpha(t) = \overline{c}+t\overline{u} \) lie in $B(\overline{c},r)$.\\
	
	Then the Directional derivative of $f$ at an interior point $\overline{c}$ in the direction $\overline{u}$ is given by
	\[ f'(\overline{c},\overline{u}) = \lim_{h \to 0} \frac{f(\overline{c}+h\overline{u}) - f(\overline{c})}{h} \]
\end{definition}

\begin{remark}
	The direction derivative of $f$ at an interior point $\overline{c}$ in the direction $\overline{u}$ exists only if the above limit exists.
\end{remark}

\begin{remark}Example, \cite[Exercise 12.2a]{apostol}\\
	Suppose \(\overline{x},\overline{a},\overline{c},\overline{u} \in \mathbb{R}^n\). Let \(f : \mathbb{R}^n \to \mathbb{R}\) such that \(f(\overline{x}) = \overline{a}\cdot\overline{x}\). Then
	\[ f'(\overline{c},\overline{u}) = \lim_{h \to 0} \frac{\overline{a}\cdot(\overline{c}+h\overline{u}) - \overline{a}\cdot\overline{c}}{h} = \overline{a}\cdot\overline{u}\]
\end{remark}

\begin{remark}[Properties] Let \(f : S \to \mathbb{R}^m \), where \( S \subset \mathbb{R}^n\)
	\begin{enumerate}
		\item \( f'(\overline{c},\overline{0}) = \overline{0} \)\\
			\textsl{Note : The zero vectors belongs to $\mathbb{R}^n, \mathbb{R}^m$ respectively.}
		\item \( f'(\overline{c},\overline{u_k}) = \frac{\partial f}{\partial u_k}(\overline{c}) = D_k f(\overline{c}) \), the $k^{th}$ partial derivative of $f$.
		\item Let \( f = (f_1, f_2, \cdots, f_m), \text{ such that } f(\overline{c}) = \left(f_1(\overline{c}),f_2(\overline{c}),\dotsc,f_m(\overline{c})\right) \). Then, 
			\[ \exists f'(\overline{c},\overline{u}) \iff \forall k, \exists f_k'(\overline{c},\overline{u}) \text{ and } f'(\overline{c},\overline{u}) = \left(f_1'(\overline{c},\overline{u}),f_2'(\overline{c},\overline{u}),\dotsc,f_m'(\overline{c},\overline{u})\right) \]
			ie, Directional derivative of $f$ exists iff directional derivative of each component function $f_k$ exists. And the components of the directional derivatives of $f$ are the directional derivaties of the components of $f$.\\
			Thus \( D_k f(\overline{c}) = \left(D_k f_1(\overline{c}),D_k f_2(\overline{c}),\dotsc,D_k f_m(\overline{c}) \right) \) holds.
		\item Let \( F(t) = f(\overline{c}+t\overline{u}) \), then \( F'(0) = f'(\overline{c},\overline{u}) \) and \( F'(t) = f'(\overline{c}+t\overline{u},\overline{u}) \)
		\item Let \( f(\overline{c}) = \overline{c}\cdot\overline{c} = \|\overline{c}\|^2 \), and \(F(t) = f(\overline{c}+t\overline{u}) \), then \( F'(t) = 2\overline{c}\cdot\overline{u}+2t\|\overline{u}\|^2 \) and \( F'(0) = f'(\overline{c},\overline{u}) = 2\overline{c}\cdot\overline{u} \)
		\item Let \(f\) be linear, then \( f'(\overline{c},\overline{u}) = f(\overline{u}) \)
		\item Existence of all partial derivatives doesn't imply existence of all directional derivatives.
			\[ f(x,y) = \begin{cases} x+y \qquad \text{ if } x = 0 \text{ or } y = 0 \\ 1 \qquad \qquad \text{otherwise} \end{cases} \]
			For above \(f\), directional derivatives exists only along the co\nobreakdash-ordinates (ie, partial derivatives).
		\item Existence of all directional derivatives doesn't imply continuity.
			\[ f(x,y) = \begin{cases} xy^2(x^2+y^4) \qquad x \ne 0 \\ 0 \hspace{2.5cm} x = 0 \end{cases} \]
				Above \(f\) is discontinuous at \((0,0)\), however all directional derivatives exists and has finite value.
	\end{enumerate}
\end{remark}

\section{Total Derivative}
	We may define a total derivative \( T_c(h) = hf'(c) \) in the case of real-functions of single variable as follows :-\\

	\[ \text{Let }E_c(h) = \begin{cases} \frac{f(c+h)-f(c)}{h} - f'(c),\qquad h \ne 0 \\ 0,\hspace{3.4cm} h = 0 \end{cases} \]
		Then, \( f(c+h) = f(c) + hf'(c) + hE_c(h) \) and as \( h \to 0 \), \( E_c(h) \to 0\). Also \( T_c(h) = f'(c)h \) is a linear function of $h$. ie, \( T_c(ah_1+bh_2) = aT_c(h_1)+bT_c(h_2) \). Now, we will define a total derivative of multivariate function that has these two properties.

\begin{definition}[Total Derivative]
	The function \( f: \mathbb{R}^n \to \mathbb{R}^m \) is differentiable at $\overline{c}$ if there exists a \textbf{linear} function \( T_{\overline{c}} : \mathbb{R}^n \to \mathbb{R}^m \) such that \( f(\overline{c}+\overline{v}) = f(\overline{c}) + T_{\overline{c}}(\overline{v}) + \|\overline{v}\| E_{\overline{c}}(\overline{v}) \) where \( E_{\overline{c}}(\overline{v}) \to \overline{0} \) as \( \overline{v} \to \overline{0} \).
\end{definition}

	The linear function $T_{\overline{c}}$ is the total derivative of $f$ at $\overline{c}$, \( T_{\overline{c}}(\overline{0}) = \overline{0} \) and the condition above gives the First Order Taylor's Formula for \( f(\overline{c}+\overline{v})-f(\overline{c}) \).

\begin{remark}[Properties] Let \( f : \mathbb{R}^n \to \mathbb{R}^m \) and \( f'(\overline{c})(\overline{v}) = T_{\overline{c}}(\overline{v}) \) be the total derivative of $f$ at $\overline{c}$ evaluated at $\overline{v}$. Then, 
	\begin{enumerate}
		\item \( f'(\overline{c})(\overline{v}) = f'(\overline{c},\overline{u}) \)
		\item If $f$ is differentiable at $\overline{c}$, then $f$ is continuous at $\overline{c}$.
		\item \( f'(\overline{c})(\overline{v}) = v_1 D_1 f(\overline{c}) + v_2 D_2 f(\overline{c}) + \dots + v_n D_n f(\overline{c}) \)
	\end{enumerate}
\end{remark}

\begin{note}
	The above $f'$ is a function from $\mathbb{R}^n$ to the set of all linear functions \( \mathscr{L} = \{ h : \mathbb{R}^n \to \mathbb{R}^m\} \). $f'(\overline{c})$ is a linear function (in fact, total derivative $T_{\overline{c}}$) which maps $\overline{v}$ into the directional derivatives of $f$ at $\overline{c}$ in the direction $\overline{v}$. This notation generalises $f'$ for univariate $f$ as well.(put $n=m=1$)\\

	In this subject, we use the following notations,
	\begin{description}
		\item[$D_kf(\overline{c})$] partial derivative
		\item[$f'(\overline{c},\overline{v})$] directional derivative
		\item[$f'(\overline{c})(\overline{v})$] total derivative
		\item[$\nabla{}f(\overline{c})$] gradient vector
	\end{description}
\end{note}

\begin{theorem}
	If $f$ is differentiable at $\overline{c}$ with total derivative $T_{\overline{c}}$, then for every $\overline{u} \in \mathbb{R}^n$, $T_{\overline{c}}(\overline{u}) = f'(\overline{c},\overline{u})$. ( ie, \( f'(\overline{c})(\overline{v}) = f'(\overline{c},\overline{v}) \) )
\end{theorem}
\begin{proof}
	For \( \overline{v} = \overline{0}$, we have $T_{\overline{c}}(\overline{0}) = 0 = f'(\overline{c},\overline{0}) \).\\

	Suppose \( \overline{v} \ne \overline{0} \), then put \( \overline{v} = h\overline{u} \). Since $f$ is differentiable at $\overline{c}$, $f$ has total derivative at $\overline{c}$. That is, there exists a linear function $T_{\overline{c}}$ such that \( f(\overline{c}+h\overline{u}) = f(\overline{c}) + T_{\overline{c}}(h\overline{u}) + \|h\overline{u}\|E_{\overline{c}} (h\overline{u}) \) where $E_{\overline{c}}(h\overline{u}) \to \overline{0}$ as $h\overline{u} \to \overline{0}$.
	\begin{align*}
		\implies  & f(\overline{c}+h\overline{u}) = f(\overline{c}) + hT_{\overline{c}}(\overline{u}) + |h|\|\overline{u}\|E_{\overline{c}} (h\overline{u}),\ E_{\overline{c}}(h\overline{u}) \to \overline{0} \text{ as } h\overline{u} \to \overline{0} \\
		\implies  & \frac{f(\overline{c}+h\overline{u}) - f(\overline{c})}{h} = T_{\overline{c}}(\overline{u}) + \frac{|h|\|\overline{u}\|E_{\overline{c}}(h\overline{u})}{h},\  E_{\overline{c}}(h\overline{u}) \to \overline{0} \text{ as } h \to 0 \\
		\implies  & \lim_{h \to 0} \frac{f(\overline{c}+h\overline{u}) - f(\overline{c})}{h} = T_{\overline{c}}(\overline{u}) + \lim_{h \to 0} \frac{|h|\|\overline{u}\|E_{\overline{c}}(h\overline{u})}{h} \\
		\implies & f'(\overline{c},\overline{u}) = T_{\overline{c}}(\overline{u})
	\end{align*}
\end{proof}

\begin{note}
	$T_{\overline{c}}$ is linear, however $E_{\overline{c}}$ is not linear. Thus $E_{\overline{c}}(h\overline{u}) \ne h E_{\overline{c}} (\overline{u})$.\\

	As $h \to 0$, $h\overline{u} \to \overline{0}$ and \( E_{\overline{c}}(h\overline{u}) \to \overline{0}$. Since the order of the function \( E_{\overline{c}}(h\overline{u}) \) is much smaller than that of $h$, the limit on the right converges to 0.
\end{note}

\begin{theorem}
	If $f$ is differentiable at \( \overline{c} \), then $f$ is continuous at \( \overline{c} \).
\end{theorem}
\begin{proof}
	Let $\overline{v} \ne 0$, then
	\begin{align*}
		\overline{v} = v_1 \overline{u_1} & + v_2 \overline{u_2} + \dots + v_n \overline{u_n},\\
		\overline{v} \to \overline{0} \implies & \forall j,\ v_j \to 0 \\
		T \text{ is linear }\implies & T_{\overline{c}} (\overline{v}) = v_1 T_{\overline{c}}(\overline{u_1}) + v_2 T_{\overline{c}}(\overline{u_2}) + \dots + v_n T_{\overline{c}}(\overline{u_n})\\
		\text{Thus, } & T_{\overline{c}}(\overline{v}) \to \overline{0} \text{ as } \overline{v} \to 0
	\end{align*}
	Since $f$ differentiable at \( \overline{c} \), there exists linear function $T_{\overline{c}}$ such that
	\begin{align*}
		f(\overline{c}+\overline{v}) & =  f(\overline{c}) + T_{\overline{c}}(\overline{v}) + \|v\|E_{\overline{c}}(\overline{v}) \\
		\implies & \lim_{\overline{v} \to \overline{0}} f(\overline{c}+\overline{v}) = f(\overline{c}) + \lim_{\overline{v} \to \overline{0}} T_{\overline{c}}(\overline{v}) + \lim_{\overline{v} \to \overline{0}} \|v\|E_{\overline{c}}(\overline{v})\\
		\implies & \lim_{\overline{v} \to \overline{0}} f(\overline{c}+\overline{v}) = f(\overline{c})
	\end{align*}
\end{proof}

\begin{theorem}
	Let $S \subset \mathbb{R}^n$ and $f : S \to \mathbb{R}^m$ be differentiable at an interior point $\overline{c}$ of $S$, where $S \subseteq \mathbb{R}^n$. If $\overline{v} = v_1\overline{u_1}+v_2\overline{u_2} + \cdots + v_n\overline{u_n}$, then
	\[ f'(\overline{c})(\overline{v}) = \sum_{k=1}^n v_k D_k f(\overline{c}) \]
	In particular, if $f$ is real-valued $(m = 1)$ we have, $f'(\overline{c})(\overline{v}) = \nabla{}f(\overline{c}).\overline{v}$
\end{theorem}
\begin{proof}
	Suppose $f : S \to \mathbb{R}^m$ is differentiable at $\overline{c}$, then there exists a linear function $f'(\overline{c}) : S \to \mathbb{R}^m$ such that $f(\overline{c}+\overline{v}) = f(\overline{c}) + f'(\overline{c})(\overline{v}) + \|\overline{v}\| E_{\overline{c}}(\overline{c})$ where $E_{\overline{c}} \to \overline{0}$ as $\overline{v} \to \overline{0}$.
	\begin{align*}
		f'(\overline{c})(\overline{v}) & = f'(\overline{c})\left(\sum_{k=1}^n v_k \overline{u_k}\right)\\
			& = \sum_{k=1}^n v_k f'(\overline{c})(\overline{u_k}), \text{ since $f'(\overline{c})$ is linear}\\
			& = \sum_{k=1}^n v_k D_k f(\overline{c}), \text{ since $f'(\overline{c})(\overline{u_k}) = f'(\overline{c},\overline{u_k}) = D_k f(\overline{c})$}\\
		\intertext{Let $m = 1$, then $f : S \to \mathbb{R}$ }
		f'(\overline{c})(\overline{v}) & = \sum_{k=1}^n v_k D_k f(\overline{c}) = \nabla{}f(\overline{c}).\overline{v}\\
		& \text{ since } \nabla{}f(\overline{c}) = \left( D_1f(\overline{c}),\ D_2f(\overline{c}),\ \cdots ,\ D_nf(\overline{c}) \right)
	\end{align*}
\end{proof}

\begin{remark}
	Let $f : S \to \mathbb{R}$, then $f(\overline{c} +\overline{v}) = f (\overline{c}) + \nabla{}f(\overline{c}).\overline{v} + o(\|\overline{v}\|)$ as $\overline{v} \to \overline{0}$.
\end{remark}

\begin{remark}[Complex-valued Functions]Seminar - Rohitha
\end{remark}

\section{Matrix of Linear Function}
	Let $T : \mathbb{R}^n \to \mathbb{R}^m$ be a linear function. Let $\{\overline{u_1},\ \overline{u_2},\ \cdots,\ \overline{u_n}\}$ be standard basis for $\mathbb{R}^n$ and  $\{\overline{e_1},\ \overline{e_2},\ \cdots,\ \overline{e_m}\}$ be standard basis for $\mathbb{R}^m$. Let $\overline{v} \in \mathbb{R}^n$, then $\overline{v} = \sum_{k=1}^n v_k\overline{u_k}$ and $T(\overline{v}) = \sum_{k=1}^n v_k T(\overline{u_k})$ and
\begin{commentary}
\begin{align*}
	T(\overline{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} T(\overline{u_1}) \\ T(\overline{u_2}) \\ \cdots \\ T(\overline{u_n}) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}\overline{e_1}+t_{21}\overline{e_2}+\cdots+t_{m1}\overline{e_m} \\ t_{12}\overline{e_1}+t_{22}\overline{e_2}+\cdots+t_{m2}\overline{e_m} \\ \cdots \\ t_{1n}\overline{e_1}+t_{2n}\overline{e_2}+\cdots+t_{mn}\overline{e_m} \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} \overline{e_1} \\ \overline{e_2} \\ \cdots \\ \overline{e_m} \end{bmatrix} \\
	\intertext{ We may take the transpose,}
	T(\overline{v}) & = \begin{bmatrix} \overline{e_1} & \overline{e_2} & \cdots & \overline{e_m} \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \cdots & t_{1n} \\ t_{21} & t_{22} & \cdots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n  \end{bmatrix} 
\end{align*}
\end{commentary}
	\[ T(\overline{v}) = T \left( \sum_{k=1}^n v_k \overline{u_k} \right) = \sum_{k=1}^n v_k T(\overline{u_k}) = \sum_{k=1}^n v_n \sum_{j=1}^m t_{kj}\overline{e_j} \]
	Thus matrix of $T$ is given by, $m(T) = (t_{ik})$ where $T(\overline{u_k}) = \sum_{k=1}^n t_{ik}\overline{e_i}$.
\begin{commentary}
\begin{remark}[Example]
	Let $T : \mathbb{R}^3 \to \mathbb{R}^2$ defined by $T(x,y,z)=(2x+y,y-z)$.
	\begin{align*}
		T(1,2,3) = & T((1,0,0) + 2(0,1,0) + 3(0,0,1)) \\
		= & T(\overline{u_1}+2\overline{u_2}+3\overline{u_3}) \\
		= & T(\overline{u_1}) + 2T(\overline{u_2}) + 3T(\overline{u_3}) \\
		= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} T(\overline{u_1}) \\ T(\overline{u_2}) \\ T(\overline{u_3}) \end{bmatrix} \\
		= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} (2,0) \\ (1,1) \\ (0,-1) \end{bmatrix} \\
			= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2(1,0) \\ (1,0)+(0,1) \\ -1(0,1) \end{bmatrix} \\
		= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2\overline{e_1} \\ \overline{e_1}+\overline{e_2} \\ -\overline{e_2} \end{bmatrix} \\
		= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 1 & 1 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} \overline{e_1} \\ \overline{e_2} \end{bmatrix} \\
		= & 4\overline{e_1}-\overline{e_2} = 4(1,0) - 1(0,1) = (4,-1)
	\end{align*}
	\[ \text{ In the above case, }m(T) = \begin{bmatrix} 2 & 0 \\ 1 & 1\\ 0 & -1 \end{bmatrix}\]
	Using the matrix of linear function $m(T)$, we can compute the image of any point in $R^3$ by matrix multiplication.
\end{remark}
\end{commentary}

\subsection{Matrix of the composition of two linear functions}
	Let $T : \mathbb{R}^n \to \mathbb{R}^m$ and $S : \mathbb{R}^m \to \mathbb{R}^p$ be two linear functions with domain of $S$ containing the range of $T$ (so that $S \circ T$ is well defined). Then $S \circ T : \mathbb{R}^n \to \mathbb{R}^p$ is defined by
	\[ S \circ T(\overline{x}) = S(T(\overline{x})),\ \forall \overline{x} \in \mathbb{R}^n\]
	Since $S,T$ are linear, $S \circ T$ is also linear.
	\begin{align*}
		S \circ T(a \overline{x} + b \overline{y}) = & S(T(a \overline{x} + b\overline{y})) = S(a T(\overline{x}) + b T(\overline{y})) = a S(T(\overline{x}))) + b S(T(\overline{y})) \\
		= & a S \circ T(\overline{x}) + b S \circ T(\overline{y}),\ \forall a,b \in \mathbb{R},\ \forall \overline{x},\overline{y} \in \mathbb{R}^n
	\end{align*}
	Let $\{\overline{u_1},\overline{u_2},\cdots,\overline{u_n}\}$ be the standards basis for $\mathbb{R}^n$, $\{\overline{e_1},\overline{e_2},\cdots,\overline{e_m}\}$ be the standards basis for $\mathbb{R}^m$ and $\{\overline{w_1},\overline{w_2},\cdots,\overline{w_p}\}$ be the standards basis for $\mathbb{R}^p$. Let $\overline{v} \in \mathbb{R}^n$, then $\overline(v) = \sum_{i=1}^n v_i\overline{u_i}$, and $S \circ T(\overline{v})=\sum_{i=1}^n v_n S \circ T(\overline{u_i})$
\begin{commentary}
\begin{align*}
	S \circ T(\overline{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} S \circ T(\overline{u_1}) \\ S \circ T(\overline{u_2}) \\ \cdots \\ S \circ T(\overline{u_n}) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} S(t_{11}\overline{e_1} + \cdots + t_{m1}\overline{e_m}) \\ S(t_{12}\overline{e_1} + \cdots + t_{m2}\overline{e_m}) \\ \cdots \\ S(t_{1n}\overline{e_1} + \cdots + t_{mn}\overline{e_m}) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}S(\overline{e_1}) + \cdots + t_{m1}S(\overline{e_m}) \\ t_{12}S(\overline{e_1}) + \cdots + t_{m2}S(\overline{e_m}) \\ \cdots \\ t_{n1}S(\overline{e_1}) + \cdots + t_{mn}S(\overline{e_m}) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} S(\overline{e_1}) \\ S(\overline{e_2}) \\ \cdots \\ S(\overline{e_m}) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11}\overline{w_1}+s_{12}\overline{w_2}+\cdots+s_{1p}\overline{w_p} \\ s_{12}\overline{w_1}+s_{22}\overline{w_2}+\cdots+s_{p2}\overline{w_p} \\ \cdots \\ s_{1m}\overline{w_1}+ s_{2m}\overline{w_2}+\cdots+s_{pm}\overline{w_p} \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11} & s_{21} & \cdots & s_{p1} \\ s_{12} & s_{22} & \cdots & s_{p2} \\ \vdots & \vdots & \ddots & \vdots \\ s_{1m} & s_{2m} & \cdots & s_{pm} \end{bmatrix} \begin{bmatrix} \overline{w_1} \\ \overline{w_2} \\ \vdots \\ \overline{w_p} \end{bmatrix}\\
	\intertext{We may take transpose,}
	S \circ T(\overline{v}) & = \begin{bmatrix} \overline{w_1} & \overline{w_2} & \vdots & \overline{w_p} \end{bmatrix} \begin{bmatrix} s_{11} & s_{12} & \cdots & s_{1m} \\ s_{21} & s_{22} & \cdots & s_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ s_{p1} & s_{p2} & \cdots & s_{pm} \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \cdots & t_{1n} \\ t_{21} & t_{22} & \cdots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n  \end{bmatrix} 
\end{align*}
	Remember : Given $T : \mathbb{R}^n \to \mathbb{R}^m$, then we may take $m(T)$ either as $m \times n$ matrix or $n \times m$ matrix. Since, we chose $m \times n$, $m(S \circ T) = m(S)m(T)$. Otherwise, $m(S \circ T) = m(T)m(S)$. This may change for different authors.\\
\end{commentary}

	Suppose $m(S) = (s_{ij})$ and $m(T) = (t_{ij})$ respectively. Then
	\[ S(e_k) = \sum_{i=1}^p s_{ik} \overline{w_i},\ k=1,2,\cdots,m \text{ and }\]
	\[ T(u_j) = \sum_{k=1}^m t_{kj} \overline{e_k},\ j=1,2,\cdots,n \]
	\begin{align*}
		(S \circ T)(\overline{u_j}) = & S(T(\overline{u_j})) = S\left(\sum_{k=1}^m t_{kj}\overline{e_k}\right) = \sum_{k=1}^m t_{kj}S(\overline{e_k}) \\ 
		= & \sum_{k=1}^m t_{kj}\left( \sum_{i=1}^p s_{ik} \overline{w_i}\right) = \sum_{i=1}^p \left(\sum_{k=1}^m s_{ik}t_{kj}\right)\overline{w_i}
	\end{align*}
	Therefore, $m(S \circ T) = \sum_{k=1}^m s_{ik}t_{kj} = (s_{ik})(t_{kj}) =  m(S)m(T)$.

\section{The Jacobian Matrix}

	Let $\overline{u_1}, \overline{u_2}, \cdots, \overline{u_n}$ be the unit co-ordinate vectors in $\mathbb{R}^n$ and $\overline{e_1}, \overline{e_2}, \cdots, \overline{e_m}$ be the unit co-ordinate vectors in $\mathbb{R}^m$. Let function $f : \mathbb{R}^n \to \mathbb{R}^m$ be differentiable at $\overline{c} \in \mathbb{R}^n$. Then there exists a linear function $T = f'(\overline{c}) : \mathbb{R}^n \to \mathbb{R}^m$ such that $f(\overline{c}+\overline{v}) = f(\overline{c})+f'(\overline{c})(\overline{v}) + \|\overline{v}\|+E_{\overline{c}}(\overline{v})$. We have, $T(\overline{u_k}) = f'(\overline{c})(\overline{u_k}) = f'(\overline{c},\overline{u_k}) = D_kf(\overline{c}) = D_k \sum_{i=1}^m f_i(\overline{c})\overline{e_i}$.\\

	Clearly, the matrix of total derivative $T$, $m(T) = (t_{ik}) = (D_k f_i(\overline{c}))$. This matrix is called Jacobian matrix of $f$ at $\overline{c}$ and is denoted by $Df(\overline{c})$.
	\[ Df(\overline{c}) = \begin{bmatrix} D_1 f_1(\overline{c}) & D_2 f_1(\overline{c}) & \cdots & D_n f_1(\overline{c}) \\ D_1 f_2(\overline{c}) & D_2 f_2(\overline{c}) & \cdots & D_n f_2(\overline{c}) \\ \vdots & \vdots & \ddots & \vdots \\ D_1 f_m(\overline{c}) & D_2 f_m(\overline{c}) & \cdots & D_n f_m(\overline{c}) \end{bmatrix} \]

\subsection{Properties of Jacobian matrix}
\begin{enumerate}
	\item $k$th row of $Df(\overline{c})$ is gradient vector of $f_k$
		\[\nabla f_k(\overline{c}) = (D_1f_k(\overline{c}),D_2f_k(\overline{c}),\cdots,D_nf_k(\overline{c}))\]
	\item When $m = 1$, $Df(\overline{c}) = \nabla f(\overline{c})$.
	\item $f'(\overline{c})(\overline{v}) = \sum_{k=1}^m \left(\nabla f_k(\overline{c}) \cdot \overline{v}\right) \overline{e_k}$
	\item $\|f'(\overline{c})(\overline{v})\| \le M\|v\|$ where $M = \sum_{k=1}^m \|\nabla f_k(\overline{c}\| $, by property (3)
	\item $f'(\overline{c})(\overline{v}) \to \overline{0}$ as $\overline{v} \to \overline{0}$, by property (4)
\end{enumerate}

\subsection{Chain Rule}
\begin{commentary}
	Chain Rule for real function : $\frac{d F \circ G}{dx}(x) = \frac{d}{dy}F(y) \frac{d}{dx}G(x) = F'(y)\ G'(x)$\\
	For example : $\frac{d}{dx} (ax+3)^3 = \frac{d}{dy}y^3 \frac{d}{dx} \left(ax+3\right) = 3ay^2 = 3a(ax+3)^2$
\end{commentary}
\begin{theorem}
	Let $g$ be differentiable at $\overline{a}$, with total derivative $g'(\overline{a})$ and $\overline{b} = g(\overline{a})$. Let $f$ is differentiable at $\overline{b}$, with total derivative $f'(\overline{b})$. Then $h = f \circ g$ is differentiable at $\overline{a}$ with total derivative $h'(\overline{a}) = f'(\overline{b}) \circ g'(\overline{a})$.
\begin{commentary}
	Try to read $h'(\overline{a}) = H$, $f'(\overline{b}) = F$, $g'(\overline{a}) = G$, then $H = F \circ G \implies H(x) = F(G(x))$
	In other words, $h'(\overline{a})(\overline{v}) = f'(\overline{b}) \circ g'(\overline{a})\ (\overline{v}) = f'(\overline{b})(g'(\overline{a})(\overline{v}))$.
\end{commentary}
\end{theorem}
\begin{proof}
	Given $\epsilon > 0$, let $y \in \mathbb{R}^p$ such that $\|y\| < \epsilon$. Let $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^p \to \mathbb{R}^n$, then $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.\\

	We have, $h(\overline{a}+\overline{y})-h(\overline{y}) = f(g(\overline{a}+\overline{y})) - f(g(\overline{a})) = f(\overline{b}+\overline{v}) - f(\overline{b})$ where $\overline{b} = g(\overline{a})$, and  $\overline{v} = g(\overline{a}+\overline{y})-g(\overline{a})$.\\

	Since $g$ is differentiable at $\overline{a}$, $g$ satisfies first-order Taylor's formula.
	\begin{align*}
		g(\overline{a}+\overline{y}) & =  g(\overline{a}) + g'(\overline{a})(\overline{y}) + \|\overline{y}\| E_{\overline{a}}(\overline{y}) \text{ where } E_{\overline{a}} \to \overline{0} \text{ as } \overline{y} \to \overline{0} \\
		\implies & \overline{v} = g(\overline{a}+\overline{y})-g(\overline{a}) = g'(\overline{a})(\overline{y}) + \|\overline{y}\| E_{\overline{a}}(\overline{y})
	\end{align*}
	Clearly, as $\overline{y} \to \overline{0} \implies \overline{v} \to g'(\overline{a})(\overline{0}) = \overline{0}$. Again,  we have $f$ is differentiable at $\overline{b}$, thus $f$ satisfies first-order Taylor's formula.
	\[ f(\overline{b}+\overline{v}) = f(\overline{b}) + f'(\overline{b})(\overline{v}) + \|\overline{v}\| E_{\overline{b}}(\overline{v}) \text{ where } E_{\overline{b}} \to \overline{0} \text{ as } \overline{v} \to \overline{0} \]
	\begin{align*}
		\implies f(\overline{b}+\overline{v}) - f(\overline{b}) & = f'(\overline{b})(\overline{v}) + \|\overline{v}\| E_{\overline{b}}(\overline{v}) \\
		& = f'(\overline{b})\left( g'(\overline{a})(\overline{y}) + \|\overline{y}\|E_{\overline{a}}(\overline{y}) \right) + \|\overline{v}\| E_{\overline{b}}(\overline{v}) \\
		& = f'(\overline{b})(g'(\overline{a})(\overline{y})) + \|\overline{y}\| E(\overline{y}) \\
		& \text{ where } E(\overline{y}) = f'(\overline{b})(E_{\overline{a}}(\overline{y})) + \frac{\|\overline{v}\|}{\|\overline{y}\|} E_{\overline{b}}(\overline{y}),\ \overline{y} \ne \overline{0}
	\end{align*}
	\[ \implies h(\overline{a}+\overline{y})-h(\overline{a}) = f(\overline{b}+\overline{v}) - f(\overline{b}) = f'(\overline{b})(g'(\overline{a})(\overline{y})) + \|\overline{y}\|E(\overline{y}) \]

	Since $f'(\overline{b})$ and $g'(\overline{a})$ are linear, their composition is also linear. Therefore, $h$ is differentiable at $\overline{a}$ with a linear, total derivative $h'(\overline{a}) = f'(\overline{b}) \circ g'(\overline{a})$ as it satisfies first-order Taylor's formula if $E_{\overline{y}} \to \overline{0}$ as $\overline{y} \to \overline{0}$. \\

	We have, $\|\overline{v}\| \le \|g'(\overline{a})(\overline{y})\| + \|\overline{y}\|\ \|E_{\overline{a}}(\overline{y})\| \le M\|y\| + \|E_{\overline{a}}(\overline{y})\|\ \|\overline{y}\|$.
	\[ \implies \frac{\|\overline{v}\|}{\|\overline{y}\|} \le M + \|E_{\overline{a}}(\overline{y})\| \]

	Thus, $\overline{v} \to \overline{0}$ as $\overline{y} \to \overline{0}$. Then $f'(\overline{b})(\overline{v}) \to f'(\overline{b})(\overline{0}) = \overline{0}$. And $E_{\overline{a}}(\overline{y}) \to \overline{0}$. Therefore, $E(\overline{y}) \to \overline{0} + M\overline{0} = \overline{0}$ as $\overline{y} \to \overline{0}$.
\end{proof}
\subsection{Matrix form of the chain rule}
	Let $f : \mathbb{R}^n \to \mathbb{R}^m$, $g : \mathbb{R}^p \to \mathbb{R}^n$. And $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$. Suppose $g$ is differentiable at $\overline{a} \in \mathbb{R}^p$ and $f$ is differentiable at $g(\overline{a}) = \overline{b} \in \mathbb{R}^n$. Then $h$ is differentiable at $\overline{a}$ and the Jacobian matrix of $h$ is given by the chain rule,
\[ Dh(\overline{a}) = Df(\overline{b})Dg(\overline{a}) \text{ where } h = f \circ g,\ \overline{b} = g(\overline{a})\]
In other words,
\[ D_jh_i(\overline{a}) = \sum_{k=1}^n D_k f_i(\overline{b}) D_j g_k(\overline{a}),\ i=1,2,\cdots,m,\ j=1,2,\cdots,p \]

	For $m=1$, $D_j h(\overline{a}) = \sum_{k=1}^n D_kf(\overline{b}) D_jg_k(\overline{a})$.\\
	
	For $m=1$ and $p=1$, $h'(\overline{a}) = \sum_{k=1}^n Df(\overline{b}) g_k'(\overline{a}) = \nabla f(\overline{b}) \cdot Dg(\overline{a})$.

\begin{theorem}
	Let $f$ and $D_2f$ be continuous functions on a rectangle $[a,b] \times [c,d]$. Let $p$ and $q$ be differentiable on $[c,d]$, where $p(y) \in [a,b]$ and $q(y) \in [c,d]$ for each $y \in [c,d]$. Define $F$ by the equation,
	\[ F(y) = \int_{p(y)}^{q(y)} f(x,y) dx,\ y \in [c,d] \]
	Then $F'(y)$ exists for each $y \in (c,d)$ and is given by,
	\[ F'(y) = \int_{p(y)}^{q(y)} D_2 f(x,y) dx + f((q,y),y)q'(y) - f(p(y),y)p'(y) \]
\end{theorem}
\begin{proof}
\end{proof}

---continue page 353 ---

%\chapter{Implicit Functions and Extremum Problems}
%\chapter{Multiple Riemann Integrals*}
%\chapter{Multiple Lebesgue Integrals*}
%\chapter{Cauchy's Theorem and the Residue Calculus*}
%\cite{rudin}

