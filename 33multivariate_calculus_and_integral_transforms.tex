%Text Books : \cite{apostol}, \cite{rudin}

%Module 1:
%The Weirstrass theorem, other forms of Fourier series, the Fourier integral theorem, the exponential form of the Fourier integral theorem, integral transforms and convolutions, the convolution theorem for Fourier transforms.
%(Chapter 11 Sections 11.15 to 11.21 of \cite{apostol}) (20 hours.)
%Module 2:
%Multivariable Differential Calculus, The directional derivative, directional derivatives and continuity, the total derivative, the total derivative expressed in terms of partial derivatives, An application of complex- valued functions, the matrix of a linear function, the Jacobian matrix, the matrix form of the chain rule. Implicit functions and extremum problems, the mean value theorem for differentiable functions,
%(Chapter 12 Sections. 12.1 to 12.11 of \cite{apostol}) (22 hours.)
%Module 3: 
%A sufficient condition for differentiability, a sufficient condition for equality of mixed partial derivatives, functions with non-zero Jacobian determinant, the inverse function theorem ,the implicit function theorem, extrema of real- valued functions of one variable, extrema of real-valued functions of several variables.
%Chapter 12 Sections-. 12.12 to 12.13 of \cite{apostol} 
%Chapter 13 Sections-. 13.1 to 13.6 of \cite{apostol} (28 hours.)
%Module 4:
%Integration of Differential Forms Integration, primitive mappings, partitions of unity, change of variables, differential forms.
%(Chapter 10 Sections. 10.1 to 10.14 of \cite{rudin}) (20 hours)

%Module 1 - \cite{apostol} 11
%Module 2 - \cite{apostol} 12
%Module 3 - \cite{apostol} 12, 13
%Module 4 - \cite{rudin} 10
%\cite{apostol}
%\chapter{The Real and Complex Number Systems*}
%\chapter{Some Basic Notations of Set Theory*}
%\chapter{Elements of Point Set Topology*}
%\chapter{Limits and Continuity*}
%\chapter{Derivatives*}
%\chapter{Functions of Bounded Variation and Rectifiable Curves*}
%\chapter{The Riemann-Stieltjes Integral*}
%\chapter{Infinite Series adn Infinite Products*}
%\chapter{Sequences of Functions*}
%\chapter{The Lebesgue Integral*}

%\chapter{Fourier Series and Fourier Integrals}
\section{Integral Transforms}
\subsection{The Weierstrass Approximation Theorem}
\begin{important}
Every continuous, real valued function on a compact interval has a polynomial approximation.\cite[Theorem 11.17]{apostol}
\end{important}
\begin{theorem}[Weierstrass]
Let $f$ be a real-valued, continuous function on a compact interval $[a,b]$.
Then for every \(\epsilon > 0\), there is a polynomial $p$ such that \(|f(x)-p(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{theorem}
\begin{synopsis}
Given a real-valued continuous function on compact interval $[a,b]$, we can construct a real-valued, continous function $g$ on $\mathbb{R}$ which is periodic with period $2\pi$.
We have, if \(f \in L(I)\) and $f$ is bounded almost everywhere in $I$, then \(f \in L^2(I)\).\cite[Theorem 10.52]{apostol}.
By Fejer's theorem (\cite[Theorem 11.15]{apostol}), the fourier series generated by $g$ (\cite[definition 11.3]{apostol}) converges to the Cesaro sum (\cite[Definition 8.47]{apostol}), which is $g$ itself in this case.
Thus for any \(\epsilon > 0\), there is a finite sum of trignometric functions.
The power series expansions of trignometric functions (\cite[definition 9.27]{apostol}) being uniformly convergent, there exists a polynomial $p_m$ which approximates $g$.
And we can construct $p$ (polynomial approximation of $g$) using $p_m$.
\end{synopsis}
\begin{proof}
Define \(g : \mathbb{R} \to \mathbb{R}\), \[ g(t) = \begin{cases} f(a+(b-a)t/\pi),\ t \in [0,\pi) \\ f(a+(2\pi-t)(b-a)/\pi),\ t \in [\pi,2\pi] \\ g(t - 2n\pi),\ t > 2\pi,\ n \in \mathbb{N} \\ g(t+2n\pi),\ t < 0,\ n \in \mathbb{N} \end{cases}\]

Thus $g$ is a continuous, real-valued, periodic function with period $2\pi$ such that
\begin{equation}
	f(x) = g\left(\frac{\pi (x-a)}{b-a}\right),\ x \in [a,b] \label{equ:fx}
\end{equation}

The fourier series generated by $g$ is given by, \[ g(t) \sim \frac{a_0}{2} + \sum_{k=1}^\infty \left( a_k \cos kt + b_k \sin kt \right)\] \[ \text{ where } a_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos kt\ dt,\ b_k = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin kt\ dt\]

Let \(\sequence{s_n(t)}\) be the sequence of partial sums of the fourier series generated by $g$.
And \( \sequence{\sigma_n(t)}\)  be the sequence of averages of $s_n(t)$ given by, \[\sigma_n(t) = \frac{1}{n} \sum_{k = 1}^n s_k(t),\text{ where } s_k(t) = \frac{a_0}{2} + \sum_{j = 1}^k \left( a_j \cos jt + b_j \sin jt \right)\]

Function \(f \in L(I)\) being real-valued continuous function on a compact interval, it is bounded and hence is Lebesgue square integrable. ie, \(f \in L^2(I)\).
Thus, \(g \in L^2(I)\).

Since $g$ is continous on $\mathbb{R}$, the function \(s : \mathbb{R} \to \mathbb{R}\) defined by, \[ s(t) = \lim_{h \to 0^+} \frac{g(t+h)-g(t-h)}{2} \] is well-defined on $\mathbb{R}$ and \(s(t) = g(t),\ \forall t \in \mathbb{R}\).

Then by Fejer's Theorem, the sequence \(\sequence{\sigma_n(t)}\) converges uniformly to $g(t)$ for every \(t \in \mathbb{R}\).
Thus, given \(\epsilon > 0\), there exists \(N \in \mathbb{N}\) such that \(\forall t \in \mathbb{R}\), \(|g(t)-\sigma_N(t)| < \frac{\epsilon}{2}\).

We have,
\begin{equation}
	\sigma_N(t) = \sum_{k=0}^N \left(A_k \cos kt + B_k \sin kt \right),\text{ where } A_k, B_k \in \mathbb{R}
	\label{equ:sigmaN}
\end{equation}
By the power series expansion of the trignometric functions about origin,
\begin{equation}
	\cos kt = \sum_{j = 1}^\infty \left(\frac{\cos^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty A_j' t^j \text{ where } A_j' \in \mathbb{R}
	\label{equ:coskt}
\end{equation}
\begin{equation}
	\sin kt = \sum_{j = 1}^\infty \left(\frac{\sin^{(j)} 0}{j!} (kt)^j \right)  = \sum_{j = 1}^\infty B_j' t^j \text{ where } B_j' \in \mathbb{R}
	\label{equ:sinkt}
\end{equation}

Since the above power series expansions of trignometric functions are uniformly convergent, their finite linear combination \(\sequence{\sigma_N(t)}\) is also uniformly convergent.
ie, Given \(\epsilon > 0\) there exists \(m \in \mathbb{N}\) such that for every \(t \in \mathbb{R}\)
\[\left|\sum_{k = 0}^m C_k t^k - \sigma_N(t)\right| < \frac{\epsilon}{2} \text{ where } C_k \in \mathbb{R}\]

Therefore, \(| p_m(t) - g(t)| \le | p_m(t) - \sigma_N(t) | + |\sigma_N(t) - g(t)| < \epsilon\) where \(p_m(t) = \sum_{k = 0}^m C_k t^k\).
Define \(p : [a,b] \to \mathbb{R}\) by,
\begin{equation}
	p(x) = p_m\left( \frac{\pi(x-a)}{b-a} \right)
	\label{equ:px}
\end{equation}

By equations \ref{equ:fx} and \ref{equ:px}, \(|p(x)-f(x)| < \epsilon\) for every \(x \in [a,b]\).
\end{proof}

\subsection{Other Forms of Fourier Series}

Let \(f \in L([0,2\pi])\), then the fourier series generated by $f$ is given by,
\[ f(x) \sim \frac{a_0}{2}+\sum_{n=1}^\infty \left( a_n \cos nx + b_n \sin nx \right) \]
\[ \text{ where } a_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \cos nt\ dt,\qquad b_n = \frac{1}{\pi} \int_0^{2\pi} f(t) \sin nt\ dt \]

By Euler's forumula \(e^{inx} = \cos nx + i\sin nx\).
We have, \(\cos nx = \frac{(e^{inx}+e^{-inx})}{2}\) and \(\sin nx = \frac{(e^{inx}-e^{-inx})}{2i}\)

\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( \alpha_n e^{inx} + \beta_n e^{-inx} \right) \]

\[ \text{ where } \alpha_n = \frac{(a_n - ib_n)}{2} \qquad \beta_n = \frac{(a_n+ib_n)}{2} \]

Therefore, by assigning \(\alpha_0 = a_0/2\), \(\alpha_{-n} = \beta_n\), we get the following exponential form of fourier series generated by $f$,

\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^{inx} \text{ where } \alpha_n = \frac{1}{2\pi} \int_0^{2\pi} f(t)\ e^{-int}\ dt \]

Note : If $f$ is periodic with period $2\pi$, then the interval of integration $[0,2\pi]$ can be replaced with any interval of length $2\pi$.
eg. $[-\pi,\pi]$

\subsubsection{Periodic with period $p$}
Let \(f \in L([0,p])\) and $f$ is periodic with period $p$.
Then
\[ f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( a_n \cos \frac{2\pi nx}{p} + b_n \sin \frac{2\pi nx}{p} \right) \]
\[ \text{ where } a_n = \frac{2}{p} \int_0^p f(t) \cos \frac{2\pi nt}{p}\ dt \qquad b_n = \frac{2}{p} \int_0^p f(t) \sin \frac{2\pi nt}{p}\ dt \]
Therefore, we have the exponential form of the above fourier series given by,
\[ f(x) \sim \sum_{n = -\infty}^\infty \alpha_n e^\frac{2\pi inx}{p},\text{ where } \alpha_n = \frac{1}{p} \int_0^p f(t)\ e^\frac{-2\pi int}{p}\ dt \]
	
\subsection{Fourier Integral Theorem}
\begin{theorem}[Fourier Integral Theorem]
Let \(f \in L(-\infty,\infty)\).
Suppose \(x \in \mathbb{R}\) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
\begin{enumerate}
	\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
	\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals \[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \] exists.
\end{enumerate}
Then, 
\[ \frac{f(x+)+f(x-)}{2} = \frac{1}{\pi} \int_0^\infty \int_{-\infty}^\infty f(u)\cos v(u-x)\ du\ dv, \] the integral $\int_0^\infty$ being an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\[ f(x+t)\frac{\sin \alpha t}{\pi t} dt \to f(u)\frac{\sin \alpha(u-x)}{\pi(u-x)} \to \frac{f(u)}{\pi} \int_0^\alpha \cos v(u-x) dv \]
By Riemann-Lebesgue lemma\cite[Theorem 11.6]{apostol},
\[ f \in L(I) \implies \lim_{\alpha \to +\infty} \int_I f(x) \sin \alpha t\ dt = 0 \]
By Jordan's Theorem\cite[Theorem 10.8]{apostol}, if $g$ is of bounded variation on $[0,\delta]$, then
\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
By Dini's Theorem\cite[Theorem 10.9]{apostol}, if the limit $g(x+)$ exists and Lebesgue integral \( \int_0^\delta \frac{g(t)+g(0+)}{t} dt \) exists for some \( \delta > 0 \), then
\[ \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) \]
The order of Lebesgue integrals can be interchanged.\cite[Theorem 10.40]{apostol}

Suppose \(f \in L(X)\) and \(g \in L(Y)\).
Then \[ \int_X f(x) \left(\int_Y g(y) k(x,y) dy \right) dx = \int_Y g(y) \left( \int_X f(x) k(x,y) dx \right) dy \]
\end{synopsis}
\begin{proof}
	Consider \( \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \).
	We prove that this integral is equal to the either sides.
	\[ \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt = \int_{-\infty}^{-\delta} + \int_{-\delta}^0 + \int_0^{-\delta}  + \int_{\delta}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt \] 
	We have, function \( \frac{f(x+t)}{\pi t} \) is bounded on \( (-\infty,-\delta)\cup(\delta,\infty) \), hence \( \frac{f(x+t)}{\pi t} \) is Lebesgue integrable on \( (-\infty,-\delta) \cup (\delta,\infty) \).
	
	By Riemann Lebesgue lemma, 
	\[ \frac{f(x+t)}{\pi t} \in L(-\infty,-\delta) \implies \int_{-\infty}^{-\delta} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0, \]
	\[ \frac{f(x+t)}{\pi t} \in L(\delta,\infty) \implies \int_{\delta}^{\infty} f(x+t) \frac{\sin \alpha t}{\pi t} dt = 0 \]

	\paragraph{Case 1}
	Suppose $f$ is of bounded variation on $[x-\delta,x+\delta]$, put \( g(t) = f(x+t) \) then $g$ is of bounded variation on $[-\delta,\delta]$.
	Thus $g$ is of bounded variation on $[0,\delta]$.
	Then by Jordan's Theorem
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	\paragraph{Case 2}
	Suppose both the limits $f(x+)$ and $f(x-)$ exists and both Lebesgue integrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and } \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \]
	exists.

	Thus, we have $f(x+)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \) exists.
	Put \( g(t) = f(x+t) \), then \( g(0+) = f(x+) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\[ \lim_{\alpha \to +\infty} \frac{2}{\pi}\int_0^\delta f(x+t)\frac{\sin \alpha t}{t} dt = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(t) \frac{\sin \alpha t}{t} dt = g(0+) = f(x+) \]

	Similarly, $f(x-)$ exists and the Lebesgue integral \( \int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \) exists.
	Put \( g(t) = f(x-t) \), then \( g(0+) = f(x-) \) exists and the Lebesgue integral \( \int_0^\delta \frac{g(t)-g(0+)}{t} dt \) exists, then by Dini's Theorem,
	\begin{align*}
		\lim_{\alpha \to +\infty} \frac{2}{\pi}\int_{-\delta}^0 f(x+t)\frac{\sin \alpha t}{t} dt 
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta f(x-\tau) \frac{\sin \alpha \tau}{\tau} d\tau\\
		& = \lim_{\alpha \to +\infty} \frac{2}{\pi} \int_0^\delta g(\tau) \frac{\sin \alpha \tau}{\tau} d\tau = g(0+) = f(x-)
	\end{align*}

	Then by either cases,
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x+t) \frac{\sin \alpha t}{\pi t} dt  
		& = \lim_{\alpha \to +\infty} \int_{-\delta}^0 + \int_0^\delta f(x+t) \frac{\sin \alpha t}{\pi t} dt \\
		& = \frac{f(x+)+f(x-)}{2}
	\end{align*}

	We have, \( \int_0^\alpha \cos v(u-x) dv = \frac{\sin v(u-x)}{u-x} \).
	\begin{align*}
		\lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt 
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \frac{ \sin \alpha (u-x)}{u-x} du,\ (\text{put }u = x+t)\\
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(u) \left( \int_0^\alpha \cos v(u-x) dv \right) du\\
		& = \lim_{\alpha \to +\infty} \int_0^\alpha \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv,\\
		& \text{since, the order of Lebesgue integrals can be reversed.}\\
		& = \int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv\\
		\text{where, } \int_0^\infty \text{ is not }& \text{a Lebesgue integral, but an improper Riemann integral }
	\end{align*}
	Therefore,
	\begin{align*}
		\int_0^\infty \left( \int_{-\infty}^\infty f(u) \cos v(u-x) du \right) dv
		& = \lim_{\alpha \to +\infty} \int_{-\infty}^\infty f(x) \frac{ \sin \alpha t}{\pi t} dt \\
		& =  \frac{f(x+)+f(x-)}{2}
	\end{align*}
\end{proof}

\begin{remark}
	If a function $f$ on $(-\infty,\infty)$ is non-periodic, then it may not have a fourier series represenation.
	In such cases, we have fourier intergral representaion.
\end{remark}

\subsection{Exponential form of Fourier Integral Theorem}
Let \( f \in L(-\infty,\infty) \).
Suppose \( x \in \mathbb{R} \) and an interval $[x-\delta,x+\delta]$ about $x$ such that either 
\begin{enumerate}
	\item $f$ is of bounded variation on an interval $[x-\delta,x+\delta]$ about $x$ or
	\item both limits $f(x+)$ and $f(x-)$ exists and both Lebesgue intergrals
	\[ \int_0^\delta \frac{f(x+t)-f(x+)}{t} dt \text{ and }\int_0^\delta \frac{f(x-t)-f(x-)}{t} dt \] exists.
\end{enumerate}
Then, \[ \frac{f(x+)+f(x-)}{2} = \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)}\ du\right) dv \]
\begin{proof}
Let \( F(v) = \int_{-\infty}^\infty f(u) \cos v(u-x) du \).
Then \( F(v) = F(-v) \) and 
\begin{align*}
	\lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) dv
	& = \lim_{\alpha \to \infty} \frac{1}{\pi} \int_0^\alpha \int_{-\infty}^\infty f(u) \cos v(u-x) du dv\\
	& = \frac{f(x+)+f(x-)}{2}
\end{align*}
Let \( G(v) = \int_{-\infty}^\infty f(u) \sin v(u-x) du \).
Then \( G(v) = -G(-v) \) and
\[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha G(v) dv = 0 \]
Thus \[ \lim_{\alpha \to \infty} \frac{1}{2\pi} \int_{-\alpha}^\alpha F(v) + iG(v) dv = \frac{f(x+)+f(x-)}{2} \]
\end{proof}

\subsection{Integral Transforms}
\begin{definition}
	Integral transform $g(y)$ of $f(x)$ is a Lebesgue integral or Improper Riemann integral of the form
	\[ g(y) = \int_{-\infty}^\infty K(x,y) f(x)\ dx \], where $K$ is the kernal of the transform.
	We write \( g = \mathscr{K}(f) \).
\end{definition}

\begin{remark}
	Integral transforms(operators) are linear operators.
	 ie, \( \mathscr{K}(af_1 + bf_2) = a\mathscr{K}f_1 + b\mathscr{K}f_2 \)
\end{remark}

\begin{remark} A few commonly used integral transforms,
\begin{enumerate}
	\item Exponential Fourier Transform $\mathscr{F}$,
		\[ \mathscr{F}f = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	\item Fourier Cosine Transform $\mathscr{C}$,
		\[ \mathscr{C}f = \int_0^\infty \cos xy f(x)\ dx \]
	\item Fourier Sine Transform $\mathscr{S}$,
		\[ \mathscr{S}f = \int_0^\infty \sin xy f(x)\ dx \]
	\item Laplace Transform $\mathscr{L}$,
		\[ \mathscr{L}f = \int_0^\infty e^{-xy} f(x)\ dx \]
	\item Mellin Transform $\mathscr{M}$,
		\[ \mathscr{M}f = \int_0^\infty x^{y-1}f(x)\ dx \]
\end{enumerate}
\end{remark}

\begin{remark} Suppose \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_{-\infty}^\infty e^{-ixy}f(x)\ dx = \int_0^\infty e^{-ixy}f(x)\ dx = \int_0^\infty \cos xy \ f(x)\ dx + i \int_0^\infty \sin xy \ f(x)\ dx \]
	\[ \mathscr{F}f = \mathscr{C}f + i\mathscr{S}f \]

	Therefore Fourier Cosine $\mathscr{C}$ and Sine $\mathscr{S}$ transforms are special cases of fourier integral transform, $\mathscr{F}$ provided $f$ vanishes on negative real axis.
\end{remark}

\begin{remark} Let \( y = u+iv \), \( f(x) = 0,\ \forall x < 0 \).
	\[ \int_0^\infty e^{-xy}f(x) = \int_0^\infty e^{-xu}e^{-ixv}f(x)\ dx = \int_0^\infty e^{-ixv} \phi_u(x) dx \]
	where \( \phi_u(x) = e^{-xu}f(x) \).
	\[ \mathscr{L}f = \mathscr{F}\phi_u \]
	Therefore Laplace transform, $\mathscr{L}$ is a special case of Fourier integral transform, $\mathscr{F}$.
\end{remark}

\begin{remark} Let \( g(y) = \mathscr{F}f(x) \).
	\[ g(y) = \int_{-\infty}^\infty e^{-ixy}f(x)\ dx \]
	Suppose $f$ is continuous at $x$, then by fourier integral theorem,
	\begin{align*}
		f(x)	& = \frac{1}{2\pi} \int_{-\infty}^\infty \left( \int_{-\infty}^\infty f(u) e^{iv(u-x)} du \right) dv\\
			& = \int_{-\infty}^\infty e^{-ivx} \left( \frac{1}{2\pi} \int_{-\infty}^\infty e^{ivu} f(u)\ du \right) dv\\
			& = \int_{-\infty}^\infty g(v) e^{-ivx} dv = \mathscr{F}g \text{ where } g(v) = \frac{1}{2\pi}\int_{-\infty}^\infty f(u) e^{ivu} du 
	\end{align*}
	The above function $g(v)$ gives the \textbf{inverse fourier transformation} of $f$.

	Let $g$ be fourier transform of $f$, then $f$ is uniquely determined by its fourier transform $g$ by,
	\[ f(x) = \mathscr{F}^{-1}g(y) = \frac{1}{2\pi} \lim_{\alpha \to \infty} \int_{-\alpha}^\alpha g(y) e^{ixy} dy \]
\end{remark}

\begin{enumerate}
	\setcounter{enumi}{5}
	\item Inverse Fourier Transform $\mathscr{F}^{-1}$,
		\[ \mathscr{F}^{-1}f = \int_{-\infty}^\infty \frac{e^{ixy}}{2\pi}f(x)\ dx \]
\end{enumerate}

\subsection{Convolutions}
\begin{definition}
	Let \( f,g \in L(-\infty,\infty) \).
	Let $S$ be the set of all points $x$ for which the Lebesgue integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists.
	Then the function \( h : S \to \mathbb{R} \) is a convolution of $f$ and $g$.
	And \( h = f \ast g \).
\end{definition}

\begin{remark}
	Convolution operator is commutative.
	ie, \( h = f \ast g = g \ast f \)
	\begin{commentary}
		(hint : take $u = x-t$)
	\end{commentary}
	% $u = x-t \implies du = -dt$
	% sign is reversed as the order of limits are switched.
	% ie, $t = \infty \to u = -\infty$
\end{remark}

\begin{remark}
	Suppose $f,g$ vanishes on negative real axis, then
	\[ h(x) = \int_{-\infty}^\infty f(t)\ g(x-t)\ dt = \int_{-\infty}^0 + \int_0^x  + \int_x^\infty f(t)\ g(x-t)\ dt = \int_0^x f(t)\ g(x-t)\ dt \] 
\end{remark}

\begin{remark}
	Singularity of convolution is a point at which the convolution integral fails to exists.
\end{remark}

\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and either $f$ or $g$ is bounded in $\mathbb{R}$.
	Then the convoluton integral
	\[ h(x) = \int_{-\infty}^\infty f(t) g(x-t) dt \]
	exists for every \( x \in \mathbb{R} \) and the function $h$ so defined is bouned in $\mathbb{R}$.
	In addition, if the bounded function is continuous on $\mathbb{R}$, then $h$ is continuous and \( h \in L(\mathbb{R}) \).
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}
	If $f,g$ are both unbounded, the convolution integral may not exist.
	\[ \text{ eg: } f(t) = \frac{1}{\sqrt{t}},\ g(t) = \frac{1}{\sqrt{1-t}} \]
\end{remark}

\begin{theorem}
	Let \( f,g \in L^2(\mathbb{R}) \).
	Then the convolution integral $f \ast g$ exists for each \( x \in \mathbb{R} \) and the function \( h : \mathbb{R} \to \mathbb{R} \) defined by \( h(x) = f \ast g (x) \) is bounded in $\mathbb{R}$.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\subsection{The Convolution Theorem for Fourier Tranforms}
\begin{theorem}
	Let \( f,g \in L(\mathbb{R}) \) and at least one of $f$ or $g$ is continuous and bounded on $\mathbb{R}$.
	Let \( h = f \ast g \).
	Then for every real $u$,
	\[ \int_{-\infty}^\infty h(x) e^{-ixu} dx = \left( \int_{-\infty}^\infty f(t) e^{-itu} dt \right) \left( \int_{-\infty}^\infty g(y) e^{-iyu} dy \right) \]
	The integral on the left exists both as a Lebesgue integral and an improper Riemann integral.
\end{theorem}
\begin{synopsis}
\end{synopsis}
\begin{proof}
\end{proof}

\begin{remark}[Application of Convolution Theorem]
	\[ B(p,q) = \frac{\Gamma{p} \Gamma{q}}{\Gamma{p+q}},\text{ where } B(p,q) = \int_0^1 x^{p-1} (1-x)^{q-1} dx,\ \Gamma{p} = \int_0^\infty t^{p-1} e^{-t} dt \]
\end{remark}

\section{Multivariate Differential Calculus}

In this chapter, we deal with real functions of several variables.
Instead of $\mathbf{c}$, we write \( \bar{c} \in \mathbb{R}^n \), then \( \bar{c} = (c_1, c_2, \dotsc, c_n) \) where \( c_j \in \mathbb{R} \) for every \(j = 1,2, \dotsc, n\).
Again, suppose \(f : \mathbb{R}^n \to \mathbb{R}^m\) and \(f(\bar{x}) = \bar{y}\), then \(\bar{y} = (y_1, y_2, \dotsc, y_m)\) where each $y_k$ is real.
The unit co-ordinate vector, $\bar{u}_k$ is given by \( {u_k}_j = \delta_{j,k} \)

\subsection{Directional Derivative}
\textsl{Motivation : The existence of all partial derivatives of a multivariate real function $f$ at a point $\bar{c}$ doesn't imply the continuity of $f$ at $\bar{c}$.
	Thus, we need a suitable generalisation for the partial derivative which could characterise continuity.
	And directional derivative is such an attempt.}

\begin{definition}[Directional Derivative]
	Let \(S \subset \mathbb{R}^n\) and \(f : S \to \mathbb{R}^m\).
	Let $\bar{c}$ be an interior points of $S$ and \( \bar{u} \in \mathbb{R}^n \), then there exists an open ball $B(\bar{c},r)$ in $S$.
	Also for some $\delta > 0$ the line segment \( \alpha : [0,\delta] \to S \) given by \( \alpha(t) = \bar{c}+t\bar{u} \) lie in $B(\bar{c},r)$.
	
	Then the Directional derivative of $f$ at an interior point $\bar{c}$ in the direction $\bar{u}$ is given by
	\[ f'(\bar{c},\bar{u}) = \lim_{h \to 0} \frac{f(\bar{c}+h\bar{u}) - f(\bar{c})}{h} \]
\end{definition}

\begin{remark}
	The direction derivative of $f$ at an interior point $\bar{c}$ in the direction $\bar{u}$ exists only if the above limit exists.
\end{remark}

\begin{remark}Example, \cite[Exercise 12.2a]{apostol}

Suppose \(\bar{x},\bar{a},\bar{c},\bar{u} \in \mathbb{R}^n\).
Let \(f : \mathbb{R}^n \to \mathbb{R}\) such that \(f(\bar{x}) = \bar{a}\cdot\bar{x}\).
Then \[ f'(\bar{c},\bar{u}) = \lim_{h \to 0} \frac{\bar{a}\cdot(\bar{c}+h\bar{u}) - \bar{a} \cdot \bar{c}}{h} = \bar{a} \cdot \bar{u}\]
\end{remark}

\begin{remark}[Properties] Let \(f : S \to \mathbb{R}^m \), where \( S \subset \mathbb{R}^n\)
\begin{enumerate}
	\item \( f'(\bar{c},\bar{0}) = \bar{0} \)\\
	\textsl{Note : The zero vectors belongs to $\mathbb{R}^n, \mathbb{R}^m$ respectively.}
	\item \( f'(\bar{c},\bar{u}_k) = \frac{\partial f}{\partial u_k}(\bar{c}) = D_k f(\bar{c}) \), the $k^{th}$ partial derivative of $f$.
	\item Let \( f = (f_1, f_2, \cdots, f_m), \text{ such that } f(\bar{c}) = \left(f_1(\bar{c}),f_2(\bar{c}),\dotsc,f_m(\bar{c})\right) \).
	Then, 
		\[ \exists f'(\bar{c},\bar{u}) \iff \forall k, \exists f_k'(\bar{c},\bar{u}) \text{ and } f'(\bar{c},\bar{u}) = \left(f_1'(\bar{c},\bar{u}),f_2'(\bar{c},\bar{u}),\dotsc,f_m'(\bar{c},\bar{u})\right) \]
	ie, Directional derivative of $f$ exists iff directional derivative of each component function $f_k$ exists.
	And the components of the directional derivatives of $f$ are the directional derivaties of the components of $f$.

	Thus \( D_k f(\bar{c}) = \left(D_k f_1(\bar{c}),D_k f_2(\bar{c}),\dotsc,D_k f_m(\bar{c}) \right) \) holds.
	\item Let \( F(t) = f(\bar{c}+t\bar{u}) \), then \( F'(0) = f'(\bar{c},\bar{u}) \) and \( F'(t) = f'(\bar{c}+t\bar{u},\bar{u}) \)
	\item Let \( f(\bar{c}) = \bar{c} \cdot \bar{c} = \|\bar{c}\|^2 \), and \(F(t) = f(\bar{c}+t\bar{u}) \), then \( F'(t) = 2\bar{c} \cdot \bar{u}+2t\|\bar{u}\|^2 \) and \( F'(0) = f'(\bar{c},\bar{u}) = 2\bar{c} \cdot \bar{u} \)
	\item Let \(f\) be linear, then \( f'(\bar{c},\bar{u}) = f(\bar{u}) \)
	\item Existence of all partial derivatives doesn't imply existence of all directional derivatives.
	\[ f(x,y) = \begin{cases} x+y \qquad \text{ if } x = 0 \text{ or } y = 0 \\ 1 \qquad \qquad \text{otherwise} \end{cases} \]
	For above \(f\), directional derivatives exists only along the co\nobreakdash-ordinates (ie, partial derivatives).
	\item Existence of all directional derivatives doesn't imply continuity.
	\[ f(x,y) = \begin{cases} xy^2(x^2+y^4) \qquad x \ne 0 \\ 0 \hspace{2.5cm} x = 0 \end{cases} \]
	Above \(f\) is discontinuous at \((0,0)\), however all directional derivatives exists and has finite value.
	\end{enumerate}
\end{remark}

\subsection{Total Derivative}
We may define a total derivative \( T_c(h) = hf'(c) \) in the case of real-functions of single variable as follows :-

\[ \text{Let }E_c(h) = \begin{cases} \frac{f(c+h)-f(c)}{h} - f'(c),\qquad h \ne 0 \\ 0,\hspace{3.4cm} h = 0 \end{cases} \]
Then, \( f(c+h) = f(c) + hf'(c) + hE_c(h) \) and as \( h \to 0 \), \( E_c(h) \to 0\).
Also \( T_c(h) = f'(c)h \) is a linear function of $h$.
ie, \( T_c(ah_1+bh_2) = aT_c(h_1)+bT_c(h_2) \).
Now, we will define a total derivative of multivariate function that has these two properties.

\begin{definition}[Total Derivative]
	The function \( f: \mathbb{R}^n \to \mathbb{R}^m \) is differentiable at $\bar{c}$ if there exists a \textbf{linear} function \( T_{\bar{c}} : \mathbb{R}^n \to \mathbb{R}^m \) such that \( f(\bar{c}+\bar{v}) = f(\bar{c}) + T_{\bar{c}}(\bar{v}) + \|\bar{v}\| E_{\bar{c}}(\bar{v}) \) where \( E_{\bar{c}}(\bar{v}) \to \bar{0} \) as \( \bar{v} \to \bar{0} \).
\end{definition}

The linear function $T_{\bar{c}}$ is the total derivative of $f$ at $\bar{c}$, \( T_{\bar{c}}(\bar{0}) = \bar{0} \) and the condition above gives the First Order Taylor's Formula for \( f(\bar{c}+\bar{v})-f(\bar{c}) \).

\begin{remark}[Properties] Let \( f : \mathbb{R}^n \to \mathbb{R}^m \) and \( f'(\bar{c})(\bar{v}) = T_{\bar{c}}(\bar{v}) \) be the total derivative of $f$ at $\bar{c}$ evaluated at $\bar{v}$.
Then, 
\begin{enumerate}
	\item \( f'(\bar{c})(\bar{v}) = f'(\bar{c},\bar{u}) \)
	\item If $f$ is differentiable at $\bar{c}$, then $f$ is continuous at $\bar{c}$.
	\item \( f'(\bar{c})(\bar{v}) = v_1 D_1 f(\bar{c}) + v_2 D_2 f(\bar{c}) + \dots + v_n D_n f(\bar{c}) \)
	\end{enumerate}
\end{remark}

\begin{note}
The above $f'$ is a function from $\mathbb{R}^n$ to the set of all linear functions \( \mathscr{L} = \{ h : \mathbb{R}^n \to \mathbb{R}^m\} \).
$f'(\bar{c})$ is a linear function (in fact, total derivative $T_{\bar{c}}$) which maps $\bar{v}$ into the directional derivatives of $f$ at $\bar{c}$ in the direction $\bar{v}$.
This notation generalises $f'$ for univariate $f$ as well.(put $n=m=1$)

In this subject, we use the following notations,
\begin{description}
	\item[$D_kf(\bar{c})$] partial derivative
	\item[$f'(\bar{c},\bar{v})$] directional derivative
	\item[$f'(\bar{c})(\bar{v})$] total derivative
	\item[$\nabla{}f(\bar{c})$] gradient vector
\end{description}
\end{note}

\begin{theorem}
If $f$ is differentiable at $\bar{c}$ with total derivative $T_{\bar{c}}$, then for every $\bar{u} \in \mathbb{R}^n$, $T_{\bar{c}}(\bar{u}) = f'(\bar{c},\bar{u})$.
( ie, \( f'(\bar{c})(\bar{v}) = f'(\bar{c},\bar{v}) \) )
\end{theorem}
\begin{proof}
For \( \bar{v} = \bar{0}$, we have $T_{\bar{c}}(\bar{0}) = 0 = f'(\bar{c},\bar{0}) \).

Suppose \( \bar{v} \ne \bar{0} \), then put \( \bar{v} = h\bar{u} \).
Since $f$ is differentiable at $\bar{c}$, $f$ has total derivative at $\bar{c}$.
That is, there exists a linear function $T_{\bar{c}}$ such that \( f(\bar{c}+h\bar{u}) = f(\bar{c}) + T_{\bar{c}}(h\bar{u}) + \|h\bar{u}\|E_{\bar{c}} (h\bar{u}) \) where $E_{\bar{c}}(h\bar{u}) \to \bar{0}$ as $h\bar{u} \to \bar{0}$.
\begin{align*}
	\implies  & f(\bar{c}+h\bar{u}) = f(\bar{c}) + hT_{\bar{c}}(\bar{u}) + |h|\|\bar{u}\|E_{\bar{c}} (h\bar{u}),\ E_{\bar{c}}(h\bar{u}) \to \bar{0} \text{ as } h\bar{u} \to \bar{0} \\
	\implies  & \frac{f(\bar{c}+h\bar{u}) - f(\bar{c})}{h} = T_{\bar{c}}(\bar{u}) + \frac{|h|\|\bar{u}\|E_{\bar{c}}(h\bar{u})}{h},\  E_{\bar{c}}(h\bar{u}) \to \bar{0} \text{ as } h \to 0 \\
	\implies  & \lim_{h \to 0} \frac{f(\bar{c}+h\bar{u}) - f(\bar{c})}{h} = T_{\bar{c}}(\bar{u}) + \lim_{h \to 0} \frac{|h|\|\bar{u}\|E_{\bar{c}}(h\bar{u})}{h} \\
	\implies & f'(\bar{c},\bar{u}) = T_{\bar{c}}(\bar{u})
\end{align*}
\end{proof}

\begin{note}
$T_{\bar{c}}$ is linear, however $E_{\bar{c}}$ is not linear.
Thus $E_{\bar{c}}(h\bar{u}) \ne h E_{\bar{c}} (\bar{u})$.

As $h \to 0$, $h\bar{u} \to \bar{0}$ and \( E_{\bar{c}}(h\bar{u}) \to \bar{0}$.
Since the order of the function \( E_{\bar{c}}(h\bar{u}) \) is much smaller than that of $h$, the limit on the right converges to 0.
\end{note}

\begin{theorem}
If $f$ is differentiable at \( \bar{c} \), then $f$ is continuous at \( \bar{c} \).
\end{theorem}
\begin{proof}
Let $\bar{v} \ne 0$, then
\begin{align*}
	\bar{v} = v_1 \bar{u}_1 & + v_2 \bar{u}_2 + \dots + v_n \bar{u}_n,\\
	\bar{v} \to \bar{0} \implies & \forall j,\ v_j \to 0 \\
	T \text{ is linear }\implies & T_{\bar{c}} (\bar{v}) = v_1 T_{\bar{c}}(\bar{u}_1) + v_2 T_{\bar{c}}(\bar{u}_2) + \dots + v_n T_{\bar{c}}(\bar{u}_n)\\
	\text{Thus, } & T_{\bar{c}}(\bar{v}) \to \bar{0} \text{ as } \bar{v} \to 0
\end{align*}
Since $f$ differentiable at \( \bar{c} \), there exists linear function $T_{\bar{c}}$ such that
\begin{align*}
	f(\bar{c}+\bar{v}) & =  f(\bar{c}) + T_{\bar{c}}(\bar{v}) + \|v\|E_{\bar{c}}(\bar{v}) \\
	\implies & \lim_{\bar{v} \to \bar{0}} f(\bar{c}+\bar{v}) = f(\bar{c}) + \lim_{\bar{v} \to \bar{0}} T_{\bar{c}}(\bar{v}) + \lim_{\bar{v} \to \bar{0}} \|v\|E_{\bar{c}}(\bar{v})\\
	\implies & \lim_{\bar{v} \to \bar{0}} f(\bar{c}+\bar{v}) = f(\bar{c})
\end{align*}
\end{proof}

\begin{theorem}
Let $S \subset \mathbb{R}^n$ and $f : S \to \mathbb{R}^m$ be differentiable at an interior point $\bar{c}$ of $S$, where $S \subseteq \mathbb{R}^n$.
If $\bar{v} = v_1\bar{u}_1+v_2\bar{u}_2 + \cdots + v_n\bar{u}_n$, then
\[ f'(\bar{c})(\bar{v}) = \sum_{k=1}^n v_k D_k f(\bar{c}) \]
In particular, if $f$ is real-valued $(m = 1)$ we have, $f'(\bar{c})(\bar{v}) = \nabla{}f(\bar{c}).\bar{v}$
\end{theorem}
\begin{proof}
Suppose $f : S \to \mathbb{R}^m$ is differentiable at $\bar{c}$, then there exists a linear function $f'(\bar{c}) : S \to \mathbb{R}^m$ such that $f(\bar{c}+\bar{v}) = f(\bar{c}) + f'(\bar{c})(\bar{v}) + \|\bar{v}\| E_{\bar{c}}(\bar{c})$ where $E_{\bar{c}} \to \bar{0}$ as $\bar{v} \to \bar{0}$.
\begin{align*}
	f'(\bar{c})(\bar{v}) & = f'(\bar{c})\left(\sum_{k=1}^n v_k \bar{u}_k\right)\\
	& = \sum_{k=1}^n v_k f'(\bar{c})(\bar{u}_k), \text{ since $f'(\bar{c})$ is linear}\\
	& = \sum_{k=1}^n v_k D_k f(\bar{c}), \text{ since $f'(\bar{c})(\bar{u}_k) = f'(\bar{c},\bar{u}_k) = D_k f(\bar{c})$}\\
	\intertext{Let $m = 1$, then $f : S \to \mathbb{R}$ }
	f'(\bar{c})(\bar{v}) & = \sum_{k=1}^n v_k D_k f(\bar{c}) = \nabla{}f(\bar{c}).\bar{v}\\
	& \text{ since } \nabla{}f(\bar{c}) = \left( D_1f(\bar{c}),\ D_2f(\bar{c}),\ \cdots ,\ D_nf(\bar{c}) \right)
\end{align*}
\end{proof}

\begin{remark}
Let $f : S \to \mathbb{R}$, then $f(\bar{c} +\bar{v}) = f (\bar{c}) + \nabla{}f(\bar{c}).\bar{v} + o(\|\bar{v}\|)$ as $\bar{v} \to \bar{0}$.
\end{remark}

\begin{remark}[Complex-valued Functions]
\end{remark}

\subsection{Matrix of Linear Function}
Let $T : \mathbb{R}^n \to \mathbb{R}^m$ be a linear function.
Let $\{\bar{u}_1,\ \bar{u}_2,\ \cdots,\ \bar{u}_n\}$ be standard basis for $\mathbb{R}^n$ and  $\{\bar{e}_1,\ \bar{e}_2,\ \cdots,\ \bar{e}_m\}$ be standard basis for $\mathbb{R}^m$.
Let $\bar{v} \in \mathbb{R}^n$, then $\bar{v} = \sum_{k=1}^n v_k\bar{u}_k$ and $T(\bar{v}) = \sum_{k=1}^n v_k T(\bar{u}_k)$ and
\begin{commentary}
\begin{align*}
	T(\bar{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} T(\bar{u}_1) \\ T(\bar{u}_2) \\ \cdots \\ T(\bar{u}_n) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}\bar{e}_1+t_{21}\bar{e}_2+\cdots+t_{m1}\bar{e}_m \\ t_{12}\bar{e}_1+t_{22}\bar{e}_2+\cdots+t_{m2}\bar{e}_m \\ \cdots \\ t_{1n}\bar{e}_1+t_{2n}\bar{e}_2+\cdots+t_{mn}\bar{e}_m \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} \bar{e}_1 \\ \bar{e}_2 \\ \cdots \\ \bar{e}_m \end{bmatrix} \\
	\intertext{ We may take the transpose,}
	T(\bar{v}) & = \begin{bmatrix} \bar{e}_1 & \bar{e}_2 & \cdots & \bar{e}_m \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \cdots & t_{1n} \\ t_{21} & t_{22} & \cdots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n  \end{bmatrix} 
\end{align*}
\end{commentary}
\[ T(\bar{v}) = T \left( \sum_{k=1}^n v_k \bar{u}_k \right) = \sum_{k=1}^n v_k T(\bar{u}_k) = \sum_{k=1}^n v_n \sum_{j=1}^m t_{kj}\bar{e}_j \]
Thus matrix of $T$ is given by, $m(T) = (t_{ik})$ where $T(\bar{u}_k) = \sum_{k=1}^n t_{ik}\bar{e}_i$.
\begin{commentary}
\begin{remark}[Example]
Let $T : \mathbb{R}^3 \to \mathbb{R}^2$ defined by $T(x,y,z)=(2x+y,y-z)$.
\begin{align*}
	T(1,2,3) = & T((1,0,0) + 2(0,1,0) + 3(0,0,1)) \\
	= & T(\bar{u}_1+2\bar{u}_2+3\bar{u}_3) \\
	= & T(\bar{u}_1) + 2T(\bar{u}_2) + 3T(\bar{u}_3) \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} T(\bar{u}_1) \\ T(\bar{u}_2) \\ T(\bar{u}_3) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} (2,0) \\ (1,1) \\ (0,-1) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2(1,0) \\ (1,0)+(0,1) \\ -1(0,1) \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2\bar{e}_1 \\ \bar{e}_1+\bar{e}_2 \\ -\bar{e}_2 \end{bmatrix} \\
	= & \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 1 & 1 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} \bar{e}_1 \\ \bar{e}_2 \end{bmatrix} \\
	= & 4\bar{e}_1-\bar{e}_2 = 4(1,0) - 1(0,1) = (4,-1)
\end{align*}
\[ \text{ In the above case, }m(T) = \begin{bmatrix} 2 & 0 \\ 1 & 1\\ 0 & -1 \end{bmatrix}\]
Using the matrix of linear function $m(T)$, we can compute the image of any point in $R^3$ by matrix multiplication.
\end{remark}
\end{commentary}

\subsubsection{Matrix of the composition of two linear functions}
Let $T : \mathbb{R}^n \to \mathbb{R}^m$ and $S : \mathbb{R}^m \to \mathbb{R}^p$ be two linear functions with domain of $S$ containing the range of $T$ (so that $S \circ T$ is well defined).
Then $S \circ T : \mathbb{R}^n \to \mathbb{R}^p$ is defined by
\[ S \circ T(\bar{x}) = S(T(\bar{x})),\ \forall \bar{x} \in \mathbb{R}^n\]
Since $S,T$ are linear, $S \circ T$ is also linear.
\begin{align*}
	S \circ T(a \bar{x} + b \bar{y}) = & S(T(a \bar{x} + b\bar{y})) = S(a T(\bar{x}) + b T(\bar{y})) = a S(T(\bar{x}))) + b S(T(\bar{y})) \\
	= & a S \circ T(\bar{x}) + b S \circ T(\bar{y}),\ \forall a,b \in \mathbb{R},\ \forall \bar{x},\bar{y} \in \mathbb{R}^n
\end{align*}
Let $\{\bar{u}_1,\bar{u}_2,\cdots,\bar{u}_n\}$ be the standards basis for $\mathbb{R}^n$, $\{\bar{e}_1,\bar{e}_2,\cdots,\bar{e}_m\}$ be the standards basis for $\mathbb{R}^m$ and $\{\bar{w}_1,\bar{w}_2,\cdots,\bar{w}_p\}$ be the standards basis for $\mathbb{R}^p$.
Let $\bar{v} \in \mathbb{R}^n$, then $\bar(v) = \sum_{i=1}^n v_i\bar{u}_i$, and $S \circ T(\bar{v})=\sum_{i=1}^n v_n S \circ T(\bar{u}_i)$
\begin{commentary}
\begin{align*}
	S \circ T(\bar{v}) & =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix} \begin{bmatrix} S \circ T(\bar{u}_1) \\ S \circ T(\bar{u}_2) \\ \cdots \\ S \circ T(\bar{u}_n) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} S(t_{11}\bar{e}_1 + \cdots + t_{m1}\bar{e}_m) \\ S(t_{12}\bar{e}_1 + \cdots + t_{m2}\bar{e}_m) \\ \cdots \\ S(t_{1n}\bar{e}_1 + \cdots + t_{mn}\bar{e}_m) \end{bmatrix} \\
	& =  \begin{bmatrix} v_1 & v_2 & \vdots & v_n  \end{bmatrix}\begin{bmatrix} t_{11}S(\bar{e}_1) + \cdots + t_{m1}S(\bar{e}_m) \\ t_{12}S(\bar{e}_1) + \cdots + t_{m2}S(\bar{e}_m) \\ \cdots \\ t_{n1}S(\bar{e}_1) + \cdots + t_{mn}S(\bar{e}_m) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} S(\bar{e}_1) \\ S(\bar{e}_2) \\ \cdots \\ S(\bar{e}_m) \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11}\bar{w}_1+s_{12}\bar{w}_2+\cdots+s_{1p}\bar{w}_p \\ s_{12}\bar{w}_1+s_{22}\bar{w}_2+\cdots+s_{p2}\bar{w}_p \\ \cdots \\ s_{1m}\bar{w}_1+ s_{2m}\bar{w}_2+\cdots+s_{pm}\bar{w}_p \end{bmatrix} \\
	& = \begin{bmatrix} v_1 & v_2 & \cdots & v_n  \end{bmatrix} \begin{bmatrix} t_{11} & t_{21} & \cdots & t_{m1} \\ t_{12} & t_{22} & \cdots & t_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ t_{1n} & t_{2n} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} s_{11} & s_{21} & \cdots & s_{p1} \\ s_{12} & s_{22} & \cdots & s_{p2} \\ \vdots & \vdots & \ddots & \vdots \\ s_{1m} & s_{2m} & \cdots & s_{pm} \end{bmatrix} \begin{bmatrix} \bar{w}_1 \\ \bar{w}_2 \\ \vdots \\ \bar{w}_p \end{bmatrix}\\
	\intertext{We may take transpose,}
	S \circ T(\bar{v}) & = \begin{bmatrix} \bar{w}_1 & \bar{w}_2 & \vdots & \bar{w}_p \end{bmatrix} \begin{bmatrix} s_{11} & s_{12} & \cdots & s_{1m} \\ s_{21} & s_{22} & \cdots & s_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ s_{p1} & s_{p2} & \cdots & s_{pm} \end{bmatrix} \begin{bmatrix} t_{11} & t_{12} & \cdots & t_{1n} \\ t_{21} & t_{22} & \cdots & t_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ t_{m1} & t_{m2} & \cdots & t_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \cdots \\ v_n  \end{bmatrix} 
\end{align*}
Remember : Given $T : \mathbb{R}^n \to \mathbb{R}^m$, then we may take $m(T)$ either as $m \times n$ matrix or $n \times m$ matrix.
Since, we chose $m \times n$, $m(S \circ T) = m(S)m(T)$.
Otherwise, $m(S \circ T) = m(T)m(S)$.
This may change for different authors.
\end{commentary}

Suppose $m(S) = (s_{ij})$ and $m(T) = (t_{ij})$ respectively.
Then
\[ S(e_k) = \sum_{i=1}^p s_{ik} \bar{w}_i,\ k=1,2,\cdots,m \text{ and }\]
\[ T(u_j) = \sum_{k=1}^m t_{kj} \bar{e}_k,\ j=1,2,\cdots,n \]
\begin{align*}
	(S \circ T)(\bar{u}_j) = & S(T(\bar{u}_j)) = S\left(\sum_{k=1}^m t_{kj}\bar{e}_k\right) = \sum_{k=1}^m t_{kj}S(\bar{e}_k) \\ 
	= & \sum_{k=1}^m t_{kj}\left( \sum_{i=1}^p s_{ik} \bar{w}_i\right) = \sum_{i=1}^p \left(\sum_{k=1}^m s_{ik}t_{kj}\right)\bar{w}_i
\end{align*}
Therefore, $m(S \circ T) = \sum_{k=1}^m s_{ik}t_{kj} = (s_{ik})(t_{kj}) =  m(S)m(T)$.

\subsection{The Jacobian Matrix}
Let $\bar{u}_1, \bar{u}_2, \cdots, \bar{u}_n$ be the unit co-ordinate vectors in $\mathbb{R}^n$ and $\bar{e}_1, \bar{e}_2, \cdots, \bar{e}_m$ be the unit co-ordinate vectors in $\mathbb{R}^m$.
Let function $f : \mathbb{R}^n \to \mathbb{R}^m$ be differentiable at $\bar{c} \in \mathbb{R}^n$.
Then there exists a linear function $T = f'(\bar{c}) : \mathbb{R}^n \to \mathbb{R}^m$ such that $f(\bar{c}+\bar{v}) = f(\bar{c})+f'(\bar{c})(\bar{v}) + \|\bar{v}\|+E_{\bar{c}}(\bar{v})$.
We have, $T(\bar{u}_k) = f'(\bar{c})(\bar{u}_k) = f'(\bar{c},\bar{u}_k) = D_kf(\bar{c}) = D_k \sum_{i=1}^m f_i(\bar{c})\bar{e}_i$.

Clearly, the matrix of total derivative $T$, $m(T) = (t_{ik}) = (D_k f_i(\bar{c}))$.
This matrix is called Jacobian matrix of $f$ at $\bar{c}$ and is denoted by $Df(\bar{c})$.
\[ Df(\bar{c}) = \begin{bmatrix} D_1 f_1(\bar{c}) & D_2 f_1(\bar{c}) & \cdots & D_n f_1(\bar{c}) \\ D_1 f_2(\bar{c}) & D_2 f_2(\bar{c}) & \cdots & D_n f_2(\bar{c}) \\ \vdots & \vdots & \ddots & \vdots \\ D_1 f_m(\bar{c}) & D_2 f_m(\bar{c}) & \cdots & D_n f_m(\bar{c}) \end{bmatrix} \]

\subsubsection{Properties of Jacobian matrix}
\begin{enumerate}
	\item $k$th row of $Df(\bar{c})$ is gradient vector of $f_k$
		\[\nabla f_k(\bar{c}) = (D_1f_k(\bar{c}),D_2f_k(\bar{c}),\cdots,D_nf_k(\bar{c}))\]
	\item When $m = 1$, $Df(\bar{c}) = \nabla f(\bar{c})$.
	\item $f'(\bar{c})(\bar{v}) = \sum_{k=1}^m \left(\nabla f_k(\bar{c}) \cdot \bar{v}\right) \bar{e}_k$
	\item $\|f'(\bar{c})(\bar{v})\| \le M\|v\|$ where $M = \sum_{k=1}^m \|\nabla f_k(\bar{c})\| $, by property (3)
	\item $f'(\bar{c})(\bar{v}) \to \bar{0}$ as $\bar{v} \to \bar{0}$, by property (4)
\end{enumerate}

\subsubsection{Chain Rule}
\begin{commentary}
Chain Rule for real function : $\frac{d F \circ G}{dx}(x) = \frac{d}{dy}F(y) \frac{d}{dx}G(x) = F'(y)\ G'(x)$

For example : $\frac{d}{dx} (ax+3)^3 = \frac{d}{dy}y^3 \frac{d}{dx} \left(ax+3\right) = 3ay^2 = 3a(ax+3)^2$
\end{commentary}
\begin{theorem}
Let $g$ be differentiable at $\bar{a}$, with total derivative $g'(\bar{a})$ and $\bar{b} = g(\bar{a})$.
Let $f$ is differentiable at $\bar{b}$, with total derivative $f'(\bar{b})$.
Then $h = f \circ g$ is differentiable at $\bar{a}$ with total derivative $h'(\bar{a}) = f'(\bar{b}) \circ g'(\bar{a})$.
\begin{commentary}
Try to read $h'(\bar{a}) = H$, $f'(\bar{b}) = F$, $g'(\bar{a}) = G$, then $H = F \circ G \implies H(x) = F(G(x))$
In other words, $h'(\bar{a})(\bar{v}) = f'(\bar{b}) \circ g'(\bar{a})\ (\bar{v}) = f'(\bar{b})(g'(\bar{a})(\bar{v}))$.
\end{commentary}
\end{theorem}
\begin{proof}
Given $\epsilon > 0$, let $y \in \mathbb{R}^p$ such that $\|y\| < \epsilon$.
Let $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^p \to \mathbb{R}^n$, then $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.

We have, $h(\bar{a}+\bar{y})-h(\bar{a}) = f(g(\bar{a}+\bar{y})) - f(g(\bar{a})) = f(\bar{b}+\bar{v}) - f(\bar{b})$ where $\bar{b} = g(\bar{a})$, and  $\bar{v} = g(\bar{a}+\bar{y})-g(\bar{a})$.

Since $g$ is differentiable at $\bar{a}$, $g$ satisfies first-order Taylor's formula.
\begin{align*}
	g(\bar{a}+\bar{y}) & =  g(\bar{a}) + g'(\bar{a})(\bar{y}) + \|\bar{y}\| E_{\bar{a}}(\bar{y}) \text{ where } E_{\bar{a}} \to \bar{0} \text{ as } \bar{y} \to \bar{0} \\
	\implies & \bar{v} = g(\bar{a}+\bar{y})-g(\bar{a}) = g'(\bar{a})(\bar{y}) + \|\bar{y}\| E_{\bar{a}}(\bar{y})
\end{align*}
Clearly, as $\bar{y} \to \bar{0} \implies \bar{v} \to g'(\bar{a})(\bar{0}) = \bar{0}$.
Again,  we have $f$ is differentiable at $\bar{b}$, thus $f$ satisfies first-order Taylor's formula.
\[ f(\bar{b}+\bar{v}) = f(\bar{b}) + f'(\bar{b})(\bar{v}) + \|\bar{v}\| E_{\bar{b}}(\bar{v}) \text{ where } E_{\bar{b}} \to \bar{0} \text{ as } \bar{v} \to \bar{0} \]
\begin{align*}
	\implies f(\bar{b}+\bar{v}) - f(\bar{b}) & = f'(\bar{b})(\bar{v}) + \|\bar{v}\| E_{\bar{b}}(\bar{v}) \\
	& = f'(\bar{b})\left( g'(\bar{a})(\bar{y}) + \|\bar{y}\|E_{\bar{a}}(\bar{y}) \right) + \|\bar{v}\| E_{\bar{b}}(\bar{v}) \\
	& = f'(\bar{b})(g'(\bar{a})(\bar{y})) + \|\bar{y}\| E(\bar{y}) \\
	& \text{ where } E(\bar{y}) = f'(\bar{b})(E_{\bar{a}}(\bar{y})) + \frac{\|\bar{v}\|}{\|\bar{y}\|} E_{\bar{b}}(\bar{v}),\ \bar{y} \ne \bar{0}
\end{align*}
\[ \implies h(\bar{a}+\bar{y})-h(\bar{a}) = f(\bar{b}+\bar{v}) - f(\bar{b}) = f'(\bar{b})(g'(\bar{a})(\bar{y})) + \|\bar{y}\|E(\bar{y}) \]

Since $f'(\bar{b})$ and $g'(\bar{a})$ are linear, their composition is also linear.
Therefore, $h$ is differentiable at $\bar{a}$ with a linear, total derivative $h'(\bar{a}) = f'(\bar{b}) \circ g'(\bar{a})$ as it satisfies first-order Taylor's formula if $E_{\bar{y}} \to \bar{0}$ as $\bar{y} \to \bar{0}$.

We have, $\|\bar{v}\| \le \|g'(\bar{a})(\bar{y})\| + \|\bar{y}\|\ \|E_{\bar{a}}(\bar{y})\| \le M\|y\| + \|E_{\bar{a}}(\bar{y})\|\ \|\bar{y}\|$.
\[ \implies \frac{\|\bar{v}\|}{\|\bar{y}\|} \le M + \|E_{\bar{a}}(\bar{y})\| \]

Thus, $\bar{v} \to \bar{0}$ as $\bar{y} \to \bar{0}$.
Then $f'(\bar{b})(\bar{v}) \to f'(\bar{b})(\bar{0}) = \bar{0}$.
And $E_{\bar{a}}(\bar{y}) \to \bar{0}$.
Therefore, $E(\bar{y}) \to \bar{0} + M\bar{0} = \bar{0}$ as $\bar{y} \to \bar{0}$.
\end{proof}

\subsubsection{Matrix form of the chain rule}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$, $g : \mathbb{R}^p \to \mathbb{R}^n$.
And $h = f \circ g : \mathbb{R}^p \to \mathbb{R}^m$.
Suppose $g$ is differentiable at $\bar{a} \in \mathbb{R}^p$ and $f$ is differentiable at $g(\bar{a}) = \bar{b} \in \mathbb{R}^n$.
Then $h$ is differentiable at $\bar{a}$ and the Jacobian matrix of $h$ is given by the chain rule,
\[ Dh(\bar{a}) = Df(\bar{b})Dg(\bar{a}) \text{ where } h = f \circ g,\ \bar{b} = g(\bar{a})\]
In other words,
\[ D_jh_i(\bar{a}) = \sum_{k=1}^n D_k f_i(\bar{b}) D_j g_k(\bar{a}),\ i=1,2,\cdots,m,\ j=1,2,\cdots,p \]

For $m=1$, $D_j h(\bar{a}) = \sum_{k=1}^n D_kf(\bar{b}) D_jg_k(\bar{a})$

For $m=1$ and $p=1$, $h'(\bar{a}) = \sum_{k=1}^n Df(\bar{b}) g_k'(\bar{a}) = \nabla f(\bar{b}) \cdot Dg(\bar{a})$

\begin{theorem}
Let $f$ and $D_2f$ be continuous functions on a rectangle $[a,b] \times [c,d]$.
Let $p$ and $q$ be differentiable on $[c,d]$, where $p(y) \in [a,b]$ and $q(y) \in [c,d]$ for each $y \in [c,d]$.
Define $F$ by the equation,
\[ F(y) = \int_{p(y)}^{q(y)} f(x,y) dx,\ y \in [c,d] \]
Then $F'(y)$ exists for each $y \in (c,d)$ and is given by,
\[ F'(y) = \int_{p(y)}^{q(y)} D_2 f(x,y) dx + f((q,y),y)q'(y) - f(p(y),y)p'(y) \]
\end{theorem}
\begin{commentary}
The following two theorems are required for proving the theorem on differentiating an integral.
\end{commentary}
\begin{theorem}
Let $\alpha$ be of bounded variation on $[a,b]$ and assume that $f \in \mathcal{R}(\alpha)$ on $[a,b]$.
\[ \text{ Define } F(x) = \int_a^x f\ d\alpha,\ x \in [a,b] \]
Then $F$ is of bounded variation on $[a,b]$ and $F$ is continuous at $x$ if $\alpha$ is continuous at $x$.
If $\alpha$ is increasing on $[a,b]$, then the derivative $F'(x)$ exists at each $x \in (a,b)$ where $\alpha'(x)$ exists and where $f$ is continuous.
And 
\[ F'(x) = f(x)\alpha'(x) \]
\label{thm:paralimit}
\end{theorem}
\begin{theorem}
Let $Q = \{ (x,y) : a \le x \le b,\ c \le y \le d \}$.
Assume that $\alpha$ is of bounded variation on $[a,b]$ and for each $y \in [c,d]$, assume that the integral
\[ F(y) = \int_a^b f(x,y)\ d\alpha(x) \]
exists.
If the partial derivative $D_2f$ is continuous on $Q$, the derivative $F'(y)$ exists for each $y \in (c,d)$ and is given by
\[ F'(y) = \int_a^b D_2f(x,y)\ d\alpha(x) \]
\label{thm:fixedlimit}
\end{theorem}
\begin{proof}
Let $G(x_1,x_2,x_3) = \int_{x_1}^{x_2} f(t,x_3)\ dt$.
Then we may write $F(y)$ in terms of $G$.
That is, $F(y) = G(p(y),q(y),y)$.

By 1-dimensional chain rule, we have 
\begin{align*}
	F'(y) & = \frac{dF}{dy} = \frac{\partial G}{\partial p} \frac{dp}{dy} + \frac{\partial G}{\partial q} \frac{dq}{dy} + \frac{\partial G}{\partial y} \\
	& = D_1 G\ p'(y) + D_2 G\ q'(y) + D_3 G
\end{align*}

\begin{itemize}
	\item $D_1 G$\\ 
	Since the variable of differentition is present in the limit of the integral, we use theorem \ref{thm:paralimit} to compute the derivative of the integral.
	We may write, $G(p(y),q(y),y)  = -\int_{q(y)}^{p(y)} f(t,y)\ dt$.
	We are differentiating (partially) with respect to $p(y)$.
	Thus $q(y)$, $y$ are constants for this differentiation.
	Suppose $G(x,a,y) = -H(x) = -\int_a^x f(t,y)\ dt \implies D_1 G = -H'(x)= -f(x,y)$.
	Thus, $D_1 G = -f(p(y),y)$.
	\item $D_2 G$\\
	Again, variable of differentiation is persent in the limit of the integral.
	Thus, we write, $G(p(y),q(y),y) = \int_{p(y)}^{q(y)} f(t,y)\ dt$.
	Now we are differentiating (partially) with respect to the the second component of $G$ which is $q(y)$.
	Clearly, $p(y)$ and $y$ are treated as constants.
	$G(a,x,y) = H(x) = \int_a^x f(t,y)\ dt \implies D_2 G = H'(x) = f(q(y),y)$.
	\item $D_3 G$\\
	Now the variable of integration is not affecting the limits of the integral.
	Also it is given that $D_2 f$ is continuous on $[a,b] \times [c,d]$.
	We write $G(a,b,x) = H(x) = \int_a^b f(t,x)\ dt \implies D_3 G = H'(x) = \int_a^b D_2 f(t,x)\ dt$.
	Thus $D_3 G = \int_{p(y)}^{q(y)} f(t,y)\ dt$.
\end{itemize}
\end{proof}

\subsubsection{The mean-value theorem for differentiable functions}
\begin{theorem}[Mean-Value]
Let $S$ be an open subset of $\mathbb{R}^n$.
Assume $f : S \to \mathbb{R}^m$ is differentiable at each point of $S$.
Let $\bar{x}$, $\bar{y}$ be two points in $S$ such that $L(\bar{x},\bar{y}) = \{ t\bar{x}+(1-t)\bar{y} : t \in [0,1] \}$ is subset of $S$.
Then for every $\bar{a} \in \mathbb{R}^m$, there exists a point $\bar{z} \in L(\bar{x},\bar{y})$ such that
\[ \bar{a}.\left( f(\bar{y})-f(\bar{x}) \right) = \bar{a}.f'(\bar{z})(\bar{y}-\bar{x}) \]
\end{theorem}
\begin{proof}
Let $\bar{u} = \bar{y}-\bar{x}$.
We have $S$ is open subset and $L(\bar{x},\bar{y}) \subset S$, thus there exists $\delta > 0$ such that $\bar{x}+t\bar{u} \in S, \forall t \in (-\delta,1+\delta)$.
\begin{commentary} In other words, the `Line segment $L(\bar{x},\bar{y})$' is properly contained in $S$, in such a way that extending the Line from $\bar{x}$ to $\bar{y}$ a little bit extra one either sides is still contained in $S$.\end{commentary}

Let $\bar{a} \in \mathbb{R}^m$ and $F : (-\delta,1+\delta) \to \mathbb{R}$ defined by $F(t) = \bar{a}.f(\bar{x}+t\bar{u})$.
Then $F$ is differentiable at each $t \in (-\delta,1+\delta)$ and the derivative $F'(t) = \bar{a}.f'(\bar{x}+t\bar{u},\bar{u})$, the directional derivative of $f(\bar{x}+t\bar{u})$ with respect to $\bar{u}$.

\[ f'(\bar{x}+t\bar{u},\bar{u}) = f'(\bar{x}+t\bar{u})(\bar{u}) \implies F'(t) = \bar{a}.f'(\bar{x}+t\bar{u})(\bar{u}) \]
By 1-dimensional mean-value theorem, we have
\[ \exists \theta \in (0,1) \text{ such that } F(1) - F(0) = F'(\theta) \]
By definition of $F$, $F(1) = \bar{a}.f(\bar{x}+\bar{u}) = \bar{a}.f(\bar{y})$.
And $F(0) =\bar{a}.f(\bar{x})$.
Therefore,
\[ F'(\theta) = F(1) - F(0) = \bar{a}.f(\bar{y}) - \bar{a}.f(\bar{x}) = \bar{a}.(f(\bar{y})-f(\bar{x})) \]
We also have,
\[ F'(\theta) = \bar{a}.f'(\bar{x}+\theta \bar{u})(\bar{u}) = \bar{a}.f'(\bar{z})(\bar{y}-\bar{x}), \text{ where } \bar{z} = \bar{x}+\theta \bar{u} \in L(\bar{x},\bar{y}) \]
\end{proof}

\begin{remark}
Suppose $S$ is convex in $\mathbb{R}^m$.
Then for every pair of points $\bar{x},\bar{y} \in S$, $L(\bar{x},\bar{y}) \subset S$.
Thus Mean-value theorem holds for all $\bar{x},\bar{y} \in S$.
\end{remark}

\section{Multivariate Calculus}
\subsection{A sufficient condition for differentiability}
\begin{theorem}
Suppose one of the partial derivatives $D_1f,D_2f,\cdots,D_nf$ exists at $\bar{c}$.
And the remaining $n-1$ partial derivatives exists in some $n$-ball $B(\bar{c})$ and are continuous at $\bar{c}$.
Then $f$ is differentiable at $\bar{c}$.
\end{theorem}
\begin{proof}
Step 1 : Real-valued function

We claim that the function $f : \mathbb{R}^n \to \mathbb{R}^m$ is differentiable at $\bar{c}$ iff each component $f_k$ is differentiable at $\bar{c}$.

Suppose $f$ is differentiable at $\bar{c}$, then there exists a linear, total derivative function $f'(\bar{c})$ satisfying first-order Taylor's formula at $\bar{c}$.

ie, $f(\bar{c}+\bar{v}) = f(\bar{c}) + f'(\bar{c})(\bar{v}) + \|\bar{v}\| E_{\bar{c}}(\bar{v})$ where $E_{\bar{c}}(\bar{v}) \to \bar{0}$ as $\bar{v} \to \bar{0}$.
\begin{align*}
	f(\bar{c}+\bar{v}) & = \left( f_1(\bar{c}+\bar{v}), f_2(\bar{c}+\bar{v}), \cdots, f_m(\bar{c}+\bar{v}) \right)\\
	f(\bar{c}) & = \left( f_1(\bar{c}), f_2(\bar{c}), \cdots, f_m(\bar{c}) \right) \\
	f'(\bar{c})(\bar{v}) & = \left( f'_1(\bar{v}), f'_2(\bar{v}), \cdots, f'_m(\bar{v}) \right) \\
	E_{\bar{c}}(\bar{v}) & = \left( E_1(\bar{v}), E_2(\bar{v}), \cdots, E_m(\bar{v}) \right)
\end{align*}
where each component of the error function $E_k(\bar{v}) \to 0$ as $\bar{v} \to \bar{0}$.
Also since $f'(\bar{c})$ is linear, each of its components $f'_k : \mathbb{R}^n \to \mathbb{R}$ are linear.
\begin{align*}
	f(\bar{c}+\bar{v}) = & \left( f_1(\bar{c}+\bar{v}), f_2(\bar{c}+\bar{v}), \cdots, f_m(\bar{c}+\bar{v}) \right) \\
	= & \left( f_1(\bar{c}), f_2(\bar{c}), \cdots, f_m(\bar{c}) \right) + \left( f'_1(\bar{v}), f'_2(\bar{v}), \cdots, f'_m(\bar{v}) \right) \\
	& + \|\bar{v}\|\left(E_1(\bar{v}), E_2(\bar{v}), \cdots, E_m(\bar{v}) \right) \text{ where } E_k(\bar{v}) \to 0 \text{ as } \bar{v} \to \bar{0}\\
	= & \left( f_1(\bar{c}), f_2(\bar{c}), \cdots, f_m(\bar{c}) \right) + \left( f'_1(\bar{v}), f'_2(\bar{v}), \cdots, f'_m(\bar{v}) \right) \\
	& + \left(\|\bar{v}\|E_1(\bar{v}), \|\bar{v}\|E_2(\bar{v}), \cdots, \|\bar{v}\|E_m(\bar{v}) \right) \text{ where } E_k(\bar{v}) \to 0 \text{ as } \bar{v} \to \bar{0}\\
	= & \left( f_1(\bar{c}) + f'_1(\bar{v}) + \|\bar{v}\|E_1(\bar{v}), \cdots, f_m(\bar{c}) + f'_m(\bar{v}) + \|\bar{v}\|E_m(\bar{v}) \right) 
\end{align*}

Thus first-order Taylor's forumula for $f$ at $\bar{c}$ gives first-order Taylor's forumula for each of its components $f_k$.
ie, $f_k(\bar{c}+\bar{v}) = f_k(\bar{c}) + f'_k(\bar{v} + \|\bar{v}\|E_k(\bar{v})$ where $E_k(\bar{v}) \to 0$ as $\bar{v} \to \bar{0}$.
Therefore, $f_k$ are differentiable at $\bar{c}$ for $k = 1,2,\cdots, m$.

Suppose each component $f_k$ of $f$ are differentiable at $\bar{c}$.
Then there exists linear, total derivative functions $f'_k$ satisfying first-order Taylor's formula at $\bar{c}$.
ie, $f_k(\bar{c}+\bar{v}) = f_k(\bar{v}) + f'_k(\bar{v}) + \|\bar{v}\|E_k(\bar{v})$ where $E_k(\bar{v}) \to 0$ as $\bar{v} \to \bar{0}$.

Define $E_{\bar{c}}(\bar{v}) = \left( E_1(\bar{v}), E_2(\bar{v}), \cdots, E_k(\bar{v}) \right)$.
Then $E_{\bar{c}}(\bar{v}) \to \bar{0}$ as $\bar{v} \to \bar{0}$.
Therefore, there exists a linear, total derivative function $f'(\bar{c}) = \left( f'_1, f'_2, \cdots, f'_m \right)$ satisfying first-order Taylor's formula at $\bar{c}$.

Thus, if each (real-valued) component function $f_k$ are differentiable, then $f$ is also differentiable.
Therefore, it is sufficient to prove the theorem for a real-valued function.

Step 2: Telescopic Sum

Assume (without loss of generality) that $D_1f$ exists at $\bar{c}$ and $D_2f,D_3f,\cdots,D_nf$ exist and continuous in some $n$-ball $B(\bar{c})$.
\begin{commentary} Suppose $D_rf$ exists at $\bar{c}$ and all partial derivatives execept $D_rf$ are continuous.
	Then $v_0 = \bar{0}$, $v_1 = y_r\bar{u}_r$, $v_2 = y_r\bar{u}_r + y_1\bar{u}_1$, \dots.
Then the following proof can be applied without any loss of generality.
\end{commentary}

Let $\bar{v} = \lambda\bar{y}$ where $\bar{y} = \frac{\bar{v}}{\|\bar{v}\|}$.
Clearly, $\|\bar{y}\| = 1$ and $\lambda = \| \bar{v} \|$.
Choose $\lambda > 0$ such that $\bar{c}+\bar{v} \in B(\bar{c})$ and all the partial derivatives $D_2f, D_3f, \cdots, D_nf$ exists and are continuous in $B(\bar{c})$.

	We have, $\bar{y} = (y_1, y_2, \cdots, y_n) = y_1 \bar{u}_1 + y_2 \bar{u}_2 + \cdots + y_n \bar{u}_n$.

	Define $\bar{v}_0 = \bar{0},\ \bar{v}_1 = y_1\bar{u}_1, \cdots, \ \bar{v}_n = y_1 \bar{u}_1 + y_2 \bar{u}_2 + \cdots + y_n \bar{u}_n$.
\begin{align*}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = & ( f(\bar{c}+\lambda{} \bar{v}_n) - f(\bar{c}+\lambda{} \bar{v}_{n-1}) ) \\
	& + ( f(\bar{c}+\lambda{} \bar{v}_{n-1}) - f(\bar{c}+\lambda{} \bar{v}_{n-2}) ) \\
	& + \cdots + ( f(\bar{c}+\lambda{} \bar{v}_1) - f(\bar{c}+\lambda{} \bar{v}_0) ) \\
	& = \sum_{k = 1}^n f(\bar{c} + \lambda{} \bar{v}_k) - f(\bar{c} + \lambda{} \bar{v}_{k-1}) \\
	& = \sum_{k = 1}^n f(\bar{c} + \lambda{} \bar{v}_{k-1} + \lambda{} y_k \bar{u}_k) - f(\bar{c} + \lambda{} \bar{v}_{k-1})
\end{align*}
Step 3 : Mean-value theorem

Define $\bar{b}_k = \bar{c}+\lambda{}\bar{v}_{k-1}$.
Then we have
\begin{equation}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = \sum_{k = 1}^n f(\bar{b}_k + \lambda{}y_k\bar{u}_k)-f(\bar{b}_k)
\end{equation}
We know that all partial derivatives exists in $B(\bar{c})$.
%Thus $f$ is continuous in $B(\bar{c})$.
Therefore by 1-dimensional mean-value theorem we have,
\[ f(\bar{b}_k+\lambda{}y_k\bar{u}_k) - f(\bar{b}_k) = \lambda{}y_kD_kf(\bar{a}_k) \text{ where } \bar{a}_k \in L(\bar{b}_k,\bar{b}_k+\lambda{}y_k\bar{u}_k) \]
\begin{equation}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = \lambda{} \sum_{k = 1}^n y_kD_kf(\bar{a}_k) \text{ where } \bar{a}_k \in L(\bar{b}_k,\bar{b}_k+\lambda{}y_k\bar{u}_k)
\end{equation}

Step 4 : Continuity of partial derivatives in $B(\bar{c})$

As $\lambda{} \to 0,\ \bar{v} \to \bar{0}$.
And both $\bar{b}_k,\ \bar{b}_k+\lambda{}y_k\bar{u}_k \to \bar{c}$.
Clearly, $\bar{a}_k$ in the line between $\bar{b}_k$ and $\bar{b}_k+\lambda{} y_k \bar{u}_k$ also converges to $\bar{c}$.

For $k \ge 2$, $D_kf$ are continuous in the $n$-ball $B(\bar{c})$.
Thus $D_kf(\bar{a}_k) \to D_kf(\bar{c})$.
We may write, $D_kf(\bar{a}_k) = D_kf(\bar{c}) + E_k(\lambda)$ where $E_k(\lambda) \to \bar{0}$ as $\lambda{} \to 0$.
Also, since $D_1 f$ exists, $D_1f(\bar{c}+\lambda{} y_1\bar{u}_1) \to D_1f(\bar{c})$ as $\lambda{} \to 0$.
\begin{commentary}
\[ \text{ Remember : } D_1 f(\bar{c}) = \lim_{h \to 0} \frac{f(\bar{c}+h\bar{u}_1) - f(\bar{c})}{h} \]
\end{commentary}
\begin{align*}
	f(\bar{c}+\bar{v}) - f(\bar{c}) = & \lambda{} \sum_{k = 1}^n y_kD_kf(\bar{c}) + \lambda{} \sum_{k = 1}^n y_kE_kf(\lambda{})\\
	= & \nabla f(\bar{c}) \cdot \bar{v} + \|\bar{v}\|E(\lambda) \\
	& \text{ where } E(\lambda{}) = \sum_{k = 1}^n y_k E_k(\lambda{}) \to \bar{0} \text{ as } \bar{v} \to \bar{0}
\end{align*}
That is, we have a linear function which satisfies first-order Taylor's formula at $\bar{c}$.
Therefore, $f$ is differentiable.
\end{proof}

\subsection{Sufficient conditions for the equality of mixed partial derivatives}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$.
Then $D_r f$ and $D_k f$ are two partial derivatives of $f$.
And $D_{r,k} f = D_r(D_k f)$ and $D_{k,r} f = D_k (D_r f)$.
\[ D_{r,k} f = \frac{\partial^2 f}{\partial x_r \partial x_k} = \frac{\partial}{\partial x_r} \frac{\partial f}{\partial x_k} \text{ and } D_{k,r} f = \frac{\partial^2 f}{\partial x_k \partial x_r} = \frac{\partial}{\partial x_k} \frac{\partial f}{\partial x_r}  \]
\begin{commentary}
	There are two sufficient conditions for the equality of these mixed partial derivatives in our scope.
\begin{enumerate*}
	\item differentiability of $D_k f$ or
	\item continuity of $D_{r,k} f$ and $D_{k,r}$
\end{enumerate*}
at $\bar{c}$ where the mixed partial derivatives are to be equal.
\end{commentary}

\subsubsection{Differentiability}
\begin{theorem}
Suppose $D_r f$ and $D_k f$ exists in an $n$-ball about $\bar{c}$ and are both differentiable at $\bar{c}$.
Then $D_{r,k} f = D_{k,r} f$.
\end{theorem}
\begin{proof}
Step 1 : Real-valued function

It is sufficient to prove the theorem for real-valued functions.
Let $f : \mathbb{R}^n \to \mathbb{R}^m$, then $f(\bar{c}) = \left( f_1(\bar{c}),f_2(\bar{c}),\cdots,f_m(\bar{c}) \right)$.
And $$D_k f(\bar{c}) = \left( D_k f_1(\bar{c}), D_k f_2(\bar{c}), \cdots, D_k f_m(\bar{c}) \right)$$
Thus it is sufficient to prove that $D_{r,k}f_j(\bar{c}) = D_{k,r}f_j(\bar{c}),\ j = 1,2,\cdots,m$.
That is, it is sufficient to prove equality of mixed partial derivatives of a real-valued function $f_j : \mathbb{R}^n \to \mathbb{R}$.
Also, we will prove it for $n = 2$ and $\bar{c} = (0,0)$.
\begin{commentary}
From Step 2 onwards, we will write $f$ instead of $f_j$ for ease of notation.
\end{commentary}

Step 2 : $\nabla(h)$

Let $f : \mathbb{R}^n \to \mathbb{R}$.
Suppose that the partial derivatives $D_k f$, $D_r f$ exist in the $n$-ball $B(n)$.
And let $h > 0$ such that the rectangle with vertices $(0,0), (0,h), (h,0), (h,h)$ lies in $B(n)$.
\begin{commentary} Suppose $n=3$, $c = (x,y,z)$, and we want to prove equality of $D_{2,3} f$ and $D_{3,2} f$.
Then we will consider the rectangle with vertices $(x,y,z), (x,y,z+h), (x,y+h,z), (x,y+h,z+h)$.
Again, we are taking $n=2$ and $\bar{c} = (0,0)$, only for the ease of notation as the same proof is applicable for any finite natural number, $n$ and any vector $\bar{c} \in \mathbb{R}^n$.\end{commentary}

Define $\nabla(h) = f(h,h)-f(h,0)-f(0,h)+f(0,0)$.

Step 3 : $D_{1,2} f = \frac{\nabla(h)}{h^2} = D_{2,1} f$

Define $G(x) = f(x,h)-f(x,0)$.
Then  we have, $\nabla(h) = G(h)-G(0)$ and $G'(x) = D_1f(x,h) - D_1f(x,0)$.
By 1-dimensional mean value theorem,
\begin{align*}
	G(h)-G(0) = & hG'(x_1)  \text{ where } x_1 \in (0,h) \\
	= & h\left( D_1f(x_1,h)-D_1f(x_1,0)\right)
\end{align*}
We have $D_1 f$ is differentiable at $(0,0)$.
There exists linear, total derivative function $(D_1f)'(0,0)$ where $(D_1f)'(0,0)(x,y) = \nabla D_1 f(0,0) \cdot{} (x,y)$ satisfiying first-order Taylor's formula at $(0,0)$. 
\begin{commentary}
\[ \text{ Remember :} f'(\bar{c})(\bar{v}) = \sum_{k = 1}^n v_k D_k f(\bar{c}) = \nabla f(\bar{c}) \cdot{} \bar{v} \]
\end{commentary}
\begin{align*}
	D_1 f((0,0) + (x_1,h)) = & D_1 f(0,0) + \nabla D_1 f(0,0) \cdot{} (x_1,h) + \|(x_1,h)\| E_1(h) \\
	& \text{ where } E_1(h) \to 0 \text{ as } h \to 0\\
	D_1 f(x_1,h) = & D_1 f(0,0) + x_1 D_{1,1} f(0,0) + h D_{2,1} f(0,0) + \left|\sqrt{x_1^2+h^2}\right| E_1(h)
\end{align*}

Similarly,
\begin{align*}
	D_1 f((0,0) + (x_1,0)) = & D_1 f(0,0) + \nabla D_1 f(0,0) \cdot{} (x_1,0) + \|(x_1,0)\| E_2(h) \\
	& \text{ where } E_2(h) \to 0 \text{ as } h  \to 0 \\
	D_1 f(x_1,0) = & D_1 f(0,0) + x_1 D_{1,1} f(0,0) + |x_1| E_2(h)
\end{align*}
Therefore $\nabla(h) = h (D_1 f(x_1,h) - D_1 f(x_1,0)) = h^2 D_{2,1} f(0,0) + E(h)$ where $E(h) = h|\sqrt{x_1^2+h^2}| E_1(h) - h|x_1|E_2(h)$ and $E(h) \to 0$ as $h \to 0$.

Since $0 < x_1 < h$, we have
$$ 0 \le E(h) \le h^2\left(\sqrt{2}E_1(h)-E_2(h)\right)$$

Therefore,
	$$\lim_{h \to 0} \frac{\nabla(h)}{h^2} \le \lim_{h \to 0} \frac{h^2 D_{2,1} f(0,0)}{h^2} + \lim_{h \to 0} \frac{h^2 (\sqrt{2}E_1(h) - E_2(h))}{h^2} = D_{2,1}f(0,0)$$
\begin{equation}
\lim_{h \to 0} \frac{\nabla(h)}{h^2} = D_{2,1} f(0,0)
\end{equation}

Similarly, define $H(y) = f(h,y)-f(0,y)$.
Then we have, $\nabla(h) = H(h)-H(0)$ and $H'(y) = D_2f(h,y) - D_2f(0,y)$.
By 1-dimensional mean value theorem,
\begin{align*}
	H(h)-H(0) = & hH'(y_1)  \text{ where } y_1 \in (0,h) \\
	= & h \left( D_2f(h,y_1)-D_2f(0,y_1) \right)
\end{align*}
We have $D_2 f$ is differentiable at $(0,0)$.
Thus there exists a linear, total derivative function $(D_2f)'(0,0)$ where $(D_2f)'(0,0)(x,y) = \nabla D_2 f(0,0) \cdot{} (x,y)$ satisfying first-order Taylor's formula at $(0,0)$.
That is,
\begin{align*}
	D_2 f((0,0)+(h,y_1)) = & D_2 f(0,0) + \nabla D_2 f(0,0) \cdot{} (h,y_1) + \|(h,y_1)\| E_3(h) \\
	& \text{ where } E_3(h) \to 0 \text{ as } h \to 0 \\
	D_2 f(h,y_1) = & D_2 f(0,0) + h D_{1,2} f(0,0) + y_1 D_{2,2} f(0,0) + \left|\sqrt{h^2+y_1^2}\right| E_3(h)
\end{align*}

Again,
\begin{align*}
	D_2 f((0,0)+(0,y_1)) = & D_2 f(0,0) + \nabla D_2 f(0,0) \cdot{} (0,y_1) + |y_1| E_4(h) \\
	& \text{ where } E_4(h) \to 0 \text{ as } h \to 0 \\
	D_2 f(0,y_1) = & D_2 f(0,0) + y_1 D_{2,2} f(0,0) + |y_1| E_4(h)
\end{align*}
Therefore, $\nabla(h) = h( D_2 f(h,y_1) - D_2 f(0,y_1) ) = h^2 D_{1,2} f(0,0) + E'(h)$ where $E'(h) = \left|\sqrt{h^2+y_1^2}\right| E_3(h) - |y_1|E_4(h)$ and $E'(h) \to 0$ as $h \to 0$. And 
\begin{equation}
	\lim_{h \to 0} \frac{\nabla(h)}{h^2}  = D_{1,2} f(0,0)
\end{equation}
Therefore, $D_{1,2} f(0,0) = D_{2,1} f(0,0)$.
\end{proof}

\subsubsection{Continuity}
\begin{theorem}
Suppose $D_r f$ and $D_k f$ exists in an $n$-ball about $\bar{c}$.
And $D_{r,k} f$ and $D_{k,r} f$ are continuous at $\bar{c}$.
Then $D_{r,k} f = D_{k,r} f$.
\end{theorem}
\begin{proof}
We have $D_r f = (D_r f_1 , D_r f_2, \cdots, D_r f_m)$.
Therefore, it is sufficient to prove the theorem for real-valued functions.
Suppose $n = 2$, $\bar{c} = (0,0)$ and the partial derivatives $D_1 f$ and $D_2 f$ exist and are continuous in some $2$-ball about $(0,0)$.
Suppose $(h,h)$ lies in that $2$-ball, then $D_1 f(h,h) \to D_1 f(0,0)$ as $h \to 0$.
\end{proof}

\begin{remark} A function $f$ such that $D_{1,2} f \ne D_{2,1} f$.
\[ \text{Let, }f(x,y) = \begin{cases} \frac{xy(x^2-y^2)}{x^2+y^2} & (x,y) \ne (0,0) \\ 0 & (x,y) = (0,0) \end{cases} \]
\begin{align*}
	D_1 f(x,y) = & \frac{\partial}{\partial x} \frac{x^3y-xy^3}{x^2+y^2} \\
	= & \frac{(3x^2y-y^3)(x^2+y^2) - 2x(x^3y-xy^3)}{(x^2+y^2)^2} \\
	= & \frac{x^4y+4x^2y^3-y^5}{(x^2+y^2)^2}\\
	D_1 f_{_{(x = 0)}} = & -y \implies D_{2,1} f_{_{(x = 0)}} = \frac{\partial}{\partial y} -y = -1
\end{align*}
\begin{align*}
	D_2 f(x,y) = & \frac{\partial}{\partial y} \frac{x^3y-xy^3}{x^2+y^2} \\
	= & \frac{(x^3-3xy^2)(x^2+y^2)-2y(x^3y-xy^3)}{(x^2+y^2)^2} \\
	= & \frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2} \\
	D_2 f_{_{(y = 0)}} = & x \implies D_{1,2} f_{_{(y = 0)}}  = \frac{\partial}{\partial x} x = 1
\end{align*}
Therefore, $D_{1,2} f \ne D_{2,1} f$ in the neighbourhood of $(0,0)$.
\begin{commentary} This treatment save a lot of time.
After $D_1 f$, we are planning to perform $D_{2,1} f = D_2 (D_1 f)$ in which the value of $x$ is going to be treated as a constant.
Therefore, we can simplify the expression by substituting $x = 0$ at this stage.
If you are not confident enough to substitute that ``early''.
You may take partial derivative with respect to $y$ and then substitute $x = 0$ and $y = 0$.
Why don't we substitute $y = 0$ before $D_2 f$ is something you should know already !\end{commentary}
\end{remark}

%\chapter{Implicit Functions and Extremum Problems}
\subsection{Implicit Functions and Extremum Problems}
\begin{definition}[Implicit function]
	Let $f$ be a function.
	Consider the equation, $f(\bar{x},\bar{y}) = 0$.
	If there exists a function $g$ such that $\bar{x} = g(\bar{y})$, then $g$ is an implicit form of $f$ or $g$ is defined implicitly by $f$.
	For example, a linear system of equations $Ax-b = 0$ implicitly defines $x = A^{-1}b$ provided $ A$ has non-zero determinant.
\end{definition}

\begin{definition}[Jacobian Determinant]
Let $f : \mathbb{R}^n \to \mathbb{R}^n$, then determinant of the Jacobian matrix $Df(\bar{x})$ is the Jacobian determinant of $f$, $J_f(\bar{x})$.
\end{definition}

\begin{theorem}
Let $f : \mathbb{C} \to \mathbb{C}$.
Then $J_f(z) = |f'(z)|^2$.
\end{theorem}
\begin{proof}
Suppose $f : \mathbb{C} \to \mathbb{C}$ where $f(z) = u(z) + iv(z)$ where $u : \mathbb{C} \to \mathbb{R}$ and $v : \mathbb{C} \to \mathbb{R}$.
\begin{commentary}These real-valued functions $u,v$ have respective $u^*,v^*$ multivariate real functions such that $u^* : \mathbb{R}^2 \to \mathbb{R}$, where $u(z) = u^*(x,y)$ and $z = x+iy$.
Let $f(z) = z^2+1$.
Then $u^*(x,y) = x^2-y^2+1$ and $v^*(x,y) = -2xy$.
And theoretically we use derivatives of $u^*$ when we mention derivatives of $u$.\end{commentary}

Then $f$ has a derivative at $z$ only if the partial derivatives $D_1u,D_2u,D_1v,D_2v$ exists at $z$ and satisfies Cauchy-Riemann equations.
ie $D_1u(z) = D_2v(z)$ and $D_1v(z) = -D_2u(z)$.\cite[Theorem 5.22]{apostol}.

Thus we have $f'(z) = D_1u + iD_1v$ \cite[Theorem 12.6]{apostol}\footnote{Prove using first-order Taylor's formula}.
\begin{align*}
	f'(z) & = D_1u(z) + iD_1v(z) \\
	|f'(z)|^2 & = (D_1u(z))^2 + (D_1v(z))^2\\
	\intertext{For ease of representation, we write $D_1u$ instead of $D_1u(z)$}
	|f'(z)|^2 & = (D_1u)^2 + (D_1v)^2
\end{align*}
We also have
\[ J_f(z) = |Df(z)| = \begin{vmatrix} D_1u & D_2u \\ D_1v & D_2v \end{vmatrix} = D_1uD_2v - D_1vD_2u = (D_1u)^2 + (D_1v)^2 \]
Therefore, $J_f(z) = |f'(z)|^2$.
\end{proof}

\subsubsection{Functions with non-zero Jacobian determinant}
\begin{commentary}
That is, $f : \mathbb{R}^n \to \mathbb{R}^n$ such that $J_f \ne 0$ in an $n$-ball. In other words, we have an $n$-ball $B(\bar{x})$ such that $J_f(\bar{y}) \ne 0,\ \forall \bar{y} \in B(\bar{x})$.
\end{commentary}

\begin{theorem}
Let $B$ be an $n$-ball about $\bar{a}$ in $\mathbb{R}^n$, $\partial B$ be its boundary and $\bar{B} = B \cup \partial B$ be its closure.\footnote{$\bar{B}$ : The line above $B$ has a different meaning compare to $\bar{a}$ (situations like this are an abuse of language).}
Let $f : \mathbb{R}^n \to \mathbb{R}^n$ be continuous in $\bar{B}$ and all partial derivatives, $D_jf_i(\bar{x})$ exists for every $\bar{x} \in B$.
Let $f(\bar{x}) \ne f(\bar{a})$ for every $\bar{x} \in \partial B$ and $J_f(\bar{x}) \ne 0$ for every $\bar{x} \in B$.
Then $f(B)$ contains an $n$-ball about $f(\bar{a})$.
\begin{commentary}
\[ B = \{ \bar{x} : \| \bar{x} - \bar{a} \| < r \} \]
\[ \partial B = \{ \bar{x} : \| \bar{x} - \bar{a} \| = r \} \]
\[ \bar{B} = \{ \bar{x} : \| \bar{x} - \bar{a} \| \le r \} \]
\end{commentary}
\end{theorem}
\begin{proof}
Define $g : \partial B \to \mathbb{R}$ where $g(\bar{x}) = \| f(\bar{x}) - f(\bar{a}) \| $.
We have, $f(\bar{x}) \ne f(\bar{a})$ for every $\bar{x} \in \partial B$, thus $g(\bar{x}) > 0$ for every $\bar{x} \in \partial B$.
Function $f$ is continuous on $\bar{B}$, thus $g$ is continuous on \begin{commentary}$\bar{B}$ and thus $g$ is continuous on its subset\end{commentary} $\partial B$.
Since $\partial B$ is compact, every continuous function on $\partial B$ attains its extrema\footnote{``Every continuous function on a compact set attains its extrema''} and thus $g$ attains its minimum value $m > 0$ somewhere on $\partial B$.

Consider $n$-ball $T$ about $f(\bar{a})$ with radius $\frac{m}{2}$, 
\[ T = B\left(f(\bar{a}),\frac{m}{2}\right) = \left\{ \bar{y} \in \mathbb{R}^n : \| f(\bar{a}) - \bar{y}\| < \frac{m}{2} \right\} \]
Therefore, it is sufficient to prove that $T \subset f(B)$.

Let $\bar{y} \in T$. Define $h : \bar{B} \to \mathbb{R}$ where $h(\bar{x}) = \| f(\bar{x}) - \bar{y} \|$. Again this continuous function $h$ on compact set $\bar{B}$ attains its extrema somewhere on $\bar{B}$. Since $\bar{y} \in T$, $h(\bar{a}) = \| f(\bar{a}) - \bar{y} \| < \frac{m}{2} $. Thus, the minimum of $h$ on $\bar{B}$ is less than $\frac{m}{2}$, \begin{commentary}since $\bar{a} \in \bar{B}$.\end{commentary}

Let $\bar{x} \in \partial B$, then 
\begin{align*}
	h(\bar{x}) = & \| f(\bar{x}) - \bar{y} \| \\
	= &  \| f(\bar{x}) - f(\bar{a}) + f(\bar{a}) - \bar{y} \| \\
	\ge &  \| f(\bar{x}) - f(\bar{a})\| + \| f(\bar{a}) - \bar{y} \| \\ 
	= &  g(\bar{x}) - h(\bar{a}) \\
	> & \frac{m}{2} \text{ since $g(\bar{x}) \ge m$ and $h(\bar{a}) < \frac{m}{2}$}
\end{align*}
	Thus $h$ doesn't attain its minimum on $\partial B$, but at an interior point $\bar{c} \in B$. Consider
	\[ h^2(\bar{x}) = \| f(\bar{x}) - \bar{y}\|^2 = \sum_{r = 1}^n (f_r(\bar{x}) - y_r)^2 \]
	The function $h^2$ also has minimum at the same point $\bar{c}$. Thus all partial derivatives of $h^2$ at $\bar{c}$ are zero. ie, 
	\[ D_k h^2(\bar{c}) = \sum_{r = 1}^n (f_r(\bar{c})-y_r)D_kf_r(\bar{c}) = 0 \]
	This is a system of linear equations with non-zero determinant since $\bar{c} \in B$ and we have $J_f(\bar{c}) \ne 0$. Therefore, $f_r(\bar{c}) = y_r$. That is, $f(\bar{c}) = \bar{y} \in f(B)$. Since $\bar{y} \in T$ is arbitrary, $T \subset f(B)$.
\end{proof}

\begin{theorem}
	Let $A$ be an open subset of $\mathbb{R}^n$ and $f : A \to \mathbb{R}^n$ is continuous and has continuous partial derivatives $D_jf_i$ on $A$. If $f$ is one-to-one on $A$ and $J_f(\bar{x}) \ne 0,\ \forall \bar{x} \in A$, then $f(A)$ is open.
\end{theorem}
\begin{proof}
	Let $\bar{b} \in f(A)$. Then $\bar{b} = f(\bar{a})$ for some $\bar{a} \in A$.
	We have, $f$ is continuous, $f$ has continuous partial derivatives on $A$ and $J_f(\bar{x}) \ne 0$ for every $\bar{x} \in A$.
	Therefore, there exists an open ball $B \subset A$ containing $\bar{a}$ such that $f(B) \subset f(A)$ contains an $n$-ball about $f(\bar{a})$.
	Since $\bar{b} \in f(A)$ is arbitary, every point in $f(A)$ has an $n$-ball containing it in $f(A)$.
	Therefore, $f(A)$ is open.

\begin{commentary} Two assumption in above theorem are trivial.
\begin{enumerate*}
	\item $f$ is continuous in the closed ball, $\bar{B}$.
Set $B$ so chosen that $\bar{B} \subset A$ and $f$ is continuous in $A$.
Thus, $f$ is continuous in $\bar{B}$.
	\item $f$ has different value at boundary compared to center.
ie, $f(\bar{a}) \ne f(\bar{x}),\ \forall x \in \partial B$.
	We have, $f$ is injective on $A$, and $\bar{B} \subset A$.
	Thus $f$ has different values for any two distinct points in it.
Thus, $\forall \bar{x},\bar{y} \in \bar{B},\  \bar{x} \ne \bar{y} \implies \bar{x}, \bar{y} \in A$, and $\bar{x} \ne \bar{y} \implies f(\bar{x}) \ne f(\bar{y})$ \end{enumerate*}
\end{commentary}
\end{proof}

\begin{theorem}
Let $S$ be an open subset of $\mathbb{R}^n$ and $f : S \to \mathbb{R}^n$.
Let components of $f$ has continuous partial derivatives on $S$, $D_jf_i$ and $J_f(\bar{a}) \ne 0$ for some point $\bar{a} \in S$.
Then there is an $n$-ball $B$ about $\bar{a}$ on which $f$ is injective.
\end{theorem}
\begin{proof}
Let $\bar{z} = (\bar{z}_1,\bar{z}_2,\cdots,\bar{z}_n)$ where $\bar{z}_i \in \mathbb{R}^n$.
ie, $\bar{z} \in \mathbb{R}^{n^2}$.
Define function $h : \mathbb{R}^{n^2} \to \mathbb{R}$ by $h(\bar{z}) = \det{[D_jf_i(\bar{z}_i)]}$.
Since $f$ has continuous partial derivatives on $S$, each component of $f$ has continuous partial derivatives in $S$ and thus $h$ is continuous on $S^n$ \begin{commentary} which is a subset of $\mathbb{R}^{n^2}$ since $S$ is an open subset of $\mathbb{R}^n$.
\end{commentary}

Let $\bar{a} \in S$ such that $J_f(\bar{a}) \ne 0$.
\begin{commentary}
Existence of such a point in $S$ is assumed.
\end{commentary}
Consider,$\bar{z}_i = \bar{a},\ \forall i$.
Then $\bar{z} = (\bar{a},\bar{a},\cdots,\bar{a})$.
And $h(\bar{z}) = \det{[D_jf_i(\bar{a})]} = J_f(\bar{a}) \ne 0$.

Since $h$ is continous and $h(\bar{z}) \ne 0$.
There exists an $n$-ball $B$ about $\bar{a}$ in $S$ such that $h(\bar{z}) \ne 0$ for $\bar{z}_i \in B$.
We claim that $f$ is injective on $B$.

Suppose $f$ is not injective.
ie, There exists $\bar{x},\bar{y} \in B(\bar{a})$ such that $\bar{x} \ne \bar{y}$ and $f(\bar{x}) = f(\bar{y)}$.
Open ball $B(\bar{a})$ is a convex set.
And the line segment $L(\bar{x},\bar{y}) \subset B(\bar{a})$.
The function $f$ is differentiable on $S$.
On applying mean-value theorem to each component of $f$, we get
$$0 = f_i(\bar{y})-f_i(\bar{x}) = \nabla f_i(\bar{Z}_i)\cdot(\bar{y}-\bar{x}),\ i=1,2,\cdots$$
where $\bar{Z}_i \in L(\bar{x},\bar{y}) \subset B(\bar{a})$.
Therefore, We have
$$ \sum_{k=1}^n D_kf_i(\bar{Z}_i)(y_k-x_k) = 0$$
The determinant of this system of linear equations is nonzero, as the function $f$ has nonzero jacobian determinant at $\bar{Z}_i \in B(\bar{a})$ for $i = 1,2,\cdots$.
Thus, $y_i = x_i$ for $i = 1,2,\cdots$. This contradicts $\bar{x} \ne \bar{y}$.
Hence, the function $f$ is injective.
\end{proof}

\begin{theorem}
Let $A$ be an open subset of $\mathbb{R}^n$ and assume that $f : A \to \mathbb{R}^n$ has continuous partial derivatives $D_jf_i$ on $A$.
If $J_f(\bar{x}) \ne 0$ for all $\bar{x} \in A$, then $f$ is an open mapping.
\end{theorem}
\begin{proof}
Let $S$ be an open subset of $A$.
Let $\bar{x} \in S$.
Clearly, $f$ has continuous partial derivatives on $S$ and $J_f(\bar{x}) \ne 0$ for all $\bar{x} \in S$.
Thus, there is an $n$-ball $B(\bar{x})$ in which $f$ is injective.
Therefore, $f(B(\bar{x})$ is open in $\mathbb{R}^n$.
Since $\bar{x} \in S$ is arbitrary, $S = \cup_{\bar{x} \in S} B(\bar{x})$.
And $f(S) = \cup_{\bar{x} \in S} f(B(\bar{x}))$.
Therefore, $f(S)$ is open.
Since open set $S$ is arbitrary, $f$ is an open mapping.
\end{proof}

\begin{commentary}
\begin{remark}[Properties]
Functions with non-zero Jacobian determinant has following properties :
\begin{enumerate}
	\item If $J_f \ne 0$ in $n$-ball $B$ about $\bar{a}$ which has different values at its boundaries, then $f(B)$ has an $n$-ball about $f(\bar{a})$.
	\item If $J_f \ne 0$, $f$ has continuous partial derivatives in $S$, and $f$ is injective in an open set $A$, then $f(A)$ is open.
	\item Let $S$ be an open set in $\mathbb{R}^n$, $f$ has continuous partial derivatives in $S$, and $J_f(\bar{a}) \ne 0$ for some $\bar{a} \in S$, then $f$ is injective on an $n$-ball $B(\bar{a})$ in $S$.
	\item Let $A$ be an open set in $\mathbb{R}^n$, $f$ has continuous partial derivatives in $A$, and $J_f \ne 0$ in $A$, then $f$ is an open mapping.
\end{enumerate}
\end{remark}
\end{commentary}

\subsubsection{Inverse function Theorem}
\begin{theorem}[Inverse function]
Let $S$ be an open subset of $\mathbb{R}^n$ and $f$ be a continuously differentiable function\footnote{$f \in C'(S)$ : $f$ is continuously differentiable on $S$} $f : S \to \mathbb{R}^n$.
If $J_f(\bar{a}) \ne 0$ for some $\bar{a} \in S$, then there are two open sets $X \subset S$, and $Y \subset f(S)$ such that
\begin{enumerate}
	\item $\bar{a} \in X \text{ and } f(\bar{a}) \in Y$
	\item $Y = f(X)$
	\item $f$ is injective
	\item there exists another function $g : Y \to X$ such that $g(f(\bar{x})) = \bar{x},\ \forall \bar{x} \in X$
	\item $g$ is continuously differentiable on $Y$
\end{enumerate}
\begin{commentary}
	In other words, if $f \in C'$ and there exists $\bar{a} \in S$ such that $J_f(\bar{a}) \ne 0$, then $f$ has an inverse $f^{-1}$ in a neighbourhood of $f(\bar{a})$ and $f^{-1} \in C'$.
\end{commentary}
\end{theorem}
\begin{proof}
	Step 1 : Construction of open sets $X$ and $Y$.\\
	Given that, $J_f(\bar{a}) \ne 0$ and $f \in C'$.
	Thus all partial derivatives of $f$ are continuous on $S$.
	Then $J_f$ is continuous on $S$, 
	By the continuity of $J_f$ at $\bar{a}$, there exists a neighbourhood of $\bar{a}$, say $B_1(\bar{a})$ in which $J_f \ne 0$.
	That is, $\forall \bar{x} \in B_1(\bar{a}),\ J_f(\bar{x}) \ne 0$.
	Therefore,(by theorem) there exists an $n$-ball $B(\bar{a})$ on which $f$ is injective.
	Let $B$ be an $n$-ball with center $\bar{a}$ contained in $B(\bar{a})$.
	Then $f$ is injective on $B$.
	Therefore,(by theorem) $f(B)$ contains an $n$-ball with center $f(\bar{a})$.
	Let $Y$ be the $n$-ball contained in $f(B)$.
	And  $X=f^{-1}(Y) \cap B$.
	That is, the inverse image of $Y$ on $B$.
	Since $f$ is continuous, $f^{-1}(Y)$ is open.
	Thus, $X$ is an intersection of open sets.
	And therefore, $X$ is open.

	Step 2 : The inverse of $f$, say $g$.\\
	Clearly $\bar{a} \in X$ and $f(\bar{a}) \in Y$.
	Also $Y = f(X)$ and $f$ is injective on $X$( since, $X \subset B$).
	
	The closure of $B$, $\bar{B}$ is compact and $f$ is injective and continuous on $\bar{B}$.
	Then\footnote{Existence of inverse of a continuous function on a compact set in metric spaces.} there exists a continuous function $g$ defined on $f(\bar{B})$ such that $g \circ f$ is the identity function on $\bar{B}$.
	That is, $\forall x \in \bar{B},\ g(f(\bar{x})) = x$.
	Thus, $g(X) = Y$ and $g$ is unique.

	Step 3 : $g$ has continuous partial derivatives.\\
	Define a real-valued function $h : S^n \to \mathbb{R}$ by $h(\bar{Z}) = \det[D_jf_i(\bar{Z}_i)]$ where $\bar{Z}_1,\bar{Z}_2,\cdots,\bar{Z}_n \in S$ and $\bar{Z} = (\bar{Z}_1,\bar{Z}_2,\cdots,\bar{Z}_n)$.
	Now, let $\bar{Z} = (\bar{a},\bar{a},\cdots,\bar{a})$.
	Then $h(\bar{Z}) \ne 0$ and $h$ is continuous on $S^n$.
	Therefore, $\bar{Z}$ has a neighbourhood on which $h$ does not vanish (that is, nonzero).
	Let $B_2(\bar{a})$ be the corresponding $n$-ball with center $\bar{a}$ such that $\bar{Z}_i \in B_2(\bar{a}) \implies h(\bar{Z}) \ne 0$.

	Let $B$ be an $n$-ball with center $\bar{a}$ contained in $B_2(\bar{a})$.
	Now $\bar{B} \subset B_2(\bar{a})$.
	And $h(\bar{Z}) \ne 0,\ \forall \bar{Z}_i \in \bar{B}$.

	We have, $g = (g_1, g_2, \cdots,g_n)$.
	It is enough to prove that $g_k \in C'$ for $k=1,2,\cdots,n$.
	Again, it is enough to prove that $D_rg_k$ exists and is continuous for $1 \le r \le n$.
	(Fix some $r$ and prove that $D_rg_k$ is continuous.)

	Let $\bar{y} \in Y$.
	Define $\bar{x} = g(\bar{y})$ and $\bar{x}' = g(\bar{y}+t\bar{u}_r)$ where $t$ is sufficiently small such that $\bar{y}+t\bar{u}_r \in Y$.
	Then $\bar{x},\bar{x}' \in X$.
	And $f(\bar{x}')-f(\bar{x}) = t\bar{u}_r$.
	Therefore $f_i(\bar{x})-f_i(\bar{x}') = 0$ when $i \ne r$.
	And $f_i(\bar{x}')-f_i(\bar{x}) = t$ when $i = r$.
	By mean-value theorem,
	$$\frac{f_i(\bar{x}') - f(\bar{x})}{t} = \nabla f_i(\bar{Z}_i) \cdot \frac{\bar{x}'-\bar{x}}{t}$$
	where $\bar{Z}_i \in L(\bar{x},\bar{x}')$, the line segment joining $\bar{x}$ and $\bar{x}'$.
	Since $\det[D_jf_i(\bar{Z}_i)] = h(\bar{Z}) \ne 0$, this system of linear equations in $n$ unknowns, $\frac{x_j'-x_j}{t}$ has a unique solution.
	As $t \to 0$, $\bar{x}' \to \bar{x}$.
	And $\bar{Z}_i \to \bar{x}$.
	Since $J_f(\bar{x}) \ne 0$, the limit
	$$ \lim_{t \to 0} \frac{g_k(\bar{y}+t\bar{u}_r)-g_k(\bar{y})}{t}$$
	exists.
	Thus, $D_rg_k(\bar{y})$ exists $\forall y \in Y$ and every $r$.	
	By Cramer's rule, this limit is a quotient of two determinants of partial derivatives of $f$, which are all continuous since $f \in C'$.
	Therefore, $D_rg_k$ are all continuous and $g \in C'$.
\end{proof}

\subsubsection{Implicit function Theorem}
\begin{theorem}
	Let $S$ be an open set in $\mathbb{R}^{n+k}$ and
	$f$ be a function $f : S \to \mathbb{R}^n$.
	Suppose $f$ is continuously differentiable on $S$.
	Let $(\bar{x}_0,\bar{t}_0) \in S$
	such that $\bar{x}_0 \in \mathbb{R}^n$, $\bar{t}_0 \in \mathbb{R}^k$, $f(\bar{x}_0,\bar{t}_0) = \bar{0}$ and $J_f(\bar{x}_0,\bar{t}_0) \ne 0$.
	Then there exists an open set $T_0$ containing $\bar{t}_0$ in $\mathbb{R}^k$ and
	a unique function $g : T_0 \to \mathbb{R}^n$ such that
\begin{enumerate}
	\item $g$ is continuously differentiable on $T_0$
	\item $g(\bar{t}_0) = \bar{x}_0$
	\item $f(g(\bar{t}),\bar{t}) = \bar{0},\ \forall \bar{t} \in T_0$
\end{enumerate}
\end{theorem}
\begin{proof}
	Given $f : S \to \mathbb{R}^n$ where $S \subset \mathbb{R}^{n+k}$.
	We have $f = (f_1,f_2,\cdots,f_m)$.
	Also given that $f \in C'(S)$, $f(\bar{x}_0;\bar{t}_0) = 0$, and $J_f(\bar{x}_0;\bar{t}_0) \ne 0$.
	Define a function $F : S \to \mathbb{R}^{n+k}$ defined by $F = (F_1,F_2,\cdots,F_{n+k})$.
	$$ F_m(\bar{x};\bar{t}) = \begin{cases}
		f_m(\bar{x};\bar{t}) & 1 \le m \le n\\
		t_{m-n} & n < m \le n+k
	\end{cases} $$
	\begin{commentary}
		For example, let $n = 3$, $k = 2$.
		Let $\bar{x} = (1,2,3)$ and $\bar{t} = (4,5)$.
		Suppose $f_k(1,2,3,4,5) = a_k$.
		Then $F(\bar{x};\bar{t}) = (a_1,a_2,a_3,4,5)$.
	\end{commentary}

	Then the Jacobian determinant of $F'(\bar{x};\bar{t})$ is given by
	$$ J_F(\bar{x};\bar{t}) = \begin{vmatrix}
		\begin{matrix}
			D_1f_1 & D_2f_1 & \cdots & D_mf_1 \\
			D_1f_2 & D_2f_2 & \cdots & D_mf_2 \\
			\vdots & \vdots & \ddots & \vdots \\
			D_1f_m & D_2f_m & \cdots & D_mf_m
		\end{matrix}
		& 0 & \cdots & 0 \\
		0 & 1 & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & 1 
	\end{vmatrix} $$
	Thus $J_F(\bar{x}_0;\bar{t}_0) = J_f(\bar{x}_0;\bar{t}_0) \ne 0$.
	Also, $F(\bar{x}_0;\bar{t}_0) = (\bar{0};\bar{t}_0)$.
	Therefore, by inverse function theorem, there exists open sets $X,Y$ containing $(\bar{x}_0;\bar{t}_0)$ and $(\bar{0};\bar{t}_0)$ such that $F$ is injective on $X$, $X = F^{-1}(Y)$ and there exists a unique local inverse function $G$ such that $G(F(\bar{x};\bar{t})) = (\bar{x};\bar{t})$ and $G \in C'(Y)$.

	Let $G = (v;w)$.
	That is, $v_i = G_i$ for $1 \le i \le n$.
	And $w_j = G_{n+j}$ for $1 \le j \le k$.
	We have, $G(F(\bar{x};\bar{t})) = (\bar{x};\bar{t})$.
	Therefore, $v(F(\bar{x};\bar{t})) = \bar{x}$ and $w(F(\bar{x};\bar{t})) = \bar{t}$.
	Since $X \subset F^{-1}(Y)$ and $F$ is one-to-one on $X$, for every $(\bar{x};\bar{t}) \in Y$, there exists $(\bar{x}';\bar{t}') \in X$ such that $F(\bar{x}';\bar{t}') = (\bar{x};\bar{t})$.
	By the definition of $F$, $\bar{t}' = \bar{t}$.
	Therefore, $v(\bar{x};\bar{t}) = v(F(\bar{x}';\bar{t}) = \bar{x}'$ and $w(\bar{x};\bar{t}) = w(F(\bar{x}';\bar{t}) = \bar{t}$.
	Now, we have $G : Y \to X$ defined by $G(\bar{x};\bar{t}) = (\bar{x}';\bar{t})$.

	Let $T_0$ be a subset of $\mathbb{R}^k$ defined by $T_0 = \{ \bar{t} \in \mathbb{R}^k : (\bar{0};\bar{t}) \in Y \}$.
	For each $\bar{t} \in T_0$ let $g : T_0 \to \mathbb{R}^n$ is defined by $g(\bar{t}) = v(\bar{0};\bar{t})$.
	The set $T_0$ is open.
	And $g \in C'(T_0)$ since $g$ is constructed from the components of $G$ which has continuous partial derivatives on $Y$.
	(ie, $G \in C'(Y)$.)

	Clearly, $g(\bar{t}_0) = v(\bar{0};\bar{t}_0) = \bar{x}_0$.
	And $(\bar{0};\bar{t}) = F(\bar{x}_0;\bar{t}_0)$.
	Therefore, we have $f(v(\bar{x};\bar{t});\bar{t}) = \bar{x}$.
	Let $\bar{x} = \bar{0}$, then $f(g(\bar{t});\bar{t}) = \bar{0}$.
	It is enough to prove that the function $g$ is unique.
	Suppose $f(g(\bar{t});\bar{t}) = f(h(\bar{t});\bar{t})$.
	Since $f$ is one-to-one on $X$, $(g(\bar{t});\bar{t}) = (h(\bar{t});\bar{t})$ for every $\bar{t} \in T_0$.
	And $g(\bar{t}) = h(\bar{t}),\ \forall \bar{t} \in T_0$.
\end{proof}

\subsubsection{Extrema of function of one variable}
\begin{theorem}
	Let $n \ge 1$ and function $f$ has $n$th partial derivative in open interval $(a,b)$.
	Suppose for some $c \in (a,b)$,
	\[ f'(c) = f''(c) = \cdots = f^{(n-1)}(c) = 0 \text{ and } f^{(n)}(c) \ne 0 \]
	If $n$ is even,
\begin{enumerate}
	\item $f$ has a local minimum at $c$ if $f^{(n)}(c)>0$
	\item $f$ has a local maximum at $c$ if $f^{(n)}(c) < 0$
\end{enumerate}
	and If $n$ is odd, there is neither a local minimum nor a local maximum at $c$.
\end{theorem}
\begin{proof}
\end{proof}

\subsubsection{Extrema of function of several variables}
\begin{definition}
	If function $f$ is differentiable at $\bar{a}$ and $\nabla f(\bar{a}) = \bar{0}$, the point $\bar{a}$ is a stationary point of $f$.
\end{definition}

\begin{definition}
	A stationary point is a saddle point if every $n$-ball $B(\bar{a})$ contains points $\bar{x}$ such that $f(\bar{x}) > f(\bar{a})$ and other points such that $f(\bar{x}) < f(\bar{a})$.
\end{definition}

\begin{definition}
	A function $Q : \mathbb{R}^n \to \mathbb{R}$ defined $Q(\bar{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j$ where $a_{ij} \in \mathbb{R}$ is a function of the quadratic form or simply a quadratic form.
\begin{description}
	\item[symmetric quadratic form] $a_{ij} = a_{ji},\ \forall i,j$. 
	\item[positive definite quadratic form] $Q(\bar{x}) > 0, \forall \bar{x} \ne \bar{0}$. 
	\item[negative definite quadratic form] $Q(\bar{x}) < 0, \forall \bar{x} \ne \bar{0}$. 
\end{description}
\end{definition}

\begin{theorem}
	Let $f$ be a function $f : \mathbb{R}^n \to \mathbb{R}^m$.
	Suppose that the second order partial derivatives $D_{i,j}f$ exists in an $n$-ball $B(\bar{a})$ and are continuous at $\bar{a}$ where $\bar{a}$ is a stationary point of $f$.
	\[ \text{Let } Q(\bar{t}) = \frac{1}{2} f''(\bar{a},\bar{t}) = \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n D_{i,j} f(\bar{a})t_it_j \]
\begin{enumerate}
	\item If $Q(\bar{t}) > 0$ for all $\bar{t} \ne \bar{0}$, $f$ has a relative minimum at $\bar{a}$.
	\item If $Q(\bar{t}) < 0$ for all $\bar{t} \ne \bar{0}$, $f$ has a relative maximum at $\bar{a}$.
	\item If $Q(\bar{t})$ takes both positive and negative values, then $f$ has a saddle point at $\bar{a}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}

\begin{theorem}
Let $f$ be a real-valued function $f : \mathbb{R}^2 \to \mathbb{R}$ with continuous second order partial derivatives at a stationary point $\bar{a} \in \mathbb{R}^2$.
Let $A = D_{1,1}f(\bar{a})$, $B = D_{1,2}f(\bar{a}) = D_{2,1}f(\bar{a})$, and $C = D_{2,2}f(\bar{a})$.
And let $\Delta = \begin{vmatrix}A & B \\ B & C \end{vmatrix} = AC - B^2$.
Then we have,
\begin{enumerate}
	\item If $\Delta > 0$ and $A > 0$, then $f$ has a relative minimum at $\bar{a}$.
	\item If $\Delta > 0$ and $A < 0$, then $f$ has a relative maximum at $\bar{a}$.
	\item if $\Delta < 0$, then $f$ has a saddle point at $\bar{a}$.
\end{enumerate}
\end{theorem}
\begin{proof}
\end{proof}


%\chapter{Multiple Riemann Integrals*}
%\chapter{Multiple Lebesgue Integrals*}
%\chapter{Cauchy's Theorem and the Residue Calculus*}
%\cite{rudin}
%\chapter{The Real and Complex Number Systems}
%\chapter{Basic Topology}
%\chapter{Numerical Sequences and Series}
%\chapter{Continuity}
%\chapter{Differentiation}
%\chapter{The Riemann- Stieltjes Integral}
%\chapter{Sequences and Series of Functions}
%\chapter{Some Special Functions}
%\chapter{Functions of Several Variables}
%\chapter{Integration on Differential Forms}
\section{Integration on Differential Forms}
\begin{definition}
	A $k$-cell in $R^k$ is given by, 
	$$I^k = \{ \bar{x} \in \mathbb{R}^k : a_i \le x_i \le b_i,\ \forall i \}$$
	where $\bar{a},\bar{b} \in \mathbb{R}^k$.
	Let $f$ be a continuous, real-valued function on $I^k$.
	Then, the integral of $f$ over $I^k$ is given by,
	$$\int_{I^k}f(\bar{x}) d\bar{x} = f_0 \text{ where } f_k = f \text{ and }$$
	$$f_{r-1} = \int_{a_r}^{b_r} f_r(x_0,x_1,\cdots,x_r) dx_r,\ r = 1,2,\cdots,k$$
\begin{commentary}
	In other words,
	$$\int_{I^k} f(\bar{x})\ d\bar{x} = \idotsint_{a_k}^{b_k} \left[f(x_0,x_1,\cdots,x_k)\ dx_k\right]\ dx_{k-1}\cdots dx_1$$
\end{commentary}
\end{definition}

\begin{theorem}
	For every $f \in \mathscr{C}(I^k)$, $L(f) = L'(f)$.

\begin{commentary}
	In other words, integral of a function over a $k$-cell is independent of the order in which those $k$ integrations are carried out.
\end{commentary}
\end{theorem}
\begin{proof}
	Step 1 : ``Separable'' Functions ie, $h(\bar{x}) = \prod h_i(x_i)$.\\
	\begin{commentary}
		(``separable'' is not standard.
		It is only for the purpose of understanding.)
	\end{commentary}

	Let $h(\bar{x}) = h_1(x_1)h_2(x_2)\cdots h_k(x_k)$ where $h_j \in [a_j,b_j]$.
	$$ L(h) = \int_{I^k} \left(\prod_{i = 1}^k h_i(x_i)\right) d\bar{x} = \prod_{i = 1}^k \int_{a_k}^{b_k} h_i(x_i)\ dx_i = L'(h)$$
	Step 2 : Algebra of ``separable'' functions, $\mathscr{A}$.\\
	Let $\mathscr{A}$ be all finite sums of functions such as $h$.
	Let $g \in \mathscr{A}$.
	\begin{align*}
		L(g) = & \int_{I^k} \left(\sum_j \prod_i h_{i,j}(x_i) \right) d\bar{x} \\
		& = \sum_j \int_{I^k} \prod_i h_{(j)}(\bar{x})\ d\bar{x}\\
		& = \sum_j \prod_i \int_{a_k}^{b_k} h_{i,j}(x_i)\ d(x_i)\\
		& = L'(g)
	\end{align*}
	Step 3 : All functions continuous on $I^k$.\\
	Let $V = \prod_{j = 1}^k (b_j - a_j)$.
	Also let $f \in \mathscr{C}(I^k)$.
	ie, a function which is continuous in $I^k$.
	Then by Stone-Weierstrass theorem,
	for any $\epsilon > 0$,	there exists a function $g \in \mathscr{A}$
	such that $\|f-g\| < \frac{\epsilon}{V}$
	where the norm of a function $f$ is defined by
	$\|f\| = \max\{f(\bar{x}) : \bar{x} \in I^k\}$.

	Therefore, it is sufficient to prove that $\|L(f)-L'(f)\| < \epsilon$.
	Since $\|f-g\| < \frac{\epsilon}{V}$, $|L(f-g)| < \epsilon$ and $|L'(f-g)| < \epsilon$. Thus,
	\begin{align*}
		L(f)-L'(f) & = L(f) - L(g) + L'(g) - L'(f) \\
		& = L(f-g) + L'(g-f)\\
		|L(f)-L'(f)| & < 2\epsilon
	\end{align*}
	Therefore, $L(f) = L'(f)$.
\end{proof}

\begin{definition}
	Let $f : \mathbb{R}^k \to \mathbb{C}$.
	The support of $f$ is the closure of the set of all points $\bar{x} \in \mathbb{R}^k$ such that $f(\bar{x}) \ne 0$.

	Let $f$ be a continuous function with compact support.
	And $I^k$ be any $k$-cell containing the support of $f$.
	Then, $\int_{\mathbb{R}^k} f\ d\bar{x} = \int_{I^k} f\ d\bar{x}$.
\end{definition}

\begin{commentary}
	$L(f) = L'(f)$ where $f$ is the limit function of a sequence of functions with compact support.
	\cite[\S10.4 Example]{apostol}

	``The definition of support permits closure of non-vanishing points.
	And this might be of some use(I don't know yet) when analysing limit functions which belong to the closure of continuous functions on $I^k$.''
\end{commentary}

\begin{definition}
	Let $E$ be an open subset in $\mathbb{R}^n$.
	Then function $G : E \to \mathbb{R}^n$ is primive if it satisfies
	\begin{equation}
		G(\bar{x}) = \sum_{i \ne m} x_i \bar{e}_i + g(\bar{x})\bar{e}_m
	\end{equation}
	for some integer $m$ and some function $g : E \to \mathbb{R}$ (where $\bar{e}_i$ are the unit co-ordinate vectors).
	\begin{equation}
		G(\bar{x}) = \bar{x} + [g(\bar{x})-x_m]\bar{e}_m
	\end{equation}
	If $g$ is differentiable at $\bar{a}$, then $G$ is also differentiable at $\bar{a}$.
	The matrix $[\alpha_{i,j}]$ of $G'(\bar{a})$ is given by 
	$$\alpha_{i,j} = \begin{cases} 
		D_jg(\bar{a}) & i = m\\
		1 & i \ne m, j = i \\
		0 & i \ne m, j \ne i
	\end{cases} $$
	The Jacobian of $G$ at $\bar{a}$ is given by,
	$J_G(\bar{a}) = \det[G'(\bar{a})] = D_mg(\bar{a})$.

	Total derivative $G'(\bar{a})$ is invertible if and only if $D_mg(\bar{a}) \ne 0$.
\end{definition}

\begin{definition}
	A linear operator $B$ on $\mathbb{R}^n$ that interchanges some pair of members of the standard basis and leaves the others fixed is a flip.
\end{definition}

\begin{theorem}
	Suppose $F$ is a $\mathscr{C}$-mapping of an open set $E \subset \mathbb{R}^n$ into $\mathbb{R}^n$, $\bar{0} \in E$, $F(\bar{0}) = \bar{0}$, and $F'(\bar{0})$ is invertible.
	Then there is a neighbourhood of $\bar{0}$ in $\mathbb{R}^n$ in which the representation $F(\bar{x}) = B_1B_2\cdots B_{n-1}G_n\circ G_{n-1}\circ \cdots G_1(\bar{x})$ is valid where $G_i$ are primitive $\mathscr{C}$-mapping in some neighbourhood of $\bar{0}$, $G_i(\bar{0}) = \bar{0}$, $G'(\bar{0})$ is invertible and each $B_i$ is either a flip or identity operator.

\begin{commentary}
	In other words, locally $F$ is a composition of primitive mappings and flips.
\end{commentary}
\end{theorem}
\begin{proof}
\end{proof}

\begin{theorem}
	Suppose $K$ is a compact subset of $\mathbb{R}^n$, and $\{V_\alpha\}$ is an open cover of $K$.
	Then there exists functions $\psi_1,\psi_2,\cdots,\psi_s \in \mathscr{C}(\mathbb{R}^n)$ such that
	\begin{enumerate}
		\item $0 \le \psi_i \le 1$ for $1 \le i \le s$
		\item each $\psi_i$ has its support in some $V_\alpha$ and
		\item $\psi_1(\bar{x}) + \psi_2(\bar{x}) + \cdots + \psi_s(\bar{x}) = 1$ for every $\bar{x} \in K$.
	\end{enumerate}
\end{theorem}

\begin{corollary}
	If $f \in \mathscr{C}(\mathbb{R}^n)$ and the support of $f$ lies in $K$,
	then\\ $f = \sum\limits_{i=1}^s \psi_i f$. Each $\psi_i$ has its support in some $V_\alpha$.
\end{corollary}

\begin{theorem}
	Suppose $T$ is a one-to-one $\mathscr{C}$-mapping of an open set $E \subset \mathbb{R}^k$ into $\mathbb{R}^k$ such that $J_T(\bar{x}) \ne 0$ for all $\bar{x} \in E$.
	If $f$ is a continuous function on $\mathbb{R}^k$ whose support is compact and lies in $T(E)$, then
	\begin{equation}
		\int_{\mathbb{R}^k} f(\bar{y})\ d\bar{y} = \int_{\mathbb{R}^k} f(T(\bar{x}))|J_T(\bar{x})|\ d\bar{x}
	\end{equation}
\end{theorem}
\begin{proof}

\end{proof}

\begin{definition}
	Suppose $E$ is an open set in $\mathbb{R}^n$.
	A $k$-surface in $E$ is a $\mathscr{C}'$-mapping $\Phi$ from a compact set $D \subset \mathbb{R}^k$ into $E$.
	$D$ is the parameter domain of $\Phi$.
\end{definition}

\begin{definition}
	Suppose $E$ is an open set in $\mathbb{R}^n$.
	A differential form of order $k \ge 1$ in $E$, a $k$-form in $E$ is a function $\omega$, symbolically represented by the sum $\omega = \sum a_{i1\cdots ik}(\bar{x})\ dx_{i1}\wedge dx_{i2} \wedge \cdots dx_{ik}$ which assigns to each $k$-surface $\Phi$ in $E$ a number $\omega(\Phi) = \int_{\Phi} \omega $, according to the rule
	\begin{equation}
		\int_{\Phi} \omega = \int_D \sum a_{i1\cdots ik}(\Phi(\bar{u})) \frac{\partial(x_{i1},x_{i2},\cdots,x_{ik})}{\partial(u_1,u_2,\cdots,u_k)}d\bar{u}
	\end{equation}
	where $D$ is the paramter domain of $\Phi$.

	A $k$-form $\omega$ is of class $\mathscr{C}'$ or $\mathscr{C}''$ if the functions $a_{i1\cdots ik}$ are all of class $\mathscr{C}'$ or $\mathscr{C}''$.
	A $0$-form in $E$ is defined to be a continuous function in $E$.
\end{definition}

\begin{remark}[Elementary properties of $k$-forms].
	\begin{enumerate}
		\item $\omega_1 = \omega_2 \iff \omega_1(\Phi) = \omega_2(\Phi)$
		\item 
	\end{enumerate}
\end{remark}

\begin{definition}
	Let $\bar{I} = (i_1,i_2,\cdots,i_k)$ be an increasing $k$-index.
	That is, $1 \le i_1 \le i_2 \le \cdots i_k \le n$.
	Then $dx_{\bar{I}}$ of the form $dx_{\bar{I}} = dx_{i1} \wedge dx_{i2} \cdots dx_{ik}$ is basic $k$-form in $\mathbb{R}^n$.

	There are $(^n_k)$ basic $k$-forms in $\mathbb{R}^n$.

	Every $k$-form can be represented in terms of basic $k$-forms.
	For every $k$-tuple $(j_1,j_2,\cdots,j_k)$, $dx_{j1} \wedge \cdots dx_{jk} = \epsilon(j_1,j_2,\cdots,j_k)dx_{\bar{J}}$ where $\bar{J}$ is the increasing $k$-index obtained by interchanging pairs.
\end{definition}

%\chapter{The Lebesgue Theory}

