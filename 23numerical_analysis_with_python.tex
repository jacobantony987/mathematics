%Text books : \cite{brigs}, \cite{saha}, \cite{kiusalaas}
%Module 1:
%Defining Symbols and Symbolic Operations, Working with Expressions, Solving Equations and Plotting Using SymPy, problems on factor finder, summing a series and solving single variable inequalities
%Chapter 4 of \cite{saha} 
%Module 2:
%Finding the limit of functions, finding the derivative of functions, higher-order derivatives and finding the maxima and minima and finding the integrals of functions are to be done. in the section programming challenges, the following problems - verify the continuity of a function at a point, area between two curves and finding the length of a curve
%Chapter 7 of \cite{saha} 
%Module 3:
%Interpolation and Curve Fitting - Polynomial Interpolation - Lagrange's Method, Newton's Method and Limitations of Polynomial Interpolation, Roots of Equations - Method of Bisection and Newton-Raphson Method.
%Sections 3.1, 3.2, 4.1, 4.3, 4.5 of \cite{kiusalaas}
%Module 4:
%Gauss Elimination Method (excluding Multiple Sets of Equations), Doolittle's Decomposition Method only from LU Decomposition Methods, Numerical Integration, Newton-Cotes Formulas, Trapezoidal rule, Simpson's rule and Simpson's 3/8 rule.
%Sections 2.2, 2.3, 6.1, 6.2 of \cite{kiusalaas}

%Programs
%Online Program, Explanation, Algorithm, Function Syntax, Terminology
\part{ME010203 Numerical Analysis with Python}

%\chapter{Module I}
%\chapter{Expressions}

%\chapter{Module II}
%\chapter{Calculus}

\chapter{Module III}
%\chapter{Interpolation \& Curve Fitting}
\begin{definition}
	Given $(n+1)$ data points $(x_k, y_k),\ k = 0,1,\cdots,n$, the problem of estimating $y(x)$ using a function $y : \mathbb{R} \to \mathbb{R}$ that satisfy the data points is the interpolation problem. ie, $y(x_k) = y_k,\ k = 0,1,\cdots,n$.
\end{definition}
\begin{definition}
	Given $(n+1)$ data points $(x_k,y_k),\ k = 0,1,\cdots,n$, the problem of estimating $y(x)$ using a function $y : \mathbb{R} \to \mathbb{R}$ that is sufficiently close to the data points is the curve-fitting problem.\\ ie, Given $\epsilon > 0,\ |y(x_k)-y_k| < \epsilon,\ k = 0,1,\cdots,n$.
\end{definition}
\begin{remark}
\begin{commentary}
The data could be from scientific experiments or computations on mathematical models. The interpolation problem assumes that the data is accurate. But, curve-fitting problem assumes that there are some errors involved which are sufficiently small.
\end{commentary}
\end{remark}
\begin{definition}
	Given $(n+1)$ data points $(x_k,y_k),\ k = 0,1,\cdots,n$, the problem of estimating $y(x)$ using a polynomial function of degree $n$ that satisfy the data points is the polynomial interpolation problem.
\end{definition}
\begin{remark}
	Polynomial is the `simplest' interpolant.\cite[3.2]{kiusalaas}
\end{remark}

\section{Polynomial Interpolation}
There exists a unique polynomial of degree $n$ that satisfy $(n+1)$ distinct data points. There are a few methods to find this polynomial : 
\begin{enumerate*}
	\item Lagrange's method
	\item Newton's method
	\item Neville's method
\end{enumerate*}. The Neville's method is out of scope.

\subsection{Lagrange's Method}
Interpolation polynomial\footnote{Using $P_n$ to represent some polynomial of degree $n$. It is quite a confusing a notation when it comes to Newton's method as author construct a psuedo-recursive definition.} is given by,
\begin{equation}
	P(x) = \sum_{i=0}^n y_i l_i(x),\text{ where } \l_i(x) = \prod_{j = 0,j \ne i}^n \frac{x-x_i}{x_j-x_i}
	\label{equ:lagrange}
\end{equation}
\begin{remark}
	Lagrange's cardinal functions $l_i$, are polynomials of degree $n$ and
	$$l_i(x_j) = \delta_{ij} = \begin{cases} 0,\ i = j \\ 1,\ i \ne j \end{cases}$$
\end{remark}
\begin{proposition}
	Error in polynomial interpolation is given by
	\begin{equation}
		f(x) - P(x) = \frac{(x-x_0)(x-x_1)\cdots(x-x_n)}{(n+1)!} f^{(n+1)}(\xi)
		\label{equ:error}
	\end{equation}
	where $\xi \in (x_0, x_n)$
\end{proposition}
\begin{remark}
	The error increases as $x$ moves away from the unknown value $\xi$.
\end{remark}

\subsection{Newton's Method}
The interpolation polynomial is given by,
\begin{equation}
	P(x) = a_0 + a_1(x-x_0) + \cdots + a_n(x-x_0)(x-x_1)\cdots(x-x_{n-1})
	\label{equ:newton}
\end{equation}
where $a_i = \nabla^i y_i,\ i = 0,1,\cdots,n$.
\begin{remark}
	For Newton's Method, usually it is assumed that $x_0 < x_1 < \cdots < x_n$.
\end{remark}
\begin{remark}
	 Lagrange's method is conceptually simple. But, Newton's method is computationaly more efficient than Lagrange's method.
\end{remark}
\subsubsection{Computing coefficients $a_i$ of the polynomial}
The coefficients are given by,
\begin{equation}
	a_0 = y_0,\ a_1 = \nabla y_1,\ a_2 = \nabla^2 y_2,\ a_3 = \nabla^3 y_3,\cdots, a_n = \nabla^n y_n
\end{equation}
\begin{remark}
	The divided difference $\nabla^i y_i$ are computed as follows:
	\begin{align*}
		\nabla y_1 = \frac{y_1 - y_0}{x_1 - x_0} & & \\
		\nabla y_2 = \frac{y_2 - y_1}{x_2 - x_1} &\qquad \nabla^2 y_2 = \frac{\nabla y_2 - \nabla y_1}{x_2-x_1} & \\
		\nabla y_3 = \frac{y_3 - y_2}{x_3 - x_2} &\qquad \nabla^2 y_3 = \frac{\nabla y_3 - \nabla y_2}{x_3-x_2} & \nabla^3 y_3 = \frac{\nabla^2 y_3 - \nabla^2 y_2}{x_3-x_2}
	\end{align*}
\end{remark}
\begin{table}[hb]
	\centering
	\begin{tabular}{|c||c|c|c|c|c|}
		\hline
		$x_0$ & $y_0$ & & & & \\ \hline
		$x_1$ & $y_1$ & $\nabla y_1$ & & &   \\ \hline
		$x_2$ & $y_2$ & $\nabla y_2$ & $\nabla^2 y_2$ & &   \\ \hline
		\dots & \dots & \dots & \dots & $\ddots$ & \\ \hline
		$x_n$ & $y_n$ & $\nabla y_n$ & $\nabla^2 y_n$ & \dots & $\nabla^n y_n$  \\ \hline
	\end{tabular}
	\caption{The $\nabla^i y_i$ Computation Table}
\end{table}

\begin{remark}
	Practise Problems\\
	Find interpolation polynomial for the following data points :
	\begin{enumerate}
		\item $\{(0,7),\ (2,11),\ (3,28)\}$\\ \textcolor{blue}{Ans : $5x^2-8x+7$} \\ \cite[Example 3.1]{kiusalaas}
		\item $\{(-2,-1),\ (1,2),\ (4,59),\ (-1,4),\ (3,24),\ (-4,-53) \}$\\ \textcolor{blue}{Ans : $x^3-2x+3$}\\ \cite[Example 3.2]{kiusalaas}
		\item $\{(-1.2,-5.76),\ (0.3,-5.61),\ (1.1,-3.69)\}$\\ \textcolor{blue}{Ans : $x^2+x-6$}\\ \cite[Problem Set 3.1.1]{kiusalaas}
	%	\item $\{(0,-1.00),\ (0.5,1.75),\ (1,4.00),\ (1.5,5.75),\ (2,7.00)\}$\\ \cite[Problem Set 3.1.4]{kiusalaas}
	%	\item $\{(-2,-1),\ (1,2),\ (4,59),\ (-1,4),\ (3,24),\ (-4,-53)\}$\\ \cite[Problem Set 3.1.6]{kiusalaas}
		\item $\{(-3,0),\ (2,5),\ (-1,-4),\ (3,12),\ (1,0)\}$\\ \textcolor{blue}{Ans : $x^2+2x-3$}\\ \cite[Problem Set 3.1.7]{kiusalaas}
		\item $\{(0,1.225),(3,0.905),(6,0.652)\}$\\ \textcolor{blue}{Ans : $0.0037x^2-0.1178x+1.225$}\\ \cite[Problem Set 3.1.9]{kiusalaas}
	\end{enumerate}
\end{remark}

\begin{remark}
	In Lagrange's Method, we can interpolate at the given point even without computing the polynomial. In Newton's method, we have to compute polynomial and then interpolate for the given point.\\

	That is, evaluate the value of cardinal polynomials at the point and substitute in Equation \ref{equ:lagrange} as shown in Section 3.2.\cite[Example 3.1]{kiusalaas}
\end{remark}

\subsection{Implementation of Newton's Method}

\begin{program}Computing Coefficients
	\begin{python}
		def coefficients(xData,yData):
			m = len(xData)
			a = yData.copy()
			for k in range(1,m):
				a[k:m] = (a[k:m]-a[k-1])/(xData[k:m]-xData[k-1])
			return a
	\end{python}
\end{program}
\begin{commentary}
	Line 1 : \textbf{Defines a function which takes two arguments/parameters, named $xData$ and $yData$.} In \cite[3.2]{kiusalaas}, you will find coeffts which I have changed to coefficients. $xData$,$yData$ are numpy array objects. $xData$ is a array with values $x_0,x_1,\cdots,x_n$. And $yData$ is array with values $y_0,y_1,\cdots,y_n$. For example, the value of $x_3$ can be accessed as $xData[3]$.\\
	
	Line 2 : \textbf{The functon $len()$ is extended by numpy to give the length of array objects.} In this context, $len(xData)$ will return the value $n+1$, since there are $n+1$ values in $xData$ array.\\

	Line 3 : We need a copy of $yData$ to work with. Unlike other programming languages like java, in python $a = yData$ will assign a new label $a$ to the same memory location and manipulating $a$ will corrupt the original data in $yData$ as well. In order to avoid this, we are \textbf{making a copy of the array object using the array method provided by the numpy library.}\\

	Line 4 : \textbf{This is a python loop statement. This ask python interpreter to repeat the following sub-block $m-1$ times.}\footnote{Python block is a group of statement with same level of indentation. A sub-block is a block with an additional indentation.} In this context, Line 5 will be executed $n$ times, since the $range(1,m)$ object is a list-type object with values $1,2,\cdots,m-1$. And intepreter executes Line 5 for each values in the $range()$ object, ie, $k=1,2,\cdots, m-1$ before interpreting Line 6.\\

	Line 5 : This is very nice feature available in python. \textbf{This statement, evaluates $m-k$ values in a single step.ie, $a[k],a[k+1],\cdots,a[m]$. This calculation corresponds to subsequent columns of the divided difference table, that we are familiar with.} For example, executing Line 5 with $k=3$ is same as evaluating the $\nabla^3y_j$ column. Note that the value $a[0]$ is never updated and similarly $a[2]$ changes when Line 5 is executed with $k=1,2$. From column 3 onward, $a[2]$ is not updated. Therefore, \textbf{after completing $n$th executing of the Line 5, we have $a[0] = y_0,\ a[1]=\nabla y_1,\ a[2]=\nabla^2 y_2,\cdots,\ a[n]=\nabla^n y_n$.}\\

	Line 6 : \textbf{This returns the array $a$ which is the array of coefficients.}\\

	The logic of this program is in Line 4 and Line 5. So they need more explanation/understanding than anything else.
\end{commentary}

\begin{program}Interpolating using Newton's Method
	\begin{python}
		def interpolate(a,xData,x):
			n = len(xData)-1
			p = a[n]
			for k in range(1,n+1):
				p = a[n-k]+(x-xData[n-k])*p
			return p
	\end{python}
	The logic this program is in Line 3, Line 4 and Line 5.\\

	Line 3 : We initialize the polynomial with the coefficient $a[n] = \nabla^n y_n = a_n$.\\

	Line 4 : We are going define the polynomial recursively. This takes exactly $n$ steps further. So we use a loop which repeats $n$ times.\\

	Line 5 : The value of $p$ and $k$ changes each time Line 5 is executed. Let $P_j$ be the value in $p$ after executing Line 5 with $k=j$. Then,\\ $P_0 = p = a[n]$\\ $P_1 = a[n-1]+(x-x_{n-1})P_0$\\ $P_2 = a[n-2]+(x-x_{n-2})P_1$\\ $\vdots$\\ $P_n = a[0]+(x-x_0)P_{n-1}$.\\ Clearly, $P_n$ is the unique $n$ degree polynomial given by the Newton's method.
\end{program}

\begin{program}How to interpolate ?
	\begin{python}
		from numpy import array
		xData = array([-2,1,4,-1,3,-4]) #change as needed
		yData = array([-1,2,59,4,24,-53]) #change as needed
		a = coefficients(xData,yData)
		print(interpolate(a,xData,2))
	\end{python}
\end{program}

\begin{remark}
\begin{commentary}
	You will have to define both the functions (coefficients, interpolate) before doing this.\\

	Line 1 : For defining array objects, we need to import them from numpy library.\\

	Line 2 : You can change this line according to the first component of the given data points.\\

	Line 3 : You can change this line according to the second component of the given data points.\\

	Line 4 : Call function \texttt{coefficients} and store the array returned into a\\

	Line 5 : Call function \texttt{interpolate} to interpolate at $x = 2$ and print the value $P(2)$
\end{commentary}
\end{remark}

\begin{program}[Just for Fun]
	We can do more using sympy !
	\begin{python}
		from numpy import array
		from sympy import Symbol
		xData = array([-2,1,4,-1,3,-4])
		yData = array([-1,2,59,4,24,-53])
		a = coefficients(xData,yData)
		x = Symbol('x')
		p = interpolate(a,xData,x)
		p.subs({x:2})
	\end{python}
\end{program}

\begin{remark}
	Programming Problems
	\begin{enumerate}
%		\item $\{(4.0,-0.06604),\ (3.9,-0.02724),\ (3.8,0.01282),\ (3.7,0.05383)\}$\\ \cite[Example 3.3]{kiusalaas}
		\item $\{(0.15,4.79867),\ (2.30,4.49013),\ (3.15,4.2243),\ (4,85,3.47313),\\ (6.25,2.66674),\ (7.95,1.51909)\}$ \cite[Example 3.4]{kiusalaas}
%		\item $\{(0,1.8421),\ (0.5,2.4694),\ (1,2.4921),\ (1.5,1.9047),\ (2,0.8509),\ (2.5,-0.4112),\ (3,-1.5727)\}$\\ \cite[Problem Set 3.1.2]{kiusalaas}
		\item $\{(0,-0.7854),\ (0.5,0.6529),\ (1,1.7390),\ (1.5,2.2071),\ (2,1.9425)\}$\\ \cite[Problem Set 3.1.5]{kiusalaas}
	\end{enumerate}
\end{remark}

\subsection{Limitations of Polynomial Interpolation}
\begin{enumerate}
	\item Inaccuracy - The error in interpolation increases as the point moves away from most of the data points.
	\item Oscilation - As the number of data points considered for polynomial interpolation increases, the degree of the polynomial increases. And the graph of the interpolant tend to oscilate excessively. In such cases, the error in interpolation is quite high. 
	\item The best practice is to consider four to six data points nearest to the point of interest and ignore the rest of them.
\end{enumerate}

\begin{remark}
	The interpolant obtained by joining cubic polynomials corresponding to four nearest data points each, is a cubic spline\footnote{Cubic spline is a function, the graph of which is piece-wise cubic}.
\end{remark}

\section{Roots of a Function}
\begin{definition}
	Let $f : \mathbb{R} \to \mathbb{R}$, then $x \in \mathbb{R}$ is a root of $f$ if $f(x) = 0$.
\end{definition}

\begin{remark}
	Suppose $a <  b$ and $f(a), f(b)$ are nonzero and are of different signs. If $f$ is continuous in $[a,b]$, then there is a point $c \in [a,b]$ such that $f(c) = 0$.\\
	
	Thus given $a<b$ and $f(a),f(b)$ are nonzero values of different sign, then there may be a bracketed root in $[a,b]$.\\
	
	Note : There is no guarantee that there exists a root in $[a,b]$ as we are not sure about the continuity of $f$.
\end{remark}

\begin{remark}
	Given a bracketed root, we can find it using
	\begin{enumerate}
		\item Bisection Method or
		\item Newton-Raphson Method
	\end{enumerate}
\end{remark}

\subsection{Bisection Method}
	Suppose $a < b$ and $f(a),f(b)$ are nonzero values of different signs. We evaluate $f(c)$ where $c = \frac{a+b}{2}$. If $f(c)$ is a nonzero value, then at least one of the pairs $f(a),f(c)$ or $f(c),f(b)$ are of different signs. WLOG suppose that $f(a),f(c)$ are of different signs, then set $b = c$ and $c = \frac{a+b}{2}$. And continue this process until we get sufficiently accurate value of a root.

\begin{remark}
	Suppose $f(x) = x^5 - 2$. Then $f(0) = -2,\ f(1) = -1,\ f(2) = 30$. Since $f$ is known to be continuous, there is a bracketed root in $[1,2]$. Now $f(1.5) > 0 \implies [1,1.5]$\\
	$f(1.25) > 0 \implies [1,1.25]$\\
	$f(1.125) < 0 \implies [1.125,1.25]$\\
	$f(1.1825)>0 \implies [1.125,1.1825]$\\
	$f(1.15375)>0 \implies [1.125,1.15375]$\\
	$f(1.139375) < 0 \implies [1.139375,1.15375]$\\
	$f(1.1465625) < 0 \implies [1.1465625,1.15375]$\\
	$f(1.150156250) > 0 \implies [1.1465625,1.15015625]$\\
	$f(1.148359375) < 0 \implies [1.1483594,1.15015625]$\\
	$f(1.149257825) > 0 \implies [1.1483594,1.14925783]$\\
	
	Thus, we have $1.14$ is a root of $f$ with accuracy upto two decimal points.
\end{remark}
%fifth root of 2 = 1.14869835

\subsection{Newton-Raphson Method}
	Suppose $f$ is differentiable at $x \in \mathbb{R}$ and $f(x) \ne 0$. Then compute $x = x - \frac{f(x)}{df(x)}$ and evaluate $f(x)$. Repeat this process to get more accurate value of a root near $x$.

\begin{remark}
	Suppose $f(x) = x^5 - 2$. Then $df(x) = 5x^4$. Let $x = 2$. Then\\
	\begin{align*}
		x = 2 - \frac{30}{80} & \implies f(1.625) = 9.330 \\
		x = 1.625 - \frac{9.330}{34.86} & \implies f(1.35735) = 2.6074 \\
		x = 1.35735 - \frac{2.6074}{16.9721} & \implies f(1.20373) = 0.52733 \\
		x = 1.20373 - \frac{0.52733}{10.4975} & \implies f(1.15351) = 0.04224 \\
		x = 1.15351 - \frac{0.042245}{8.85225} & \implies f(1.148738) = 0.00034312 \\
	\end{align*}

	Thus we have $1.1487$ is quite close to a root of $f$.
\end{remark}

%\chapter{Matrix Operations}
\chapter{Module IV}

	Consider a system of $n$ linear, simultaneous equations in $n$ unknowns,
	\begin{align*}
		A_{11}x_1 + A_{12}x_2 + \cdots + A_{1n}x_n & = b_1 \\
		A_{21}x_1 + A_{22}x_2 + \cdots + A_{2n}x_n & = b_2 \\
		\vdots \\
		A_{n1}x_1 + A_{n2}x_2 + \cdots + A_{nn}x_n & = b_n 
	\end{align*}

	We may represent them using matrices as $Ax = b$. That is,
	\[ \begin{bmatrix} A_{11} & A_{12} & \cdots & A_{1n} \\ A_{21} & A_{22} & \cdots & A_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ A_{n1} & A_{n2} & \cdots & A_{nn} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} \]
		
	Gauss Elimination and Doolittle Decomposition are two methods to solve sytem of equaitons, $Ax = b$.

\section{Gauss elimination method}
	Gauss elimination method has of two phases
	\begin{enumerate*}
		\item elimination and
		\item back substitution
	\end{enumerate*}.
	In elimination phase, system $Ax = b$ is transformed into an equivalent system $Ux = c$ where $U$ is an upper-triangular\footnote{upper triangular - all the entries below the main diagonal are zero. ie $U_{ij} = 0,\ \text{ if }i<j$} matrix. And in back substitution phase, $Ux = c$ is solved. Since $Ax = b$ and $Ux =  c$ are equivalent, they have the same solution $x$.

\subsection{Elimination Phase}
	We can eliminate unknowns from an equation by adding a scalar multiple of an equation to another equation of the system. In matrices, this is equivalent to adding a scalar multiple of one row to another row, say $R_i \leftarrow R_i + \lambda{}R_k$.
	\begin{align*}
		A_{k1}x_1 + A_{k2}x_2 + \cdots + A_{kn}x_n = b_k  & + \\
		\lambda \left( A_{i1}x_1 + A_{i2}x_2 + \cdots + A_{in}x_n  = b_i \right) & \\ \hline
		(A_{k1}+\lambda{}A_{i1})x_1 + (A_{k2}+\lambda{}A_{i2})x_2 + \cdots + (A_{kn} + \lambda{}A_{in})x_n & = b_k + \lambda{}b_i
	\end{align*}

	\[ \begin{bmatrix} \vdots & \vdots & \vdots & \vdots \\ A_{k1} & A_{k2} & \cdots & A_{kn} \\ \vdots & \vdots & \ddots & \vdots \\ A_{i1} & A_{i2} & \cdots & A_{in} \\ \vdots & \vdots & \vdots & \vdots \end{bmatrix} \xrightarrow{R_i \leftarrow R_i + \lambda{}R_k} \begin{bmatrix} \vdots & \vdots & \vdots & \vdots \\ A_{k1} & A_{k2} & \cdots & A_{kn} \\ \vdots & \vdots & \ddots & \vdots \\ A_{i1}+\lambda{}A_{k1} & A_{i2}+\lambda{}A_{k2} & \cdots & A_{in}+\lambda{}A_{kn} \\ \vdots & \vdots & \vdots & \vdots \end{bmatrix}\]
		\[ \begin{bmatrix} \vdots \\ b_k \\ \vdots \\ b_i \\ \vdots \end{bmatrix} \xrightarrow{R_i \leftarrow R_i + \lambda{}R_k} \begin{bmatrix} \vdots \\ b_k \\ \vdots \\ b_i + \lambda{}b_k \\ \vdots \end{bmatrix} \]
			
%	However, we can reduce the number of operations\footnote{Number of steps is an important measure of the quality of algorithm.} required for the elimination. First we will eliminate $x_1$ from row $R_i,\ i = 2,3,\cdots,n$ using a suitable scalar, say $A_{i1}/A_{11}$. Then, we will eliminate $x_2$ from $R_i,\ i = 3,4,\cdots,n$. And so on.\\

%	When $x_1$ is eliminated from $R_2$, we get $A_{k1} = 0,\ k=2,3,\cdots,n$. The variable $x_2$, is eliminated from equations $3,4,\cdots,n$ by the operation, $R_i \rightarrow R_i + \lambda{}R_2$ where $i = 3,4,\cdots,n$. However, $A_{21}$ is already reduced to zero, therefore we have to add only the scalar multiples of the nonzero values in the row $R_2$. Consider the elimination of $x_k$ from equation $i$ where $ i = k+1,k+2,\cdots,n$, we have $A_{k,1},A_{k,2},\cdots, \text{and}, A_{k,k-1}$ are already zero due to the elimination of $x_1,x_2,\cdots, \text{and} ,x_{k-1}$. Thus, adding scalar multiple (zero) of these entries can be avoided to reduce the number of operations.

\subsection{Back substitution}
	Let $Ux = c$ be a system of $n$ linear equations in $n$ unknowns and $U$ is an upper triangular matrix. Then we can solve the system of equations from the back.
	\[ \begin{bmatrix} U_{1,1} & U_{1,2} & \cdots & U_{1,n-1} & U_{1,n} \\ 0 & U_{2,2} & \cdots & U_{2,n-1} & U_{2,n} \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & U_{n-1,n-1} & U_{n-1,n} \\ 0 & 0 & \cdots & 0 & U_{n,n} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_{n-1} \\ x_n \end{bmatrix} = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_{n-1} \\ c_n \end{bmatrix} \]
	\begin{align*}
		U_{n,n}\ x_n = c_n & \implies x_n = \frac{c_n}{U_{n,n}} \\
		\sum_{i=n-1}^{n} U_{n-1,i}\ x_i = c_{n-1}  & \implies x_{n-1} = \frac{c_{n-1} - U_{n-1,n} x_n}{U_{n-1,n-1}} \\
		& \cdots \\
		\sum_{i=1}^n U_{1,i}\ x_i  = c_1 & \implies x_1 = \frac{c_1 - \sum_{i = 2}^{n} U_{1,i} x_i}{U_{1,1}}
	\end{align*}

\subsection{Illustrative example}
	Consider the following system of linear equations,
	\begin{align*}
		4x_1  - 2x_2  + x_3 & = 11 \\
		-2x_1  + 4x_2 - 2x_3 & = -16 \\
		x_1  - 2x_2 + 4x_3 & = 17 
	\end{align*} 
	We may represent the above system of linear equations using matrices,

	\[ \begin{bmatrix} 4 & -2 & 1 \\ -2 & 4 & -2 \\ 1 & -2 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 11 \\ -16 \\ 17 \end{bmatrix} \]

	Phase 1 : Elimination Process\\

	Using eq.1, the unknown $x_1$ is eliminated from all subsequent equations. An equivalent operation can be performed on both the matrices $A$ and $b$ by adding a suitable scalar multiples of row $R_1$ to row $R_2$ and $R_3$. 

	\[ \begin{bmatrix} 4 & -2 & 1 \\ -2 & 4 & -2 \\ 1 & -2 & 4 \end{bmatrix} \xrightarrow{\substack{R_2 \leftarrow R_2 + 0.5R_1\\ R_3 \leftarrow R_3 - 0.25R_1}} \begin{bmatrix} 4 & -2 & 1 \\ 0 & 3 & -1.5 \\ 0 & -1.5 & 3.75 \end{bmatrix}\]
	\[ \begin{bmatrix} 11 \\ -16 \\ 17 \end{bmatrix} \xrightarrow{\substack{R_2 \leftarrow R_2 + 0.5R_1 \\ R_3 \leftarrow R_3 - 0.25R_1}} \begin{bmatrix} 11 \\ -10.5 \\ 14.25 \end{bmatrix} \]

	And using eq.2, $x_2$ is eliminated from all subsequent equations( only those rows below it). Again, we perform this by adding suitable scalar multiples of row 2 to row $R_3$.

	\[ \begin{bmatrix} 4 & -2 & 1 \\ 0 & 3 & -1.5 \\ 0 & -1.5 & 3.75 \end{bmatrix} \xrightarrow{R_3 \leftarrow R_3 + 0.5R_2} \begin{bmatrix} 4 & -2 & 1 \\ 0 & 3 & -1.5 \\ 0 & 0 & 3 \end{bmatrix} \]
	\[ \begin{bmatrix} 11 \\ -16 \\ 17 \end{bmatrix}  \xrightarrow{R_3 \leftarrow R_3 + 0.5R_2} \begin{bmatrix} 11 \\ -10.5 \\ 9 \end{bmatrix} \]

	The elimination process is complete when all entries below the diagonal elements are reduced to zero. ie, upper triangular matrix.\\

	Phase 2 : Back substitution Process\\

	The unknowns are easily found from the equations by solving them in the reverse order. The unknowns are solved from the bottom and solved variables are used to solve the remain unknowns.

	\[ \begin{bmatrix} 4 & -2 & 1 & 11 \\ 0 & 3 & -1.5 & -10.5 \\ 0 & 0 & 3 & 9 \end{bmatrix} \to \begin{cases} 4x_1 - 2x_2 + &x_3  = 11 \\ \quad \qquad 3x_2 - &1.5x_3  = -10.5 \\  &3x_3 = 9 \end{cases}  \]

	\begin{align*} x_3 & = \frac{9}{3} = 3 \\  x_2 & = \frac{-10.5 + 1.5x_3}{3} = -2 \\ x_1 & = \frac{11-x_3+2x_2}{4}=1 \end{align*}

\begin{remark}
\begin{commentary}
		Why don't they use row-reduced echelon matrix of $A$ to simplify the back substitution phase ?\\

		This doesn't have much advantage from algorithmic point of view. That is, the time complexity ( number of steps for computation) is unaffected. And algorithms always prefer methods even with slight advantage in time or memory. And they won't consider complications in the manual execution of the method. Therefore, programmers won't consider alternate algorithm for the sake of computational simplicity.
\end{commentary}
\end{remark}

\subsection{Algorithm : Gauss elimination method}
\begin{program}[Gauss elimination]
	\begin{python}
	from numpy import dot
	def gaussElimination(a,b):
		n = len(b)
		for k in range(0,n-1):
			for i in range(k+1,n):
				if a[i,k] != 0.0:
					lam = a[i,k]/a[k,k]
					a[i,k+1:n] = a[i,k+1:n]-lam*a[k,k+1:n]
					b[i] = b[i]-lam*b[k]
		for k in range(n-1,-1,-1):
			x[k] = (b[k]-dot(a[k,k+1:n],x[k+1:n]))/a[k,k]
		return b
	\end{python}
\end{program}

\begin{remark}Explaination
\begin{commentary}
\begin{itemize}
	\item \texttt{from numpy import dot} \\ Imports the ``dot()'' function for numpy arrays which takes two `numpy arrays' as input arguments and returns the dot product of them.
	\item \texttt{def gaussElimination(a,b):}\\ it defines ``gaussElimination()'' as a function which takes two arguments (inputs). First argument is the coefficient matrix $A$ and second argument is the constant matrix $b$ of the linear system of the form $Ax = b$.
	\item \texttt{$n = len(b)$}\\ it assigns the length of the list $b$ into variable $n$ which is obviously the number of equations.
	\item \texttt{for $k$ in range($0$,$n-1$):}  \\ it is a loop construct. Five instructions following it are part of this loop body, which are executed for each values of $k$ ie, $k = 0, 1, \cdots, n-1$. For each value of $k$, the unknown $x_{k+1}$ is selected for elimination process.
	\item \texttt{for i in range($k+1$,$n$):}\\ it is a loop inside another loop. Four instructions following it are part of this loop body, which are executed for each values of $i$, ie, $i = k+1, k+2, \cdots, n$. This eliminates $x_{k+1}$ from all the equations after the $k+1$th equation of the system. Value of $i+1$ is the equation\footnote{Python starts counting from zero. For example : $A_{11} = a[0,0]$, $x_1 = x[0]$ and $b_1 = b[0]$} from which $x_{k+1}$ is eliminated.
	\item \texttt{if $a[i,k] != 0.0$:} \\ If $a[i,k] = A_{i+1,k+1} \ne 0$, then those three instruction following it are executed. Otherwise, it skips the execution of those three statements. If $x_{k+1}=x[k]$ is not there in the $i$th equation, it doesn't need to be eliminated.
	\item \texttt{$lam = a[i,k]/a[k,k]$} \\ In this step, $\lambda$ is computed so that equ.($i+1$) - $\lambda$ equ.($k+1$) doesn't have $x_{k+1}$ term in it.
	\item \texttt{$a[i,k+1:n] = a[i,k+1:n]-lam \times a[k,k+1:n]$} \\ Coefficients of $(i+1)$th equation are updated.\\ Equivalent to $a[i,0:n] = a[i,0:n] - lam \times a[k,0:n]$, since zeroes need not be substracted. This is same as equ.($i+1$) $\leftarrow$ equ.($i+1$) - $\lambda$ equ.($k+1$)
	\item \texttt{$b[i] = b[i]-lam*b[k]$} \\ The same row operations are performed on the matrix $b$ instead of using an augmented matrix.
	\item \texttt{for $k$ in range($n-1$,$-1$,$-1$):} \\ This is another loop construct. The following statement is executed $n$ times for values of $k = n-1, n-2, \cdots, 0$. Value of $k+1$ gives the unknown $x_{k+1}$ which is solved by the back substitution process.
	\item \texttt{$x[k] = (b[k]-dot(a[k,k+1:n],x[k+1:n]))/a[k,k]$} \\ This is the back substitution process. After elimination phase we have $k$ equation in the form $A_{k,k}x_{k} + A_{k,k+1}x_{k+1}+\cdots+A_{k,n}x_n = b_k$. And we already have values of $x_{k+1},\ x_{k+2},\ \cdots,\ x_n$. Then
	\[x_{k} = \frac{b_k - (A_{k,k+1}x_{k+1} + A_{k,k+2}x_{k+2}+\cdots)}{A_{k,k}}\]
	This is equivalent to 
	\[ b_k \leftarrow \frac{ b_k - \begin{bmatrix}A_{k,k+1} & A_{k,k+2} & \cdots A_{k,n} \end{bmatrix}\begin{bmatrix} x_{k+1} \\ x_{k+2} \\ \vdots \\ x_n \end{bmatrix}}{A_{k,k}} \]
	Remember : The values of $x_k$ are updated into $b_k$ as they are computed. Thus $x_{k}, x_{k+1}, \cdots, x_n$ are stored in $b$ for next back substitution ie, for evaluating $x_{k-1}$. We start with $x_{n-1}$, as $x_n = b_n$ is already solved.
	\item \texttt{return b} \\ It returns the new $b$ matrix as output of the ``gaussElimination()'' function where $x_k = b_k,\ \forall k$.
\end{itemize}
\end{commentary}
\end{remark}

\section{LU Decomposition Method : Doolittle}
	Let $Ax = b$ be a linear system of $n$ equations in $n$ unknowns and let $A = LU$ for some lower triangular matrix $L$ and upper triangular matrix $U$. Then we have $LUx = Ly = b$ where $y = Ux$. There are two phases for this method : \begin{enumerate*} \item LU decomposition and \item substitution \end{enumerate*}.\\

	First, we compute $L$ and $U$ such that $A = LU$ using gauss elimination. Then We can solve $Ly = b$ using forward substitution process and then solve $Ux = y$ using back substitution process.\\

	For Doolittle decomposition,
	\[ A = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ L_{21} & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ L_{n1} & L_{n2} & \cdots & 1 \end{bmatrix} \begin{bmatrix} U_{11} & U_{12} & \cdots & U_{1n} \\ 0 & U_{22} & \cdots & U_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & U_{nn} \end{bmatrix} \]

	\[ [L\text{\textbackslash{}}U] = \begin{bmatrix} U_{11} & U_{12} & \cdots & U_{1n} \\ L_{21} & U_{22} & \cdots & U_{2n} \\ \vdots & \vdots & \ddots & \vdots \\  L_{n1} & L_{n2} & \cdots & U_{nn} \end{bmatrix} \]

	is the combined matrix containing both the triangular matrices.

	\begin{commentary}Note that in Doolittle's decomposition method, the diagonal entries of the lower triangular matrix $L$ are all 1. ie, $L_{ii} = 1,\ \forall i$. Thus, we can use an $n \times n$ matrix to represent both $L$ and $U$ by overwriting trivial entries( zeroes and ones) of both the matrices. And this matrix is reprensented by $[L\text{\textbackslash{}}U]$ Thus, we can use an $n \times n$ matrix to represent both $L$ and $U$ by overwriting trivial entries of both the matrices. And this matrix is reprensented by $[L\text{\textbackslash{}}U]$.\footnote{algorithmic implementation all decomposition algorithms prefer to use a combined matrix}\end{commentary}\\

\subsection{Illustrative example}
	Suppose, we have a system of three linear equations, then

	\[ A = LU = \begin{bmatrix} 1 & 0 & 0 \\ L_{21} & 1 & 0 \\ L_{31} & L_{32} & 1 \end{bmatrix} \begin{bmatrix} U_{11} & U_{12} & U_{13} \\ 0 & U_{22} & U_{23} \\ 0 & 0 & U_{33} \end{bmatrix} \]

	\[ A= \begin{bmatrix} U_{11} & U_{12} & U_{13} \\ U_{11}L_{21} & U_{12}L_{21} + U_{22} & U_{13}L_{21}+U_{23} \\ U_{11}L_{31} & U_{12}L_{31}+U_{22}L_{32} & U_{13}L_{31} + U_{23}L_{31} + U_{33} \end{bmatrix} \]

	Thus, we can compute $L$ and $U$ using the gauss elimination process\footnote{We usually need a proof for such a strong statement}. The matrix obtained after gauss elimination on $A$ is $U$ and the values of the variable $lam$ used in gauss elimination are the entries in $L$.


\subsection{Algorithm : Doolittle's LU Decomposition method}
\begin{program}
	\begin{python}
		from numpy import dot
		def LUdecomposition(a):
			n = len(a)
			for k in range(0,n-1) : 
				for i in range(k+1,n):
					if a[i,k] != 0.0:
						lam = a[i,k]/a[k,k]
						a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
						a[i,k] = lam
			return a
		def LUsolve(a,b):
			n = len(a)
			for k in range(1,n):
				b[k] = b[k]-dot(a[k,0:k],b[0:k])
			b[n-1] = b[n-1]/a[n-1,n-1]
			for k in range(n-2,-1,-1):
				b[k] = (b[k] - dot(a[k,k+1:n],b[k+1:n]))/a[k,k]
			return b
	\end{python}
\end{program}

\begin{remark}Explaination
\begin{commentary}
	This program mainly uses the gauss elimination algorithm. Thus, the explanation for Lines 3-8 are not repeated here.\\

	But remember the loop at Line 4 has inner loop at Line 5 and Line 7-9 are at same level of indentation which means they all are either executed or skipped depending on the truthness of the condition in Line 6. And Line 6-9 are executed for each instance of inner loop. Again, Line 5-9 are executed for each instance of the outer loop.\\

This time the gaussElimination() function which you have seen earlier is split into two functions \begin{enumerate*} \item LUdecomposition() and \item LUsolve() \end{enumerate*}. And forward substituion is also added to LUsolve().

	\begin{itemize}
		\item \texttt{def LUdecomposition(a):} \\ LUdecomposition($A$) computes $L$ and $U$ such that $A = LU$ and combine both triangular matrices into a single matrix $[L/U]$, by over-writting their trivial entries. And returns this combined matrix
		\item \texttt{$a[i,k]=lam$} \\ Clearly, $\lambda_{i,k} = a[i,k]/a[k,k] = L[i,k],\ \forall k, \forall i>k$. Also $a[i,k]$ which is reduced zero by gaussian elimination process is not used anymore in gaussian elimination process. Thus $L[i,k]$ can stored at $a[i,k]$ straight away. And $U[i,j],\ j\le i$ are already the entries of the matrix obained from gaussian elimination. Thus for each iterations of $k$, the matrix $a$ is updated ($k+1$th row and $k+1$th column) with respective entries of the combined matrix $[L\text{\textbackslash{}}U]$.
		\item \texttt{return $a$} \\ Matrix $a$ is already $[L\text{\textbackslash{}}U]$, and thus LUdecomposition($A$) returns $[L\text{\textbackslash{}}U]$ such that $A = LU$.
		\item \texttt{def LUsolve(a,b):} \\ LUsolve() function does both forward substitution and back substitution. Suppose $Ax = b$ is the system to be solved. Then the inputs of LUsolve() are $ a = [L\text{\textbackslash{}}U]$ where $A = LU$. 
		\item \texttt{$n = len(a)$} \\ We have to compute this again as this function starts fresh and thus value of the variable $n$ from LUdecomposition() is lost.
		\item \texttt{for $k$ in range($1$,$n$):} \\ This is the loop for forward substitution.
		\item \texttt{$b[k] = b[k]-dot(a[k,0:k],b[0:k])$} \\ Forward substitution process.
		\item \texttt{$b[n-1] = b[n-1]/a[n-1,n-1]$} \\ Updating $b$ with $y$ for back substitution process. ie, $b[i+1]=y_i$ such that $Ly = b$
		\item \texttt{for $k$ in range($n-2$,$-1$,$-1$):}  \\ This is the loop for back substitution.
		\item \texttt{$b[k] = (b[k] - dot(a[k,k+1:n],b[k+1:n]))/a[k,k]$} \\ back substitution process and updating $b$ with $x$ such that $Ux = y$.
		\item \texttt{return $b$} \\ LUsolve($[L\text{\textbackslash{}}U]$,$b$) returns $b$ where $b[i+1]=x_i$.
\end{itemize}
	There are few things to remember when spliting a function into two functions. \begin{enumerate*} \item These functions are completely independent of one another \item Variables defined inside a function are not available outside \item Best way to give/take data to/from a function is through arguements/return-value \end{enumerate*}.
\end{commentary}
\end{remark}

%\section{}
