%Text books : \cite{brigs}, \cite{saha}, \cite{kiusalaas}
%Module 1:
%Defining Symbols and Symbolic Operations, Working with Expressions, Solving Equations and Plotting Using SymPy, problems on factor finder, summing a series and solving single variable inequalities
%Chapter 4 of \cite{saha} 
%Module 2:
%Finding the limit of functions, finding the derivative of functions, higher-order derivatives and finding the maxima and minima and finding the integrals of functions are to be done. in the section programming challenges, the following problems - verify the continuity of a function at a point, area between two curves and finding the length of a curve
%Chapter 7 of \cite{saha} 
%Module 3:
%Interpolation and Curve Fitting - Polynomial Interpolation - Lagrange's Method, Newton's Method and Limitations of Polynomial Interpolation, Roots of Equations - Method of Bisection and Newton-Raphson Method.
%Sections 3.1, 3.2, 4.1, 4.3, 4.5 of \cite{kiusalaas}
%Module 4:
%Gauss Elimination Method (excluding Multiple Sets of Equations), Doolittle's Decomposition Method only from LU Decomposition Methods, Numerical Integration, Newton-Cotes Formulas, Trapezoidal rule, Simpson's rule and Simpson's 3/8 rule.
%Sections 2.2, 2.3, 6.1, 6.2 of \cite{kiusalaas}

%Programs
%Online Program, Explanation, Algorithm, Function Syntax, Terminology
\part{ME010203 Numerical Analysis with Python}
%\chapter{Expressions}
%\chapter{Calculus}
\chapter{Module III}
%\chapter{Interpolation \& Curve Fitting}
\begin{definition}
	Given $(n+1)$ data points $(x_k, y_k),\ k = 0,1,\cdots,n$, the problem of estimating $y(x)$ using a function $y : \mathbb{R} \to \mathbb{R}$ that satisfy the data points is the interpolation problem. ie, $y(x_k) = y_k,\ k = 0,1,\cdots,n$.
\end{definition}
\begin{definition}
	Given $(n+1)$ data points $(x_k,y_k),\ k = 0,1,\cdots,n$, the problem of estimating $y(x)$ using a function $y : \mathbb{R} \to \mathbb{R}$ that is sufficiently close to the data points is the curve-fitting problem.\\ ie, Given $\epsilon > 0,\ |y(x_k)-y_k| < \epsilon,\ k = 0,1,\cdots,n$.
\end{definition}
\begin{remark}
	\textcolor{purple}{The data could be from scientific experiments or computations on mathematical models. The interpolation problem assumes that the data is accurate. But, curve-fitting problem assumes that there are some errors involved which are sufficiently small.}
\end{remark}
\begin{definition}
	Given $(n+1)$ data points $(x_k,y_k),\ k = 0,1,\cdots,n$, the problem of estimating $y(x)$ using a polynomial function of degree $n$ that satisfy the data points is the polynomial interpolation problem.
\end{definition}
\begin{remark}
	Polynomial is the `simplest' interpolant.\cite[3.2]{kiusalaas}
\end{remark}

\section{Polynomial Interpolation}
There exists a unique polynomial of degree $n$ that satisfy $(n+1)$ distinct data points. There are a few methods to find this polynomial : 
\begin{enumerate*}
	\item Lagrange's method
	\item Newton's method
	\item Neville's method
\end{enumerate*}. The Neville's method is out of scope.

\subsection{Lagrange's Method}
Interpolation polynomial\footnote{Using $P_n$ to represent some polynomial of degree $n$. It is quite a confusing a notation when it comes to Newton's method as author construct a psuedo-recursive definition.} is given by,
\begin{equation}
	P(x) = \sum_{i=0}^n y_i l_i(x),\text{ where } \l_i(x) = \prod_{j = 0,j \ne i}^n \frac{x-x_i}{x_j-x_i}
	\label{equ:lagrange}
\end{equation}
\begin{remark}
	Lagrange's cardinal functions $l_i$, are polynomials of degree $n$ and
	$$l_i(x_j) = \delta_{ij} = \begin{cases} 0,\ i = j \\ 1,\ i \ne j \end{cases}$$
\end{remark}
\begin{proposition}
	Error in polynomial interpolation is given by
	\begin{equation}
		f(x) - P(x) = \frac{(x-x_0)(x-x_1)\cdots(x-x_n)}{(n+1)!} f^{(n+1)}(\xi)
		\label{equ:error}
	\end{equation}
	where $\xi \in (x_0, x_n)$
\end{proposition}
\begin{remark}
	The error increases as $x$ moves away from the unknown value $\xi$.
\end{remark}

\subsection{Newton's Method}
The interpolation polynomial is given by,
\begin{equation}
	P(x) = a_0 + a_1(x-x_0) + \cdots + a_n(x-x_0)(x-x_1)\cdots(x-x_{n-1})
	\label{equ:newton}
\end{equation}
where $a_i = \nabla^i y_i,\ i = 0,1,\cdots,n$.
\begin{remark}
	For Newton's Method, usually it is assumed that $x_0 < x_1 < \cdots < x_n$.
\end{remark}
\begin{remark}
	 Lagrange's method is conceptually simple. But, Newton's method is computationaly more efficient than Lagrange's method.
\end{remark}
\subsubsection{Computing coefficients $a_i$ of the polynomial}
The coefficients are given by,
\begin{equation}
	a_0 = y_0,\ a_1 = \nabla y_1,\ a_2 = \nabla^2 y_2,\ a_3 = \nabla^3 y_3,\cdots, a_n = \nabla^n y_n
\end{equation}
\begin{remark}
	The divided difference $\nabla^i y_i$ are computed as follows:
	\begin{align*}
		\nabla y_1 = \frac{y_1 - y_0}{x_1 - x_0} & & \\
		\nabla y_2 = \frac{y_2 - y_1}{x_2 - x_1} &\qquad \nabla^2 y_2 = \frac{\nabla y_2 - \nabla y_1}{x_2-x_1} & \\
		\nabla y_3 = \frac{y_3 - y_2}{x_3 - x_2} &\qquad \nabla^2 y_3 = \frac{\nabla y_3 - \nabla y_2}{x_3-x_2} & \nabla^3 y_3 = \frac{\nabla^2 y_3 - \nabla^2 y_2}{x_3-x_2}
	\end{align*}
\end{remark}
\begin{table}[hb]
	\centering
	\begin{tabular}{|c||c|c|c|c|c|}
		\hline
		$x_0$ & $y_0$ & & & & \\ \hline
		$x_1$ & $y_1$ & $\nabla y_1$ & & &   \\ \hline
		$x_2$ & $y_2$ & $\nabla y_2$ & $\nabla^2 y_2$ & &   \\ \hline
		\dots & \dots & \dots & \dots & $\ddots$ & \\ \hline
		$x_n$ & $y_n$ & $\nabla y_n$ & $\nabla^2 y_n$ & \dots & $\nabla^n y_n$  \\ \hline
	\end{tabular}
	\caption{The $\nabla^i y_i$ Computation Table}
\end{table}

\begin{remark}
	Practise Problems\\
	Find interpolation polynomial for the following data points :
	\begin{enumerate}
		\item $\{(0,7),\ (2,11),\ (3,28)\}$\\ \textcolor{blue}{Ans : $5x^2-8x+7$} \\ \cite[Example 3.1]{kiusalaas}
		\item $\{(-2,-1),\ (1,2),\ (4,59),\ (-1,4),\ (3,24),\ (-4,-53) \}$\\ \textcolor{blue}{Ans : $x^3-2x+3$}\\ \cite[Example 3.2]{kiusalaas}
		\item $\{(-1.2,-5.76),\ (0.3,-5.61),\ (1.1,-3.69)\}$\\ \textcolor{blue}{Ans : $x^2+x-6$}\\ \cite[Problem Set 3.1.1]{kiusalaas}
	%	\item $\{(0,-1.00),\ (0.5,1.75),\ (1,4.00),\ (1.5,5.75),\ (2,7.00)\}$\\ \cite[Problem Set 3.1.4]{kiusalaas}
	%	\item $\{(-2,-1),\ (1,2),\ (4,59),\ (-1,4),\ (3,24),\ (-4,-53)\}$\\ \cite[Problem Set 3.1.6]{kiusalaas}
		\item $\{(-3,0),\ (2,5),\ (-1,-4),\ (3,12),\ (1,0)\}$\\ \textcolor{blue}{Ans : $x^2+2x-3$}\\ \cite[Problem Set 3.1.7]{kiusalaas}
		\item $\{(0,1.225),(3,0.905),(6,0.652)\}$\\ \textcolor{blue}{Ans : $0.0037x^2-0.1178x+1.225$}\\ \cite[Problem Set 3.1.9]{kiusalaas}
	\end{enumerate}
\end{remark}

\begin{remark}
	In Lagrange's Method, we can interpolate at the given point even without computing the polynomial. In Newton's method, we have to compute polynomial and then interpolate for the given point.\\

	That is, evaluate the value of cardinal polynomials at the point and substitute in Equation \ref{equ:lagrange} as shown in Section 3.2.\cite[Example 3.1]{kiusalaas}
\end{remark}

\subsection{Implementation of Newton's Method}

\begin{program}Computing Coefficients
	\begin{python}
		def coefficients(xData,yData):
			m = len(xData)
			a = yData.copy()
			for k in range(1,m):
				a[k:m] = (a[k:m]-a[k-1])/(xData[k:m]-xData[k-1])
			return a
	\end{python}
\end{program}
\begin{commentary}
	Line 1 : \textbf{Defines a function which takes two arguments/parameters, named $xData$ and $yData$.} In \cite[3.2]{kiusalaas}, you will find coeffts which I have changed to coefficients. $xData$,$yData$ are numpy array objects. $xData$ is a array with values $x_0,x_1,\cdots,x_n$. And $yData$ is array with values $y_0,y_1,\cdots,y_n$. For example, the value of $x_3$ can be accessed as $xData[3]$.\\
	
	Line 2 : \textbf{The functon $len()$ is extended by numpy to give the length of array objects.} In this context, $len(xData)$ will return the value $n+1$, since there are $n+1$ values in $xData$ array.\\

	Line 3 : We need a copy of $yData$ to work with. Unlike other programming languages like java, in python $a = yData$ will assign a new label $a$ to the same memory location and manipulating $a$ will corrupt the original data in $yData$ as well. In order to avoid this, we are \textbf{making a copy of the array object using the array method provided by the numpy library.}\\

	Line 4 : \textbf{This is a python loop statement. This ask python interpreter to repeat the following sub-block $m-1$ times.}\footnote{Python block is a group of statement with same level of indentation. A sub-block is a block with an additional indentation.} In this context, Line 5 will be executed $n$ times, since the $range(1,m)$ object is a list-type object with values $1,2,\cdots,m-1$. And intepreter executes Line 5 for each values in the $range()$ object, ie, $k=1,2,\cdots, m-1$ before interpreting Line 6.\\

	Line 5 : This is very nice feature available in python. \textbf{This statement, evaluates $m-k$ values in a single step.ie, $a[k],a[k+1],\cdots,a[m]$. This calculation corresponds to subsequent columns of the divided difference table, that we are familiar with.} For example, executing Line 5 with $k=3$ is same as evaluating the $\nabla^3y_j$ column. Note that the value $a[0]$ is never updated and similarly $a[2]$ changes when Line 5 is executed with $k=1,2$. From column 3 onward, $a[2]$ is not updated. Therefore, \textbf{after completing $n$th executing of the Line 5, we have $a[0] = y_0,\ a[1]=\nabla y_1,\ a[2]=\nabla^2 y_2,\cdots,\ a[n]=\nabla^n y_n$.}\\

	Line 6 : \textbf{This returns the array $a$ which is the array of coefficients.}\\

	The logic of this program is in Line 4 and Line 5. So they need more explanation/understanding than anything else.
\end{commentary}

\begin{program}Interpolating using Newton's Method
	\begin{python}
		def interpolate(a,xData,x):
			n = len(xData)-1
			p = a[n]
			for k in range(1,n+1):
				p = a[n-k]+(x-xData[n-k])*p
			return p
	\end{python}
	The logic this program is in Line 3, Line 4 and Line 5.\\

	Line 3 : We initialize the polynomial with the coefficient $a[n] = \nabla^n y_n = a_n$.\\

	Line 4 : We are going define the polynomial recursively. This takes exactly $n$ steps further. So we use a loop which repeats $n$ times.\\

	Line 5 : The value of $p$ and $k$ changes each time Line 5 is executed. Let $P_j$ be the value in $p$ after executing Line 5 with $k=j$. Then,\\ $P_0 = p = a[n]$\\ $P_1 = a[n-1]+(x-x_{n-1})P_0$\\ $P_2 = a[n-2]+(x-x_{n-2})P_1$\\ $\vdots$\\ $P_n = a[0]+(x-x_0)P_{n-1}$.\\ Clearly, $P_n$ is the unique $n$ degree polynomial given by the Newton's method.
\end{program}

\begin{program}How to interpolate ?
	\begin{python}
		from numpy import array
		xData = array([-2,1,4,-1,3,-4]) #change as needed
		yData = array([-1,2,59,4,24,-53]) #change as needed
		a = coefficients(xData,yData)
		print(interpolate(a,xData,2))
	\end{python}

	You will have to define both the functions (coefficients, interpolate) before doing this.\\

	Line 1 : For defining array objects, we need to import them from numpy library.\\

	Line 2 : You can change this line according to the first component of the given data points.\\

	Line 3 : You can change this line according to the second component of the given data points.\\

	Line 4 : Call function \texttt{coefficients} and store the array returned into a\\

	Line 5 : Call function \texttt{interpolate} to interpolate at $x = 2$ and print the value $P(2)$
\end{program}

\begin{program}[Just for Fun]
	We can do more using sympy !
	\begin{python}
		from numpy import array
		from sympy import Symbol
		xData = array([-2,1,4,-1,3,-4])
		yData = array([-1,2,59,4,24,-53])
		a = coefficients(xData,yData)
		x = Symbol('x')
		p = interpolate(a,xData,x)
		p.subs({x:2})
	\end{python}
\end{program}

\begin{remark}
	Programming Problems
	\begin{enumerate}
%		\item $\{(4.0,-0.06604),\ (3.9,-0.02724),\ (3.8,0.01282),\ (3.7,0.05383)\}$\\ \cite[Example 3.3]{kiusalaas}
		\item $\{(0.15,4.79867),\ (2.30,4.49013),\ (3.15,4.2243),\ (4,85,3.47313),\\ (6.25,2.66674),\ (7.95,1.51909)\}$ \cite[Example 3.4]{kiusalaas}
%		\item $\{(0,1.8421),\ (0.5,2.4694),\ (1,2.4921),\ (1.5,1.9047),\ (2,0.8509),\ (2.5,-0.4112),\ (3,-1.5727)\}$\\ \cite[Problem Set 3.1.2]{kiusalaas}
		\item $\{(0,-0.7854),\ (0.5,0.6529),\ (1,1.7390),\ (1.5,2.2071),\ (2,1.9425)\}$\\ \cite[Problem Set 3.1.5]{kiusalaas}
	\end{enumerate}
\end{remark}
\subsection{Limitations of Polynomial Interpolation}
\begin{enumerate}
	\item Inaccuracy - The error in interpolation increases as the point moves away from most of the data points.
	\item Oscilation - As the number of data points considered for polynomial interpolation increases, the degree of the polynomial increases. And the graph of the interpolant tend to oscilate excessively. In such cases, the error in interpolation is quite high. 
	\item The best practice is to consider four to six data points nearest to the point of interest and ignore the rest of them.
\end{enumerate}
\begin{remark}
	The interpolant obtained by joining cubic polynomials corresponding to four nearest data points each, is a cubic spline\footnote{Cubic spline is a function, the graph of which is piece-wise cubic}.
\end{remark}

\section{Roots of a Function}
\begin{definition}
	Let $f : \mathbb{R} \to \mathbb{R}$, then $x \in \mathbb{R}$ is a root of $f$ if $f(x) = 0$.
\end{definition}

\begin{remark}
	Suppose $a <  b$ and $f(a), f(b)$ are nonzero and are of different signs. If $f$ is continuous in $[a,b]$, then there is a point $c \in [a,b]$ such that $f(c) = 0$.\\
	
	Thus given $a<b$ and $f(a),f(b)$ are nonzero values of different sign, then there may be a bracketed root in $[a,b]$.\\
	
	Note : There is no guarantee that there exists a root in $[a,b]$ as we are not sure about the continuity of $f$.
\end{remark}

\begin{remark} Given a bracketed root, we can find it using
	\begin{enumerate}
		\item Bisection Method or
		\item Newton-Raphson Method
	\end{enumerate}
\end{remark}

\subsection{Bisection Method}
	Suppose $a < b$ and $f(a),f(b)$ are nonzero values of different signs. We evaluate $f(c)$ where $c = \frac{a+b}{2}$. If $f(c)$ is a nonzero value, then at least one of the pairs $f(a),f(c)$ or $f(c),f(b)$ are of different signs. WLOG suppose that $f(a),f(c)$ are of different signs, then set $b = c$ and $c = \frac{a+b}{2}$. And continue this process until we get sufficiently accurate value of a root.

\begin{remark}
	Suppose $f(x) = x^5 - 2$. Then $f(0) = -2,\ f(1) = -1,\ f(2) = 30$. Since $f$ is known to be continuous, there is a bracketed root in $[1,2]$. Now $f(1.5) > 0 \implies [1,1.5]$\\
	$f(1.25) > 0 \implies [1,1.25]$\\
	$f(1.125) < 0 \implies [1.125,1.25]$\\
	$f(1.1825)>0 \implies [1.125,1.1825]$\\
	$f(1.15375)>0 \implies [1.125,1.15375]$\\
	$f(1.139375) < 0 \implies [1.139375,1.15375]$\\
	$f(1.1465625) < 0 \implies [1.1465625,1.15375]$\\
	$f(1.150156250) > 0 \implies [1.1465625,1.15015625]$\\
	$f(1.148359375) < 0 \implies [1.1483594,1.15015625]$\\
	$f(1.149257825) > 0 \implies [1.1483594,1.14925783]$\\
	
	Thus, we have $1.14$ is a root of $f$ with accuracy upto two decimal points.
\end{remark}
%fifth root of 2 = 1.14869835

\subsection{Newton-Raphson Method}
	Suppose $f$ is differentiable at $x \in \mathbb{R}$ and $f(x) \ne 0$. Then compute $x = x - \frac{f(x)}{df(x)}$ and evaluate $f(x)$. Repeat this process to get more accurate value of a root near $x$.

\begin{remark}
	Suppose $f(x) = x^5 - 2$. Then $df(x) = 5x^4$. Let $x = 2$. Then\\
	$x = 2 - \frac{30}{80} \implies f(1.625) = 9.330 $\\
	$x = 1.625 - \frac{9.330}{34.86} \implies f(1.35735) = 2.6074$ \\
	$x = 1.35735 - \frac{2.6074}{16.9721} \implies f(1.20373) = 0.52733$ \\
	$x = 1.20373 - \frac{0.52733}{10.4975} \implies f(1.15351) = 0.04224$ \\
	$x = 1.15351 - \frac{0.042245}{8.85225} \implies f(1.148738) = 0.00034312 $\\

	Thus we have $1.1487$ is quite close to a root of $f$.
\end{remark}

%\chapter{Matrix Operations}
\chapter{Module IV}
\section{Gauss elimination method}
\par Gauss Elimination method is a method for solving simultaneous equations.(ie, system of linear equations of the form $Ax = B$). It consists of two phases \begin{enumerate*} \item elimination \item back substitution \end{enumerate*}. The elimination phase transforms the equation into $Ux = C$ form where $U$ is an upper-triangular\footnote{upper triangular - all the entries below the main diagonal are zero. ie $U_{ij} = 0,\ \text{ if }i<j$} matrix. And the unknown variables are evaluated in the reverse order substituting those which are discovered.\\

\subsection{Illustrative example}
Consider the following system of linear equations,
\begin{align} 4x_1  - 2x_2  + x_3 & = 11 \label{eqn:a1}\\ -2x_1  + 4x_2 - 2x_3 & = -16 \label{eqn:a2}\\ x_1  - 2x_2 + 4x_3 & = 17 \label{eqn:a3}\end{align} 
\par We may represent the above system of linear equations using matrices,
\[ \begin{bmatrix} 4 & -2 & 1 \\ -2 & 4 & -2 \\ 1 & -2 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 11 \\ -16 \\ 17 \end{bmatrix} \]

\par Phase 1 : Elimination Process\\

Using eq.\ref{eqn:a1}, the unknown $x_1$ is eliminated from all subsequent equations. An equivalent operation can be performed on the augmented\footnote{We use augmented matrix $[A:B]$ for computational simplicity.} matrix by adding suitable scalar multiples of row 1 to each row underneath it.
\[ \begin{bmatrix} 4 & -2 & 1 & 11 \\ -2 & 4 & -2 & -16 \\ 1 & -2 & 4 & 17 \end{bmatrix} \xrightarrow{\substack{R_2 = R_2 + 0.5R_1\\ R_3 = R_3 - 0.25R_1}} \begin{bmatrix} 4 & -2 & 1 & 11 \\ 0 & 3 & -1.5 & -10.5 \\ 0 & -1.5 & 3.75 & 14.25 \end{bmatrix}\]
And using eq.\ref{eqn:a2}, $x_2$ is eliminated from all subsequent equations( only those rows below it). Again, we perform this by adding suitable scalar multiples of row 2 to each row below.
\[ \begin{bmatrix} 4 & -2 & 1 & 11 \\ 0 & 3 & -1.5 & -10.5 \\ 0 & -1.5 & 3.75 & 14.25 \end{bmatrix} \xrightarrow{R_3 = R_3 + 0.5R_2} \begin{bmatrix} 4 & -2 & 1 & 11 \\ 0 & 3 & -1.5 & -10.5 \\ 0 & 0 & 3 & 9 \end{bmatrix} \]
	We continue this process until pivot\footnote{Here, we use $A_{ii}$ as the pivot element of row $i$ instead of taking first non-zero value.} element of each row, is the only non-zero entry in its column.

\par Phase 2 : Back substitution Process\\

The unknowns are easily found from the equations by solving them in the reverse order. The unknowns are solved from the bottom and solved variables are used to solve the remain unknowns.
\[ \begin{bmatrix} 4 & -2 & 1 & 11 \\ 0 & 3 & -1.5 & -10.5 \\ 0 & 0 & 3 & 9 \end{bmatrix} \to \begin{cases} 4x_1 - 2x_2 + &x_3  = 11 \\ \quad \qquad 3x_2 - &1.5x_3  = -10.5 \\  &3x_3 = 9 \end{cases}  \]

\begin{align*} x_3 & = \frac{9}{3} = 3 \\  x_2 & = \frac{-10.5 + 1.5x_3}{3} = -2 \\ x_1 & = \frac{11-x_3+2x_2}{4}=1 \end{align*}

\begin{remark}\textcolor{purple}{Why don't they use row-reduced echelon matrix of $A$ to simplify the back substitution phase ?}\\

\textcolor{purple}{This doesn't have much advantage from algorithmic point of view. That is, the time complexity ( number of steps for computation) is unaffected. And algorithms always prefer methods even with slight advantage in time or memory. And they won't consider complications in the manual execution of the method. Therefore, programmers won't consider alternate algorithm for the sake of computational simplicity.}
\end{remark}

\subsection{Algorithm : Gauss elimination method}
\begin{program}[Gauss elimination]
	\begin{python}
	def gaussElimination(a,b):
		n = len(b)
		for k in range(0,n-1):
			for i in range(k+1,n):
				if a[i,k] != 0.0:
					lam = a[i,k]/a[k,k]
					a[i,k+1:n] = a[i,k+1:n]-lam*a[k,k+1:n]
					b[i] = b[i]-lam*b[k]
		for k in range(n-1,-1,-1):
			x[k] = (b[k]-dot(a[k,k+1:n],x[k+1:n]))/a[k,k]
		return b
	\end{python}
\end{program}

\begin{remark}[Program Explanation]
	We define a python function ``gaussElimination()'' as a function which takes two arguments (inputs). First argument is the coefficient matrix $A$ and second argument is the constant matrix $B$ of the linear system of the form $Ax = B$.
\end{remark}

